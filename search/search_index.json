{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Runtime Procedural Terrain Generation (in Unity)","text":""},{"location":"#alas-unity-we-hardly-knew-ye","title":"Alas, Unity, we hardly knew ye","text":"<p>So far as I know, there's nobody out there actually reading any of this, but just in case, here's a final update.</p> <p>In mid September of 2023, Unity (the company) basically imploded; pushing out a revenue model that would have been disastrous for indie developers, retroactively counted installs for a new going-forward fee on existing games, changed their terms of service (also retroactively), and repeatedly lied about it.</p> <p>There was, to put it mildly, anger in the dev community.   As Unity's share price plummeted and the news cycles revolved around developers announcing they were leaving Unity for other platforms (mostly Unreal Engine, but for a few indie developers, Godot is far enough along), Unity walked back as little as they could get away with, but even after a replacement CEO and layoffs, are still going forward with the \"runtime fee,\" albeit modified slightly so that the cost doesn't turn out to be unlimited in the case of a mild unexpected success.</p> <p>Unity has (or had, at least, there's no way of knowing what their numbers look like now, since they're not giving stock guidance any more) a significant developer presence.   They more or less owned the mobile game development market and held a significant share of the indie games, as well.    For developer just learning their trade, Unity's C#-based engine was a little easier to learn than the C++-based Unreal Engine.    Their engine isn't as good as Unreal Engine, and few triple-A titles used it, but for many games--even pretty big names--it was good enough.   Similarly, Unity is infamous for constantly changing recommended practices while leaving the previous recommendation 70% done:  there are currently something like five rendering pipelines, two different UI systems, two different networking systems, two different control systems...none of which are \"complete\" without significant workarounds.    And they've been recently pushing into a \"DOTS\" system that basically throws away almost everything in favor of a new \"but this time it's better!\" model.    Still, the price used to be right, and at least they were giving you options.    But they needed a laser-like focus on game engine technologies and people, but instead were internally hijacked by a focus on borderline predatory monetization tools that seemed to suck all the air from their engineering efforts. </p> <p>It might have been, and might still be, possible for Unity to recover from this.   My intent was to go back to Unity to start implementing some of this stuff on the new Apple Vision Pro, and see if their supposed refocus on their tools was real or not.    But Unity had one final gasp of greed, and\u2014despite receiving significant marketing and support from Apple as the \"premier game engine\" for the Vision Pro\u2014decided to lock it behind a multi-thousand-dollar per year paid license; effectively pricing it out of reach of independent devs entirely.   </p> <p>That's $2K per year, per developer, and eliminates the \"you don't pay until you go over $100,000\" zone that made it the choice of beginning devs everywhere.   A small two-person team would pay more for Unity than they did for the Vision Pro hardware itself--every single year, whether they shipped anything or not.</p> <p>(As an aside, a month and a half after the Vision Pro shipped, I'm seeing almost no Unity-based software on it; the reason for this isn't clear, but none of the possibilities are particularly encouraging toward it's value in developing Vision Pro software.)</p> <p>I'm not a beginner dev (although I'm relatively new to game tech), and I know C++.   Even if I weren't, the C++ mechanics that Unreal Engine uses aren't much harder to learn than C# is, and the engine is all-around better.    And it's shortly going to start shipping its own support for the Vision Pro, not with additional fees.    Given Unity's repeated lack of trustworthiness and focus on its core tools, I'd be insane not to switch.</p> <p>And so that's the bottom line, at least for me.      I'll transfer as much of my knowledge and assets as possible over to Unreal Engine, and continue from there.   But it also means this particular project will go no further, at least not in Unity.  The \"terrain\" objects in Unreal Engine are very different, and I'm not sure yet how much will transfer.   We'll see.</p> <p>If you did read all this, thank you for coming along for the ride.</p> <p>--Christopher Kempke, March, 2024</p> <p>{{ blog_content }}</p>"},{"location":"#update-september-2022","title":"Update, September 2022","text":"<p>I like books.   I don't necessarily mean the physical, dead-tree artifact (although I like those, too), but rather the format.   Books have a beginning, a middle, and an ending.  For nonfiction, they separate topics, explore them to whatever depth they're going to, and then move on to the next one.  It's easy to find what you need in a convenient table of contents, and in well-written material, there's a general flow of information from easiest to hardest, or at least they present each part in an order that builds on the previous ones.</p> <p>Most of all, Books are static.  Sure, there are apparently enough discoveries in basic Calculus every year to merit a new edition of every textbook, and certainly books are revised or replaced over time.  But at a given point in time, they're stable.</p> <p>This project (and in fact all development) isn't.   Software development is an iterative process; experimental development doubly so.   The order in which things are implemented is unlikely to match the order in which you'd present them in a book.</p> <p>I've been trying to present this material as a sort of online book.   And for me, it's working fine.  But I'm hoping that this isn't just a solipsist effort\u2014that there are people out there reading it.   And for them, this presentation sort of sucks.    The main issue is that the process is globally iterative.   I make changes to the global terrain generation, then some local changes, then some regional ones, then more to the global generation, and so on and on.</p> <p>When I make changes, I go back and add to the sections that I've presented.    But that creates a poor experience for the reader.   Partly it's just time dissonance:  you'll see screen shots that are from later in the process than ones that occur later in the book.   But more importantly, unless you're reading this from (virtual) cover to cover every day, you don't know when I've updated a previous section.  It's never \"done,\" and there may well be people trying to \"follow along,\" but there's no good mechanism for knowing when more content is available.   Consider this very section, in which I had to put a huge label at the top.</p> <p>So, while I'm not giving up on the \"book\" idea; I think that format is best for the postmortem stage, when new information has dropped to a trickle.    During the actual development, I'm going to move over the next few weeks to something more like a diary or a blog, probably using the same software I am over in my actual blog.   As I add information, it'll be at the top, and I'll use tags and such to categorize where things will eventually go in a potential \"book form\".   It'll likely be longer-form than most blogs, but that's OK.      The pages that are already here will be moved into blog articles; if you read from bottom to top, you'll get the existing information in its existing order. But new stuff will be at the top, always accessible, and I'll limit the edits to the existing material to things like grammar, spelling, typos, and adding the occasional pointer to something newer.</p> <p>Stay tuned</p> <p>---Chris</p>"},{"location":"#introduction","title":"Introduction","text":"<p>Note:  This is a very long, multi-page document.   If you're not seeing it as such, you're probably using a relatively small browsing window, and the navigation is hidden in a hamburger menu.   You can also use the \"next page\" / \"previous page\" buttons at the bottom of the screen, and in many browsers (if you've got a keyboard), \"N\" and \"P\" (or \",\" and \".\") will navigate as well.   If your reading environment can make the window sufficiently wide, you'll get navigation outlines on both sides of the text (page selection on the left, contents within a page on the right).</p> <p>I've always loved open world games, with their illusion of complete freedom of movement in a vast world ready for exploration.    And as a developer, I've similarly been a fan of procedural generation of game environments: the notion that the game can\u2014through application of rules\u2014generate its own content.    In the best scenarios, this gives way to emergent behavior, in which those rules combine in ways that are more than the sum of their parts, and can produce results (pleasantly) surprising even to the developer.  (Contrast with bugs, which are results unpleasantly surprising to the developer...)</p> <p>The logical endpoint of this are games like No Man's Sky, in which virtually the entire world is procedurally generated, like this:</p> <p></p> <p>As a developer, one of the attractions of such systems is that a small team--perhaps even an individual--can create a \"large\" game in this way.   Since, despite what the scale tells me, I am an individual, and I've now got time to look into projects like this, I'm combining an effort to learn the Unity game engine with a desire to push some boundaries of procedural design.</p> <p>So what is this site, anyway?  Well, it's something between a reference, a blog, and a developer diary as I play around with building terrains.    I show only snippets of code, but if you want the actual code in whatever state it's in, send me an e-mail.  You'll notice that it's a little disordered; some sections just sort of trail off until I implement the next part of them, when I go back and add more content.   The physical organization is meant to be readable rather than either strictly lumping topics together or chronological.    And I keep tweaking it!   Will this turn into something more than some musings on the web?   Who knows?   That's the wonder of it.</p> <p>Another wonder of the modern Internet is that you never know who's going to wander by, courtesy of Bing, Google, or the world's most specific typo.    If you're looking for me, contact information is over at https://www.chriskempke.com, or use email to \"ckempke\" at the domain mac.com.</p> <p>--Christopher Kempke</p>"},{"location":"#tools","title":"Tools","text":"<p>This site is being hosted at GitHub Pages, but it's not using their Jekyll parser.  Instead, I'm using MkDocs, with the Material theme, and with the Mermaid extensions turned on.</p> <p>Mermaid allows graphs such as flowcharts and sequence diagrams:</p> <pre><code>flowchart TB\n    c1--&gt;a2\n    subgraph one\n    a1--&gt;a2\n    end\n    subgraph two\n    b1--&gt;b2\n    end\n    subgraph three\n    c1--&gt;c2\n    end\n    one --&gt; two\n    three --&gt; two\n    two --&gt; c2\n</code></pre> <p>I also enable KaTeX, which allows both inline (\\(c = \\pm\\sqrt{a^2 + b^2}\\)) and callout LaTeX mathematics symbols:</p> \\[ \\mathbf{V}_1 \\times \\mathbf{V}_2 =  \\begin{vmatrix} \\mathbf{i} &amp; \\mathbf{j} &amp; \\mathbf{k} \\\\ \\frac{\\partial X}{\\partial u} &amp;  \\frac{\\partial Y}{\\partial u} &amp; 0 \\\\ \\frac{\\partial X}{\\partial v} &amp;  \\frac{\\partial Y}{\\partial v} &amp; 0 \\\\ \\end{vmatrix} \\] <p>Docsify itself parses Markdown files of various sorts to produce the HTML display you're looking at now, including the sidebar to the left (or in the hamburger menu if you're browsing on mobile) and the live text search.</p>"},{"location":"Background/","title":"Background","text":"<p>This paper is an investigation into natural terrain generation in the Unity game engine, meaning environments that conform to the expected geological, botanical, and other attributes of the \u201creal world\u201d (or whatever world\u2019s expectations we\u2019re trying to render) without being directly sampled from it.</p> <p>Terrain generation is applicable to a number of fields, but primarily computer games and simulations.   It\u2019s often desirable to have \u201creal-world-like,\u201d yet still fictitious settings for player interaction, or be able to generate \u201cprobable\u201d terrains for unexplored areas that we wish to answer questions about (or land a spacecraft on).</p> <p>The exact usage often doesn\u2019t matter\u2014and Unity\u2019s primary use is for games\u2014so we\u2019ll use \u201cgame\u201d as a catch-all term for the rest of this discussion.</p> <p>Author's Note:  I'm leaving the above paragraphs in.  But as I've written this, it's become pretty obvious that \"game\" nature and \"real\" nature are mostly incompatible goals.   So, when forced to make a choice--which happens more often than you'd expect, typically a sort of cascade effect around distance and scale--I've gone with team \"game.\"</p>"},{"location":"Background/#geology-biome-and-water","title":"Geology, Biome, and Water","text":"<p>A \"Terrain\" as we customarily think of it is made up of at least two very high-level properties.</p> <p>First, we have the \"Geology,\" which determines the physical structure of the \"ground\" in the terrain.  Examples include \"mountains\", \"hills\", \"plains\", and more exotic structures like canyons, badlands, moors, buttes, plateaus, land bridges, caves, etc.   We can even stretch the word \u201cnatural\u201d to the breaking point to include things like Avatar\u2019s air-floating islands for magical worlds. In Unity's built-in terrain objects, geology is represented by the components related to the mesh: the mesh itself, as well as colliders, heightmaps, nav meshes, and the like.</p> <p>Second, we have the \"Biome,\" which is associated with the (generally biological) details of an area. Examples here would be \"swamp\", \"sea\", \"lake\", \"desert\", \"barren\", \"forest,\" and many more.   Unity's terrain objects include these as the \"trees\" and \"details\" components.    Note that there\u2019s some fuzziness about \u201cbiological\u201d here.   Things like small rock formations, scattered stones, stalactites and stalagmites, and other \u201csmall mineralogical\u201d things aren\u2019t biological, but they\u2019d generally be handled in a manner similar to the locating of plants, and hence lumped into the biome.</p> <p>\u201cWater\u201d (or \u201chydrology\u201d) is a weird component which could be considered either geology or biome, both, or entirely independent. How a game or simulation deals with it will largely depend on whether water is a \"decoration\" or an interactive environment of its own.   Unity itself shares this ambiguity, and has generally just given up on implementing water as a standard part of the engine in the early-2020\u2019s versions of the engine.   We\u2019ll consider it separately just because many implementations are likely to.</p> <p>For example, consider this screenshot from Bethesda Softwerks's Skyrim:Enhanced Edition.<sup>1</sup></p> <p></p> <p>The shape of the world around us would be the geology, including the terraced rocks, sloping shores, and even the (underwater) river bed.     The river itself is of course water, and most of the rest of what we see would be the biome:  the trees, the grass, the large stump on the opposite shore, and maybe even some of those smaller rocks areound the river.</p> <p>It's common in games today to combine the two (usually by including geology as part of the biome), and certainly they're often overlapping concepts.  But keeping them separate helps is in a couple of ways. First, they're handled at very different parts of the terrain generation process.  Second, being able to combine and apply different combinations lets us re-use components to create flexibility.   For example, a relatively flat terrain might be a swamp, desert, moor, plain, wheatfield, or several other things depending on which biome it\u2019s paired with.  Similarly, a high-altitude biome might be reasonably applied to multiple \u201cmountain\u201d geologies and even things like high meadows.</p> <p>Any categorizations we use are likely to be fuzzy, and when we encounter such ambiguity, we'll likely just default to \"whatever makes the implemenation easier.\"   For specific examples:</p> <ul> <li> <p>Are the smaller rocks along the river shore part of the geology, or part of the biome?   The answer might depend on our game engine, but if we were  implementing this in Unity, we could be reasonably certain that the larger \"shape\" of the terrain would be implemented as a terrain object (or some similar standin, see the later discussions of voxel worlds) and hence geology, and the \"lumpy\" rocks would be details built in (as would any place where the rocks overhang the world below, since a Unity terrain object cannot represent overhangs).</p> </li> <li> <p>More generally, how about the grass and small plants?   There's two types here.    The more interesting (to the player) kind that has three-dimensional shape (even if it's represented by a billboard), and the kind that's just \"painted\" onto the ground.    They're implemented in most game engines by at least three different subsystems: </p> </li> </ul> <p>textures    : which make up the \"paint job\" of the actual polygons</p> <p>billboards    :that show a two-dimensional illusion of three dimensions by insuring the textured face always faces the player.  These are used as a way to save valuable polygons by not building 3d meshes of objects that are unimportant, vast in quantity, or distant</p> <p>meshes    :that represent things that need 3d physicality in the world (and themselves are textured).   This would be things like rocks on which the player can sit or stand; trees they can climb, cut down, or must avoid; and really any non-terrain object that the player needs to be able to see from all sides.</p> <p>(Note:  Playing with markdown, here's the same thing as a table!)</p> Type Unity Type Description Texture Sprite/Material The images used to paint the polygons of a mesh Billboard Mesh + Material a 2-dimensional illusion of a 3-dimensional object, where the single polygon face (usually a quad) faces the player at all times Meshes Mesh A 3D object made up of vertices, edges, polygons, and textures (and possibly other elements) for drawing an \"object\" in the virtual world <p>It is not atypical for a single \"object\" to be represented at various times by all of these methods.   A forest sufficiently distant that a player is unlikely to be resolving (or caring about) individual trees may well just be a \"trees texture\" painted on the terrain itself.   As the player gets closer, the trees will individualize as billboards, and as meshes when the player gets closer still.    Modern game engines have mechanisms to manage this magic, reducing the \"jumping\" as billboard suddenly becomes a mesh as the player approaches.</p>"},{"location":"Background/#unity-terrain-gameobjects","title":"Unity Terrain GameObjects","text":"<p>The Unity engine provides a high-level game object called \u201cTerrain.\u201d    Unity Terrain objects consist of both geology (represented as a heightmap), and biome (represented as a \u201ctrees\u201d collection and a \u201cdetails\u201d collection).   It does not handle water directly (and since about 2019 Unity has not included the \u201cStandard Assets\u201d collections that used to provide the easiest implementation), so water features are imposed as separate game objects, usually as one or more geometric volumes or a simple \u201csurfaces\u201d at a particular altitude, depending on the needs of the game. </p> <ol> <li> <p>Note that Skyrim is not a Unity game (and in fact, most of the examples in these early chapters aren't), it's just used as an example of the types of things we might like to build.    Skyrim is built using Bethesda Softwerks's Creation Engine.\u00a0\u21a9</p> </li> </ol>"},{"location":"Requirements/","title":"Requirements","text":"<p>If the goal is to build something, it helps to know what we're trying to build.   I've called this section \"Requirements,\" but this is an experimental project, so what we're listing here are really \"aspirational goals.\"   Not all of these may be achievable, reasonable, or even technically possible.   But we still want to describe what an \"ideal\" system would look like.</p> <p>For the moment, we're going to consider just Geology (that is, just the shape of the terrain, not what's on/in it.</p>"},{"location":"Requirements/#target-software-platform","title":"Target Software Platform","text":"<p>Unity3D initially.   Presumedly many of the concepts would move to other engines (particularly Unreal), but we're going to take advantage of whatever technologies Unity offers us without much concern for whether or not they're portable.</p>"},{"location":"Requirements/#scalability","title":"Scalability","text":"<p>Our first requirement is that this system be scalable to different hardware.    Our specific target is 90/90/90: 90 frames per second 90% of the time on 90th percentile gaming hardware (that is, a PC gaming system which can outperform 90 percent of all PC game systems.)</p> <p>Why exclude 90% of PCs?   First, because the tail on systems is very long:  See Steam's System Surveys here for samples.   The bottom half or so of these systems aren't playing modern games much.  But more importantly:</p> <ul> <li> <p>That's more or less where my current system sits, so it's the most practical one to target.</p> </li> <li> <p>We're building engine-level code rather than a game, so even if this gets widely adopted, there's a long lead time between \"now\" and when the first games will be available, and the the speed of game systems increases quickly.</p> </li> </ul> <p>However, we'd like the system to scale to lower level hardware; we'd like at least 60fps on current generation Unity-capable consoles or Apple Silicon Macs, and ideally at least 30fps on late model Intel Macs and reasonably capable mobile devices (say, everything iOS and the \"top tenth\" of Android - basically flagship class devices, not the sub-$250 ones that make up the bulk of the Android world).</p>"},{"location":"Requirements/#world-size-and-variety","title":"World Size and Variety","text":"<p>Ideally we'd like to be able to generate infinite worlds.   As an initial goal, we'll aim for a generation of a one million square kilometer area (1000x1000 km) as the proof of concept.   Our generated space should contain as high a variety of geomes as possible--at least sea, mountain, hills, plains, and riverbeds.    We will include \"sea level\" water, but for the moment not worry about higher elevation, emitted, or moving water (any lakes or rivers will be \"dry\"), since Unity's support for such things is in flux right now.  </p> <p>The world should be capable of having undercut erosion (i.e. overhangs and natural bridges) and cave systems of arbitrary complexity, but we'll assume that these systems are (to within an order of magnitude or so) similar in number to their real-world counterparts.   That is, such features will be relatively rare; comprising a very small percentage of terrain space.</p>"},{"location":"Requirements/#configurability","title":"Configurability","text":"<p>The generation of worlds should allow the generator to control limits and biases.  For example, there should be options to set the height of the highest mountains and the sea level, determine whether undersea features are present or not, determine the frequency of biomes/geomes in the world, and similar things.    In addition, we'll want to allow the option to \"gamify\" the environment, things like:</p> <ul> <li>Number of caves increased, and caves made larger so as to be traversable by walking humans.</li> <li>A tendency for caves or spaces behind waterfalls</li> <li>An increase in (or even mandate for) natural \"switchbacks\" and \"collapsed slopes\" that allow access to most terrain without climbing or high-jumping mechanics (e.g that you can \"walk\" to most of the world, even mountain peaks and the like).</li> <li>Vertically spaced \"handholds\" for most steep surfaces (for games with climbing mechanics).</li> <li>More regions of increased verticality in general (canyons, badlands, cliffs, other eroded or uplifted spaces)</li> <li>An increase in \"dead end\" type terrains:  places with few or even just one point of entry.  For example: deep canyons, calderas accessed through only one path or cave system, mountain valleys, etc.</li> <li>Region separation: natural boundaries that separate the world into mutually inaccessible (or difficult to access) regions.    This allows the game to control player progression into \"higher level\" areas over time.  Such boundaries will suppress the handholds/switchback options in the boundary areas.</li> </ul> <p>All of these should be able to be independently turned on or off; when they're all disabled, the worlds will tend toward the \"natural,\" independent of player accessibility.</p>"},{"location":"biomes-unity/","title":"Biomes: Unity Implementation Notes","text":"<p>Aside from the Terrain itself, Unity supports effectively three kinds of add-ons:</p>"},{"location":"biomes-unity/#ordinary-game-objects","title":"Ordinary Game Objects","text":"<p>You can place game objects manually or by script in the scene; they act as they do in any other scene and get no particular special handling.   We'll use this for most things that are meant to be dynamic:  monsters, treasures, weather effects and the like.    It's also necessary for objects that the player will interact with physically:  for example, large rocks that the player needs to stand on or be blocked by, as opposed to just passing through.</p>"},{"location":"biomes-unity/#details","title":"Details","text":"<p>The detailmap in TerrainData allows you to place simple meshes or billboards into the scene.   They are optimized for efficient drawing (so you can have hundreds or thousands of them), aren't drawn at all beyond a dev-selectable distance from the user, and have no colliders--even if there are colliders attached to the mesh prefabs.   So these are fine for small objects, or for things like tall grass and grain that the player is meant to pass through, but it doesn't work well for things like large boulders, which need to be placed either as trees (if a capsule collider is sufficient) or as ordinary objects.</p>"},{"location":"biomes-unity/#grass-details","title":"Grass Details","text":"<p>Details with a render mode of \"Grass\" can be moved to the GPU (on all platforms other than WebGL, and custom shaders permitting), and will gain the ability to blow in the wind (but lose the healthy vs. dry coloration options.)</p>"},{"location":"biomes-unity/#trees","title":"Trees","text":"<p>Another optimized layer on the TerrainData allows placement of trees.   Tree objects are rendered as meshes near the player and billboards at a distance, with the exact functionality being dependent on the tool that created the tree.  A setting on the terrain data allows trees to generate simple capsule colliders to make the trees \"solid\" against attempts to walk through them.   Non-tree meshes can be placed as trees so long as they reasonably act like them (in particular, their anchor points are on the bottom) and the simple capsule collider is good enough for interactions.  If they need a real mesh collider (most things will if a player is meant to stand on or against them), they need to be placed as ordinary objects.</p> <p>Unity supports two basic tree modeling sources (although you can make basically any mesh a \"tree\" if you want to.).  There's a built-in tree editor that makes \"static\" trees (no LOD/billboard support, no wind handling) and a third-party tool called SpeedTree which builds more flexible (in both senses of the word) models with wind and distance optimizations.</p>"},{"location":"biomes/","title":"Biomes","text":"<p>In our parlance, a biome is the physical characteristics of a terrain that aren't its basic shape.   Practically speaking, that is:</p> <ul> <li>The base textures of the ground (TerrainLayer objects)</li> <li>Imposed structure (\"shape\" that's specific to this biome, like pools in a swamp, spires in a badland, dunes in a desert, cave entrances, flattened regions for buildings, etc.), particularly at the local or regional level.</li> <li>Vegetation (grasses, bushes, trees) and small mineralogical (rocks, stalactites) details</li> <li>Water and water behavior.</li> </ul> <p>Buildings and other sentient-being-constructed things may also apply here.   Let's ignore that for the moment, since it rapidly leaves \"terrain generation\" and heads toward \"game design.\"</p> <p>We're also going to ignore water-mostly.   Some of it we can treat like any other biome, placing corals, rocks, kelp \"trees,\" and the like.   But water has all sorts of other gameplay effects (gravity, visual distance, opacity) and visual characteristics (surface appearance, caustics, light filtration) that are going to merit their own treatment later.</p>"},{"location":"biomes/#biome-maps","title":"Biome Maps","text":"<p>Early versions of Minecraft used to have fairly strict delineations between biomes:  you'd be in the desert, and a dozen steps later you'd be in the plains.  There was a little bit of \"mixing\" right at the edges, but for the most part where one biome ended, another would begin.  You can see the boundaries of 3-4 different biomes here (more if you count the ocean.)</p> <p></p> <p>Valheim hides it better, but there's still no overlap of biomes.  Note the fir trees and deciduous trees don't \"mix\" at all:</p> <p></p> <p>Real worlds aren't like that (and as Minecraft has evolved, it's started blending, a little).  Changes between biomes are often so gradual that a traveler doesn't really notice them at all.  And even in the middle of, say, an arid grassland, there may be the occasional patch of scrub forest from the a neighboring slightly wetter biome.   There are also places where biome changes are sudden, usually because of the presence of either drastic elevation changes or water:  a desert oasis or the green vegetation around a river or pond in an otherwise dry landscape, for example, or the even starker \"tree line\" on mountains above which nothing grows.</p> <p>Part of this is just thinking of \"biomes\" as being discrete things: no two deserts are the same, and no two places in the same desert are generally the same, either.   Biomes, in the computer generation sense, are a shorthand meant to increase practicality, at some cost to realism.   But even so, our model shouldn't think of a place as having \"a biome\", but rather being affected by \"this set of biomes\" to different degrees.</p> <p>So ideally we'd like to have a floating-point map for every biome we support, so we can look at a specific tile and say things like \"this tile is half underwater, a quarter beach, and the rest rocky cliffs\", or \"50% desert, 50% arid grassland\" when we're generating the actual tile.</p> <p>Is that practical?  Maybe.  A roughly million-meter-per-edge map (1024x1024 meters) using 256x256 terrain patches is probably a good case for the largest practical map we can generate.   That's 4096x4096 total tiles, which at a 32-bit floating point value per tile is 64 MB per map, and we'd need one of these per biome type.   Add a little bit of overhead, and we can probably expect to store about 15 biome maps per gigabyte of ram.</p> <p>How many biomes are we looking at?  The Internet tells me that Minecraft has about 60.   Valheim has about a dozen.  No Man's Sky has nine, but uses a trick we'll see below to effectively have much more.   All of these are much lower than the real world.  The total number varies based on who you ask:  NASA says 7, various sources say 6, others say 12.  But the one thing they all have in common is that these \"real world\" numbers are using a vastly more inclusive definition of \"biome\" than a game would.   For example, they lump all \"temperate forests\" together, regardless of tree species and other differentiators -- nearly all of the eastern United States is a single biome in that sort of categorization.   The root cause of this is that the ecological definition of \"biome\" differs a lot from the game-development definition.   For our purpose, I'm going to make a back-of-the-envelope guess that \"Earth\" has between two thousand and five thousand different game-style biomes; and frankly, I might be low by orders of magnitude.</p> <p>Nobody but the most dedicated game developer is likely to generate anywhere near that, but 100 biomes doesn't seem out of the question.  That's almost 7 GB...just for biome maps, and not counting things like the terrains themselves, which can be similarly sized.   Other than a fairly high-end gaming PC, that's going to be pretty prohibitive:  the Playstation 4 had only 8Gb of RAM, and even it's successor has only 16.   Many iPads have less than 4GB (note that this is RAM, not storage.  Apple doesn't usually publish the RAM number, so it can be hard to find for any specific model).  Android tablet RAM can be larger--or much smaller.</p> <p>So, practically, we can't do this.   What are some options?</p>"},{"location":"biomes/#smaller-maps","title":"Smaller Maps","text":"<p>The above calculation is \"worst case\" in the sense that it's the largest we're likely to generate while having the Global Terrain Template still fit in a Unity Terrain (even at 4096x4096, Terrain objects aren't very performant except on very powerful systems).   We can do the math for some other combinations, of either smaller total worlds or larger patch sizes:</p> World Size (meters) Patch Size (meters) GTT Tiles Map Size (1 float/tile) Biomes/GB 1,048,576 (~1M sq km) 256 4096x4096 64 MB ~ 15 1,048,576 512 2048x2048 16 MB ~ 60 1,048,576 1024 1024x1024 4MB ~ 250 262,144 (~64K sq km) 256 1024x1024 4MB ~ 250 262,144 512 512x512 1MB ~ 1000 262,144 1024 256X256 256 KB ~ 4000 <p>That gives is a nice set of options we can provide to the developers:  based on how many biomes you want to have and the size of your memory constraints, here's how much map you can have.  Even the smaller map is about half the size of England, so large enough for many games.</p>"},{"location":"biomes/#compress-the-maps","title":"Compress the Maps","text":"<p>As our number of biomes increases, more and more of our biome maps are going to be sparse.   That is, most entries in the (say) 4096x4096 array of floats will be zero.   For worlds with only one sea biome, that particular map will have a lot of \"1f\" in it, and a lot of \"0f\" in it, and only the shorelines will have any other value.</p> <p>Whenever you see \"lots of the same value,\" and especially \"lots of the same value in large regions\" in an array, that screams \"compressibility.\"   Any decent lossless compression algorithm will do wonders on making these smaller; that 64MB biome map we were describing above would probably compress to 5-10K in most cases.   Whether or not this is practical will depend on how often and how much effort it takes to uncompress them when we need them.</p> <p>Another compression possibility is to not use a map of the whole world, but rather just the bounding boxes of each particular biome (and if a biome appears in, say, 8 places, store 8 much smaller bounding boxes).  This eliminates much of the empty space while adding just a bit of complexity to the data structure and time to the lookup.</p> <p>And we can let someone else do much of that work for us:  there are numerous implementations of \"sparse arrays\" out on the Internet; these are data structures that effectively omit the zeros blocks and just store the elements with values in them.  There are different implementations with different tradeoffs.</p>"},{"location":"biomes/#dont-store-them-in-ram","title":"Don't store them in RAM","text":"<p>For many platforms, 7GB is no big deal as long as it's not in memory.  Hard drive and SSD flash memory is relatively cheap, and most non-mobile platforms have lots of it available.   Even on mobile, a 10-15 GB game (or larger) isn't unheard of, although it's going to limit your potential audience.</p> <p>So if we just write these maps to disk immediately on creating them, and load them back when we need them just a few at a time, we can trade off disk access for memory.</p> <p>The downside here is that loading from spinning disks can be extremely slow, and even from SSD is usually a few hundred times slower than memory.   And we're going to need these maps a lot -- the simplest algorithms are going to require looking at every biome map (or at least one cell of every map) at the creation/loading of every single terrain patch.</p> <p>That \"one cell of every map\" implies that if you want to go down the rabbit-hole of memory-mapped files (where a file's contents are assigned to a set of otherwise unused memory addresses, and the file acts like a sort of specialized swap file), you'll need to make very few actual memory page loads: generally just the pages that hold the single value for the tile you're interested in, and because of locality (the player will generally be moving to nearby tiles), you'll hit cache a lot in most real-world usage.</p>"},{"location":"biomes/#allow-biomes-to-self-differentiate","title":"Allow Biomes to self-differentiate","text":"<p>This is a technique No Man's Sky uses:  let a \"biome\" describe a set of options rather than a particular specific set of characteristics, and then choose a specific value of those options for each instance of that biome occurring.    For example, we could use something like the \"temperate forest\" biome, but give it several options for \"dominant tree species\"  (oak, aspen, pine, fir, maple, etc.), base texture, undergrowth, density, etc.</p> <p>Then, using some value available at runtime ((x,z) position, wind or moisture levels. perlin-class noise, elevation, temperature, whatever), we select a specific set of those options for each area of the biome.</p> <p>This mechanism allows us to compress a lot of \"specific selection\" biomes into one \"smart biome,\" effectively by bringing the implementation of \"biome\" closer to the natural science definition of the word.</p> <p>There are two tradeoffs here.   The first is complexity for the developers that need to create these biomes.   They need to understand and implement the options available, as well as build separate biomes any time the various \"options\" can be incompatible with each other. (For example, if certain undergrowth can't occur with maple trees, then they either need to be separate biomes or we need to build in exclusion mechanisms.)   The opposite is true as well:  it seems perfectly reasonable for a pine forest and an aspen forest to overlap (both are cold-tolerant, high-altitude trees), but since we're using the same biome for both, the biome needs to always allow both types of trees or the overlap is impossible without moving one to a second biome type.</p> <p>The second tradeoff is on us:  we need to make sure that whatever value we use for the \"select a set of options\" index is consistent (or smoothly varying) over the entire region of a biome (e.g. even across multiple tiles), in order to prevent our \"pine forest\" from abruptly turning into a \"maple forest\" at a tile boundary.</p>"},{"location":"biomes/#whats-this-biome-thing-anyway","title":"What's this \"Biome\" thing, anyway?","text":"<p>The last option we'll consider here is eliminating the notion of \"biome\" altogether.  Biome is a shorthand for a set of specific options:  ground textures, vegetation species, etc.   If we talk a step back a bit, what we're really saying is \"these things tend to occur together.\"   Our human tendency is to group and name categories:  that's a \"swamp\" not \"an area with shallow pools, a lot of mud, snakes, alligators, noxious gas clouds, and quicksand.\"    So, it's natural for us to try and place \"swamps\" in our world.</p> <p>But the alligator doesn't care that he lives in a \"swamp.\"   He cares about having deep enough waters to float in, moving slowly enough to conserve his swimming energy, enough prey to eat, temperatures warm enough not to cause torpor, and enough overhanging vegetation to create shaded places to hide or sunbeams to bask in.   If you provided all those things, but called the result a \"desert,\" the gator would still be happy there.  A rose by any other name, and all.   The snake may have similar desires, but with fewer moisture and water constraints, so it can live in more places.</p> <p>Unlike the real world, a game world won't have millions of animal species, tens of thousands of plant species, and an infinite variety of rocks and minerals.  All of them together are unlikely to top a few hundred, at least not if you ever give your artists any time off.   The number of ground textures might be a little higher, but probably under a thousand of those, too.</p> <p>So if each of those plants, animals, minerals, and ground textures, has a detailed set of dependencies (temperature, moisture, animal and vegetation density, groundwater, sunlight, elevation, daylight/nighttime hours, latitude, underground or underwater, proximity to open water, etc. etc.), and our world is sufficiently defined that we can determine whether any given tile (or even smaller region, like an oasis or lake) matches those dependencies (and maybe how well it matches them), well, what do we we need biomes for?  And if we need to differentiate them even further, we can create artificial dependencies, like requiring a certain minimum value on a Perlin-class noise field as well as everything else.    </p> <p>There are difficulties:  We may be implicitly or explicitly forcing an order to the generation of tiles: for example, if pandas require bamboo, or koalas eucalyptus, then we need to make sure that plants are always placed before the animals.  Plants, in turn, might depend on certain base terrains (it's atypical for a tree to grow on rocks).   But as long as we avoid (or resolve) circular dependencies, this should be workable.</p> <p>In some sense, this would be trading one set of maps for another:  we now need to keep track of a lot of different attributes of our world in order to answer the various dependency questions.  But consider:</p> <ul> <li>The total number of these attributes is likely to be much smaller than the number of biomes.   For example, just four floating-point attribute values would give us potentially millions of specific combinations, even if we didn't require very high floating precision.</li> <li>Several of these (elevation, moisture, wind) we're storing anyway, since we need them in other parts of the generation engine.</li> <li>Others--like temperature, latitude, and sunlight--can be calculated in whole or in part from combinations of other variables.</li> <li>And several of these maps are pretty slow-changing with respect to distance; we could probably keep them at lower resolution than the heights.</li> </ul> <p>An advantage of the \"no-biome\" solution is that it makes nearly all of these decisions local:  we make the maps once at world creation, but the actual placement of stuff happens on patch load at runtime.</p>"},{"location":"biomes/#self-generating-biomes","title":"Self-Generating Biomes","text":"<p>So let's start by trying out the no-biome mechanism.  Instead, we'll just give the various things (textures, details, trees) that would normally be part of a \"biome\" a set of dependencies, and let the \"biome\" behavior arise as a sort of emergent behavior.</p> <pre><code>[Serializable]\npublic abstract class GenerationDependentObject : ScriptableObject\n{\n    [Tooltip(\"Relative commonness (lower numbers = rarer) of this entity.\")]\n    [SerializeField] public int bias = 100;\n\n    // Dependencies\n    // Sea (note that shallow water isn't considered sea, but \n    // does have a 100% moisture level)\n    [Tooltip(\"If true, this entity appears in deep waters.\")]\n    [SerializeField] public bool undersea = false;\n\n    // 0 - .9 are the standard land range.  Items above 1 appear only in water.\n    [Tooltip(\"Minimum moisture level required for this entity. 0-.9 for land, 1 for pond/river/shallow water.\")]\n    [SerializeField] public float minMoistureLevel = 0f;\n    [Tooltip(\"Maximim moisture level required for this entity.\")]\n    [SerializeField] public float maxMoistureLevel = .9f;\n\n    // Elevation.  Real range is 0 - WorldGenerator.maxHeight\n    [Tooltip(\"Minimum elevation (meters) for this entity\")]\n    [SerializeField] public int minElevation = 0;\n    [Tooltip(\"Maximum elevation (meters) for this entity\")]\n    [SerializeField] public int maxElevation = 999999;\n\n    // Wind. Range 0-1\n    [Tooltip(\"Minimum average wind likelihood\")]\n    [SerializeField] public float minWind = 0f;\n    [Tooltip(\"Maximum average wind likelihood\")]\n    [SerializeField] public float maxWind = 1f;\n\n    // Slope\n    [Tooltip(\"Minimum slope (absolute value of degrees) for this entity\")]\n    [SerializeField] public float minSlope = 0f;\n    [Tooltip(\"Maximum slope (absolute value of degrees) for this entity\")]\n    [SerializeField] public float maxSlope = 90f;\n}\n</code></pre> <p>GenerationDependentObject here will be the base class for anything that has dependencies.    We're almost certainly going to need more attributes to be dependent on, but this gives us a good first set.</p> <p>If you're unfamiliar with Unity, the \"Serialize\" and \"SerializeField\" attributes just mark the class and the fields in it (respectively) as being able to be handled by the built-in serialization system.  That is, they can be turned into some intermediate object and restored from it, often used for save/load mechanisms.    It also makes the fields visible in the Unity editor when inspecting an asset of this type (not strictly necessary on the fields, since all public fields also appear in the editor by default, but we're likely going to need to serialize these, and it's standard for ScriptableObjects.)    The \"Tooltip\" attributes, as you might guess, provide the tooltip \"hint\" when the user hovers over one of these in the editor.</p> <p>The key here is deriving from ScriptableObject, GameObject's oft-misunderstood cousin.   This is a distinction worth its own page; see unity root classes for more details.</p> <p>GenerationDependentObject is an abstract class (you can't instantiate it directly, only a subclass).   So let's make a concrete subclass of it to hold terrains:</p> <pre><code>[Serializable]\n[CreateAssetMenu(fileName = \"BiomeTextureData\", menuName = \"WorldGeneration/BiomeTextureElement\", order = 1)]\npublic class BiomeTextureElement : GenerationDependentObject\n{\n    // Base element texture\n    [SerializeField] public Texture2D baseTexture;\n\n    // Optional normal maps and mask maps.\n    [SerializeField] public Texture2D normalTexture;\n    [SerializeField] public Texture2D maskTexture;\n}\n</code></pre> <p>That \"CreateAssetMenu\" attribute is a key to editability here:  it creates a base menu called \"WorldGeneration\" in the \"Create\" menu that appears when you right-click in the Assets, and a specific \"BiomeTextureElement\" entry on that menu.</p> <p>Whenever I want to create a new texture, I right-click in the Assets, select WorldGeneration-&gt;BiomeTextureElement, and a new BiomeTextureElement asset is created.  I can use the inspector on it like any other asset, in this case to set up to three textures (the baseTexture is mandatory, the other two are optional), as well as all the attributes inherited from GenerationDependentObject.</p> <p></p> <p>In this particular case, the sample texture I've got doesn't have a normal or mask map, so I just set the base texture.  Most of the rest of the fields are self-explanatory, but let's talk about bias for a bit.</p>"},{"location":"biomes/#bias-and-rarity","title":"Bias and Rarity","text":"<p>This is a trick I use a lot for \"choose from a list of items of unknown length at random\" type problems, which appear extremely often in game design.  The idea's certainly not original with me.</p> <p>The idea here is that you give every potential element a \"Bias\" field, which is sort of an inverse of the idea of rarity.   A higher bias means this object is more common than if it had a lower bias.</p> <p>The algorithm is pretty simple:   First, we filter out the objects (terrains, monsters, trees, whatever) that don't match our criteria.    From whatever's left, we sum the biases, choose a random number less than that sum, and then remove each item's \"bias\" from that random number one at a time until there isn't enough random number left to subtract, at which point the object we're currently looking at is selected.</p> <p>As an example, let's say we've got five items, with these rarities (Hat, 1) (Glove, 3) (Sword 3) (Magic Toothpick 1) (Health Potion 5).</p> <p>Those biases sum to 13, so  we pick a random number from 1-13.    Let's say that random number is \"5\", which becomes our target:    </p> <ul> <li>\"Hat\" has a rarity of 1, which is less than our target number, so we subtract 1 from the target to get 4, and discard \"Hat\" as an option.   </li> <li>Next, \"Glove\" has a rarity of 3, which is still less than our target 4.   So we discard \"Glove,\" subtract 3 from our target number (giving us 1), and look at \"Sword.\"    </li> <li>\"Sword\" has 3 rarity, which is higher than our remaining target (1), so we've selected \"Sword\" as our item.</li> </ul> <p>Most notably, biases are unitless (or maybe more accurately, they define their own units).  They tell you the relative rarity of each possibility compared to all the others, on whatever scale you want.    For example, if our intrepid adventurer opens a lot of chests, in the long run, we expect him to have received five times as many Health Potions as Magic Toothpicks.</p>"},{"location":"biomes/#base-terrains","title":"Base Terrains","text":"<p>You'll notice that the name I gave the example BiomeTextureElement, above was \"Undersea Base.\"   I don't mean that in the sense of a place for supervillains, but rather that this is the base texture for undersea terrains (Undersea is checked.)   I've got a corresponding \"rock\" terrain that is the \"Land Base\" texture for non-undersea terrains.</p> <p>By \"Base Texture,\" I just mean \"the texture that will get selected if nothing more specific fits.\"   In this case, the rest of the dependencies define the entire range:  these \"base\" textures can apply to any slope, elevation, moisture level, etc..  They have a bias of (1), which means that no matter what scale we end up choosing, these will be the rarest textures.</p> <p>We're likely to have a lot of BiomeTextureElements.  The more we have, the more varied our world will look.  Some will have broad applicability, some will have very narrow acceptable dependencies.   As we add more content to our world, the set will increase, and in theory, we'll have nice terrain elements that match any combination, and hopefully we'll have several textures that match any actual combination, so that every square of a given terrain patch doesn't end up looking similar to every other.</p> <p>But...if we mess up, if there's a terrain location that doesn't match any set of dependencies, at least it will match this one (or rather, either the land or sea one), and get a reasonable default (sand underwater, rock above).</p>"},{"location":"biomes2/","title":"Biome Elements","text":"<p>Continuing with our \"self-creating\" biomes experiment, we derive four classes of object from GenerationDependentObject.   These might seem to be arbitrary distinctions (and to some extent, they are), but they were chosen to match specific features in Unity:  See Unity Notes for some information on these implementations.</p> <ul> <li>BiomeTextureElements are the base terrain textures (the \"flat\" area that is intended to represent grass, rock, snow, mud, whatever) that underly everything else.</li> <li>BiomeTreeElements are Trees and other simple-collider-based solid objects placed into the Unity \"trees\" layer of the terrain.   If the tree models are built for it, they support structural variation, size variation, and automatic level of detail handling (in Unity, that means billboard -&gt; model transformation at a specified distance from the camera).  They can also (again, model permitting) bend and blow in the wind. </li> <li>BiomeDetailElements are placed into the Unity \"details\" terrain layer.   They are objects which do not have colliders (that is, the player can walk right through them as though they weren't there, and they don't participate in physics.).   We're using them primarily for grasses and small plants, and some underwater stuff like corals.</li> <li>BiomeFeatureElements are whatever's left.  These are \"real\" mesh objects.   We use these for large rocks/rock formations and other features that the player needs to physically interact with, and therefore need more sophisticated colliders (often mesh colliders) to allow the player to step on, climb, or whatever without visual artifacts.   This is a concession to playability:  if your application doesn't require a player or other element to interact with it directly (for example, a \"fly-through\" scene in a movie), \"Features\" can just become \"Details\" or vice versa.</li> </ul> <p>Uh, About Those Models:  From here on out, we're going to start needing a lot of models of things (plants, ground textures, trees, rocks, maybe even things like animals) to fill in our world.</p> <p>I have a level of artistic talent comparable to a not-particularly-precocious five year old.   Much to the disappointment of everyone who's ever tried to teach me artistic abilities, I could probably reduce Bob Ross to tears.</p> <p>Up until now, I've been using texture packs from Unity itself; either stuff that's built in, or available from the Asset Store in the sort of packages that used to be called the \"Standard Assets.\"    I'll continue to use those, but they don't give a sufficient breadth of models for much of a varied world.    Nevertheless, if you're looking at the WorldForge source code, that's all that's going to be used in the sample biomes there, because it's all that I can be sure you (dear reader) have access to.</p> <p>For this documentation, though, and my own use, I'm extending this with assets from numerous professional artists.  These are obtained through the asset store, various sources on the Internet, and probably other third-party sources.    So for the elements of the world that are not procedurally generated, you'll start seeing more variety in the screen shots.</p>"},{"location":"biomes2/#initial-implementations","title":"Initial Implementations","text":"<p>Once we define a handful of these Biome Elements of each type and provide them to our world generator for selection, we start getting some more interesting landscapes:</p> <p></p> <p>The underlying textures here (visible in the \"bare patches\") are texture elements, the sunlit rock is a feature, the grass and heather are details, and the various trees are...well, trees.   Some of the closer trees are models, the more distant ones are billboards for performance.</p>"},{"location":"biomes2/#terrainfeature-mismatches","title":"Terrain/Feature mismatches","text":"<p>There are some issues here.    One of them isn't visible from this viewpoint, but looking at another shot where the woman is relatively higher:</p> <p></p> <p>That \"bare ground\" in the distance is the \"detail render distance\" being exceeded.   This is a performance setting present in most games/engines that prevents the rendering of vast numbers of meshes.</p> <p>Note that there is ground cover over in that bare area:  Unity just isn't rendering it for performance reasons.   And that makes sense, given that in the screen shots shown here, there are over 250,000 individual \"patch of grass or flowers\" models per terrain patch.   Since we have adjacent patches, as well, there are probably several million ground cover models potentially available to be rendered.</p> <p>(It's worth noting that Unity does extremely well with this vast amount of data:  on a 2022 Mac Studio, I'm still getting over 200 frames per second with the detail distance set to 100 meters.)</p> <p>The general idea is that players don't need to see explicit models for small elements that are very distant from the camera.   If the game has \"fog\" limiting the player's vision, it's usually set so that the vision distance and the detail render distance are similar.    For more modern games with longer visual ranges, the expectation is that the ground cover will be captured in textures at range.</p> <p>You can see a related issue with this shot from above, looking down:</p> <p></p> <p>The problem here is that the texture engine picked a rough, rocky soil as the texture for this area, which doesn't blend very well with the plants growing from it.   This has several issues:</p> <ul> <li>The texture being so different from the ground cover makes the detail draw distance very obvious.</li> <li>The texture being so different from the ground cover makes the ground cover look \"thin\" unless we make it very, very dense (which has its own performance issues).</li> <li>You don't get significant ground cover growing out of hard-packed, dry rocky soil in the real world.</li> </ul> <p>The logic here was sound:   both the ground cover and the ground texture are both appropriate for the level of moisture, wind, elevation, etc. at this location in the world\u2014they're just not appropriate for each other.</p> <p>From the images above, you can also see that ground cover continues under the trees, as well, even when those trees are in small groups.   Real-world trees tend to reduce the grasses and such directly beneath them (by blocking sunlight and absorbing water and nutrients.).   So the ground cover and the trees probably need to \"know things\" about each other, too.</p> <p>So a first takeaway here is that we need to find a way to make different types of elements combine into more logical pairings, ideally without having to resort all the way back to predefined \"biomes.\"</p>"},{"location":"biomes2/#clustering","title":"Clustering","text":"<p>Let's look at this screen shot again.    The trees here are fairly uniformly spread out (the 3-trunk tree groupings here are a single model and placed as a unit).    But real trees don't do that.   This is too sparse to be a forest, but even sporadic trees tend to cluster (often around water) rather than this sort of uniform distribution.</p> <p>On the other hand, the ground cover is too uneven.  Certainly you see patchy cover like that sometimes in the real world, but there's a reason why people talk about grasses using \"blanket,\" \"sea,\" and \"wave\" metaphors.    Usually ground cover is fairly even, at least across large areas that are uniform in lighting, moist, and elevation.</p> <p>In both cases,  the visuals above are a result of a more or less \"pure\" random distribution.</p> <p>There's not enough variety in the trees here yet for it to matter, but there's also a sort of \"species clustering\" that occurs in nature, as well.  Forests are often primarily of a single species of tree, and even in mixed-species ones, they tend to cluster together in groups of single species rather than a very uniform mix.</p>"},{"location":"biomes2/#density","title":"Density","text":"<p>We also need to consider the total number of elements in a given space (in our case, the \"terrain patch\" makes a good unit of area).   The above implementation just uses some uniform \"Tree Density\" and \"Detail Density\" numbers from the world generator, but we're going to want to control these on a much more granular level.   And it applies across all the element types:  a forest has far more trees than a scrubland, which in turn has less than a tundra, seashore,  or desert.    But that seashore likely has more large rocks (as would a mountainside, for different reasons).   The numbers of almost any plant are going to vary based on moisture levels, especially in combination with winds.    Even for something as simple as a single tree species, we're likely going to want to either define several different element blueprints for it (say, \"maple trees in a forest\", \"maple trees in a meadow,\" or \"maple trees along a coastline\"), or give those blueprints a direct way to vary density based on light, elevation, moisture, or whatever.</p> <p>Performance Note:   We'd expect objects with complex colliders and a broader set of materials to be slower for Unity to render, and these experiments clearly show this to be true.    Adding 500,000 grass patch details has less effect on frame rate than a mere 1000 feature rocks (at least for the models I was using).  This is exacerbated by the Level of Detail (LOD) support in Unity:  Terrain-based tree and detail maps have sophisticated and automatic LOD support (billboards, distance culling, GPU instancing, etc.) that's not automatically present for basic prefabs.    So as a specific implementation detail, we either need to keep our total \"feature\" density lower than the others, or build our own LOD system around them.</p>"},{"location":"biomes2/#hard-limit-boundaries","title":"Hard Limit Boundaries","text":"<p>One final issue:</p> <p></p> <p>This isn't distance culling.   Rather, the player is standing near a tile edge, and the tile to the right has a moisture level that's just slightly outside the limit of the ground cover blueprint.  So you get heavy ground cover in the left-hand tile, and none at all in the right hand tile, with a visible line separating them.  (The \"fuzziness\" of the line is because the ground cover models cover a significant area, so they \"overlap\" to different degrees.)</p> <p>So we also need to consider falloff or blending of tile attributes.   Either the tile on the left should have almost no ground cover (because it's so close to the moisture limit that will disallow it), the tile on the right should allow some (because it's close to the moisture limit that will allow it, or just because it knows there's some on the tile next to it), or both.   An \"all or nothing\" hard limit gives us these obvious lines as you cross borders.  You wouldn't expect that on a large world with relatively slow-changing parameters this would crop up very often, but I've seen it happen a bunch of times even in my development to date.</p>"},{"location":"biomes2/#second-look-at-generationdependentobject","title":"Second look at GenerationDependentObject","text":"<p>So, armed with some results, we can go back and add some capabilities to the GenerationDependentObject class, to give us more control over the \"biomes\" generated, and to the generation system that uses it.</p>"},{"location":"erosion-implementation/","title":"Implementing Erosion","text":"<p>So at all scales of our terrain, we want to see the effects of erosion.   Algorithmic generation tends to be either overly blocky or overly smooth to pass for real world terrains--in a single word, it's just too \"uniform.\"</p> <p>Jacob Olsen describes a number of techniques for implementing erosion in his paper here:  https://web.mit.edu/cesium/Public/terrain.pdf, and Dr. de Byl covers the same and others in her Procedural Generation course at Udemy or the Holistic3D web site: https://www.h3dlearn.com/course/procedural-terrain-generation-with-unity.  </p> <p>We'll use some of those.    But we've got a few quirks to throw in the mix.    One is that despite the generally fractal nature of the real world, many of these scale poorly outside of their domains:  the techniques that erode mountains nicely in at something like our global template level don't work nearly as well when we're coming down to individual terrain-patch slopes.   Inversely, things like raindrop erosion make no sense at all on global scales.   The second is that many of these are very expensive, requiring hundreds or thousands of passes over the terrain to build their shapes.</p> <p>We can accept that level of computational expense for the global template generation -- it's likely to take many seconds or even a few minutes to generate that world model, and since for most games it'll be a one-time (or one time per \"new game\") expense, we'll just suck it up.   But our more local levels (regional, patch) need to be generable in real-time.   Terrains can't take more than a few seconds to generate:  after a teleport or on initial load because the player is waiting for them, and in buffer patches because we don't want the player to be able to get close enough to the generating patches to see it when it \"snaps in,\" or worse, to fall off the edge because the new patch isn't ready yet.</p>"},{"location":"erosion-implementation/#landslides-and-shearing","title":"Landslides and Shearing","text":"<p>Nomenclature: Dr. de Byl's course calls these \"Thermal\" and \"Rivers,\" respectively.  In our larger context, both of those terms are likely to cause confusion with other concepts, so I'll stick to the more visually descriptive terms.   Other papers tend to describe shearing as \"hydraulic,\" which I'm avoiding since it describes the mechanism rather than the result, and I'm applying some of these more broadly.</p> <p>These two effects are similar.  In both, material is removed from a higher point and potentially placed at a lower one.   </p> <p>Landslides are the more gradual process, basically a movement of material from steeper points to shallower ones, tending to result in a flattening of the entire structure (think the little movements down the side of a pile of sand to which more sand is being added).   Unless iterated on a lot, this is a pretty subtle effect, generally dwarfed by both any addition of Perlin-class noisse or smoothing.  It's effect is also a lot like that produced by the Gaussian Blur (which also tends to make slopes shallower), while being more expensive to implement.  Because it's so subtle, I don't actually implement it anywhere at the moment.</p> <p>The classic implemenation here is basically:</p> <pre><code>repeat a lot\n   foreach x in xdimension\n       foreach z in zdimension\n           foreach neighbor of (x,z) (often in random order)\n               if (x,z) is \"sufficiently\" higher than the neighbor\n                   remove a little bit from (x,z)\n                   add it to the neighbor\n</code></pre> <p>Shearing is sort of the same effect in reverse, where large pieces of terrain are \"sheared\" away from their parent (in the real world, generally by expanding ice or erosion of softer material) and disappear.  This turns gradual slopes into steep cliffs and drops, separated by flatter (but often still sloped) areas.  This is a geologically sudden event, and is largely what gives young mountains their jagged appearance.   As they age, the landslide effect described above makes the mountains smaller and smoother, eventually wearing them down into mere shallow hills.  In the real world, most of the material is---at least initially---deposited lower down as large debris.  But this debris erodes away much faster than the structure that it came from, so it's often absent entirely below sheared cliffs.</p> <p>The classic algorithm here is a little more sophisticated</p> <pre><code>repeat a lot\n   pick a location (x,z)\n      lower our height by a fixed amount (the \"strength\")\n      repeat a few times (the number of \"rivulets\" or \"cracks\")\n         find a random neighbor that's lower in height\n            set the neighbor to our height\n               then recurse on that neighbor (up to a given depth)\n</code></pre> <p>This basically makes \"cracks\" spread out downhill from a given point, which leaves a sort of field of depressed terrain (with some spikes) behind.   A subsequent guassian blur removes or dampens the spikes.  The net effect is like a flat-bottomed scoop was taken out of the terrain at \"strength\" depth down from the highest point.</p> <p>It looks good, but it's incredibly expensive.   On a steeply sloped 512x512 terrain, even 10,000 iterations is insufficient:</p> <p></p> <p>That screen shot is without the guassian blur, and the terrain took over 20 seconds to generate on my fastest machine (an Apple Silicon M1 Max; it takes more than twice as long on the game PC).   This is basically just a bunch of pits (the blur smooths them out a bit, but it still looks like a bunch of less-sharp pits).   At a guess, getting something natural looking there would take hundreds of thousands of iterations (and some tweaking of the rivulets / strength parameters), or adjusting to make each \"pit\" many, many times larger.</p> <p>Note that this algoritm works fine in environment where the features are relatively small compared to the resolution of the height map:  such as our global terrain template.   But for our detail patches, it's going to be too expensive.</p>"},{"location":"erosion-implementation/#shearing-algorithm-2","title":"Shearing Algorithm 2","text":"<p>We've been in this situation before:  the \"classic\" algorithm isn't working (or rather, it's working exactly as intended, but it's too expensive and finicky for \"unattended\" realtime use).   </p> <p>The real problem here is human judgement:   I'll spare you images of the effects of the above algorithm applied to shallower slopes or plains.  But the general idea is:  we can make it work in each situation, but only by testing out various parameters until it \"looks good.\"   If we want fully procedural generation, though, we can't be tweaking for every possible scenario.</p> <p>So we'll borrow the idea from our canyon-based mountain implementation:   we'll erode in reverse.  We know what we want it to look like when we're done, so we'll start with that and work it backward.</p> <p>So here's the new plan:</p> <pre><code>repeat a few times\n   on our slope, take a large, contiguous area, and push it all down to the lowest height it touches.\nadd a little noise\nsmooth the results a bit.\n</code></pre> <p>And contiguous areas are no problem for us, since we still have a cheap Voronoi-ish algorithm lying around from the inital world generation.   So:</p> <pre><code>generate a voronoi(-ish) map for our terrain\npick a subset of the generated regions\nfor each element of that subset\n   push the whole region down to the lowest height in the region.\nadd a little noise\nsmooth the results a bit.\n</code></pre> <p>For a similarly steep hillside, that give us this:</p> <p></p> <p>That's a lot closer to what we're looking for.   That's using a 50% pushdown (that is, half the regions are \"flattened\" to their lowest point.).  The average region size is pretty big, which translates to a lot of very high cliffs.   Both of these values are easily tweaked.   Better, the algorithm doesn't distort less steep terrains nearly as much.  It's still probably too much to use everywhere, but it's not wildly disruptive.</p> <p>This also has a game-friendly function, in that there are generally \"paths\" up the mountains that avoid the drops, although sometimes you need to go significantly far \"out of the way\" to reach them.   This will make our life a little easier when we get to the \"gamify\" functions, later.</p> <p>This is still pretty unnatural looking:  the slopes are too even, the lines at the edges of patches are too sharp, and since this is implemented independently on each patch, it makes some very weird things happen along the stitch lines.     But, baby steps.</p> <p>With a little more tweaking, looking \"up\" a mountain gives us a sense of how bad the seam-stitching is going to be:</p> <p></p> <p>We can see at least two crossing terrain edges here, one running horizontally across the screen near the top, and the other, more obvious one runing vertically.   In both cases, our stitching does the best it can, but the results are...poor.   Bob Ross's mountains never looked like that!</p> <p>This is the first time we've seen this issue (at least to this extent), but it won't be the last.  Any time we apply transformations to terrains that are independent from the terrains next to them, we have the chance of significant mismatches like this, and the results will typically be the same as we see here:  sharp vertical cliffs that run along the terrain boundaries.    We'll discuss some ways to fix these later.  For now, we're primarily concerned with the erosion algorithm itself.</p> <p>The biggest complaints here are the obvious \"smooth\" slope of the unindented terrain portions, and the sort of global, uniform roundness of the cliff edges.</p> <p>The former is just the relatively smooth pre-erosion terrain showing through.   We'll ignore it for now, and revisit it after we're done with our various erosion methods if it's still a problem.</p> <p>The rounding, though....   It's reminiscent of our issues with Perlin noise.   It's both irregular and too uniform at the same time.  This one's caused by the Gaussian blur.   It takes every edge and sharpens it the same amount.   There's no real randomness here.</p> <p>Of course, total randomness wouldn't be all that useful, either.   What we want is the amount of smoothing to vary---itself smoothly---across a two-dimensional surface.    Ladies, Gentlemen, and sentient hunter-killer robots reading this in the future: I present PerlinGaussian!</p> <pre><code>const float scale = 0.025f;\npublic static float[,] PerlinGaussian(float [,] array, int dimension, int maxWindow)\n{\n    float[,] returnVal = new float[dimension, dimension];\n\n    for (int x = 0; x &lt; dimension; x++)\n        for (int z = 0; z &lt; dimension; z++)\n        {\n            float newVal = 0f;\n\n            int thisWindow = Mathf.RoundToInt(Mathf.Abs(\n                  SimplexNoise.Noise.Generate(x * scale, z * scale)) * (float)maxWindow);\n\n            // If the window would exceed the bounds, just copy the existing value.\n            // (Cheaper, but less accurate than doing a partial sum.)\n            if (x &lt; thisWindow || x &gt;= dimension - thisWindow || z &lt; thisWindow || z &gt;= dimension - thisWindow)\n            {\n                returnVal[x, z] = array[x, z];\n            }\n            else\n            {\n                for (int i = -thisWindow; i &lt;= thisWindow; i++)\n                    for (int j = -thisWindow; j &lt;= thisWindow; j++)\n                    {\n                        newVal += array[x + i, z + j];\n                    }\n\n                newVal /= (float)((thisWindow * 2 + 1) * (thisWindow * 2 + 1));\n                returnVal[x, z] = newVal;\n            }\n        }\n\n    return returnVal;\n}\n</code></pre> <p>The only thing that differentiates this function from GaussianBlur is that calculation of thisWindow.   As we move in X and Z, we vary the size of the \"window\" that the blur averages over.</p> <p>It's a small thing, but it makes all the difference in the world:</p> <p></p> <p>Some places are sharp, some are rounded, and the entire terrain feels much more natural.   There do tend to be a lot of \"spiky\" areas (where the window was zero), but we can get rid of those by just using 1 as the minimum window size.</p> <p>A final tweak here is that the algorithm produces very smooth terraces; the \"cutout\" portions are completely flat.  We fix that by putting back a percentage of the missing height at each step; this simulates erosive debris accumulation.</p> <p></p> <p>That's a lot more \"gouges\" than natural mountains, of course, so we'll probably set this to a much smaller amount later.</p> <p>Note from the future:  A little browsing of images of natural mountains indicates that the previous sentence is wrong as often as it's right; many mountains have more such shearing than what's shown above.   The real takeaway appears to be that there's a massive amount of variation in the natural world.   Also, that there are an awful lot of real mountains that you'd swear were bad renders by looking at them.  See also: clouds.</p>"},{"location":"erosion-implementation/#stitching-limits","title":"Stitching Limits","text":"<p>Our stitching algorithm can handle a fair amount of discrepancy between edges, but our erosions here are pushing it beyond its limits.   Even after adding some additional smoothing and tweaking, we're still seeing very harsh separations:</p> <p></p> <p>Those \"hard\" cliffs along the top ridge and to the right of the character are caused by dramatic mismatches between the terrains that are being stitched.    The problem here isn't the base elevations -- those are well within the abilities of the stitching algorithm to compensate for, especially if we add a little skirting rather than a simple edge average.</p> <p>The problem is the erosion.   Since we're applying what amounts to very deep \"scoops\" out of terrain at the patch level, you get these issues any time one of the scoops touches an edge (unless you get very lucky and there's a corresponding scoop in the next patch.)</p> <p>This is the first time we're having to deal with this, but it won't be the last:  we're going to have the same problem with any feature that is larger than or crosses patch boundaries and results in significant elevation changes:  ridges, lakes, glaciers, etc.   </p> <p>Our solution is not to do these things at the patch level at all, but rather apply them more broadly across \"regions\" of the map, which we'll discuss when we talk about regional locality.</p>"},{"location":"erosion-implementation/#canyoning","title":"Canyoning","text":"<p>The same net of canyons that define our mountain ranges at the global level makes sense at the local level, as well.   In fact, this is a stronger effect than the shearing, most of the time.</p> <p>I've recently had the opportunity to make a long drive across the central United States, and of course I paid close attention to the geographic features I was passing through.    From the nearly flat midwest to the mountainous Rockies, and everywhere in between, canyoning was the single most dominant aspect of the landscape features except for erosive smoothing.</p> <p>It was also clear that the canyoning algorithm described there was incomplete.  From observation, there appear to be two different \"kinds\" of canyons, although they're really just variations on a theme.</p>"},{"location":"erosion-implementation/#separator-canyons","title":"Separator Canyons","text":"<p>What I'm calling \"separator canyons\" are what we implemented for our mountain range algorithm.   They are more or less at the same elevation at both ends (although, of course they're caused mostly by water, and and so have some elevation drop that our current algorithm ignores), and visually separate geographic features from each other:  mountain ridges, hills, bluffs, badlands, etc.   Generally speaking, these are roughly equally \"deep\" along their entire length.  Our mountain algorithm made them all the same depth as each other, though, which is definitely not the case in nature -- these canyons occur at varying elevations, often with depth corresponding to length (longer canyons carry more water and are carved deeper).   When they're shallow and eroded, you get gentle hills or old, weathered mountains; where they're deep and sharp, new mountain ranges.</p>"},{"location":"erosion-implementation/#drainage-canyons","title":"Drainage Canyons","text":"<p>The \"other kind\" of canyon I'll call \"drainage canyons.\"   These are features that occur only on slopes, distinctly change elevation, and tend to not be of uniform depth:  they're shallower at the high end where they begin, and become deeper and wider as they descend in elevation.    At the lower end, unless they hit some other geological feature, they tend to become shallower again, either spreading out into eventual invisibility, or by narrowing and simply ending.</p> <p>You'd expect these to be related mostly to the size of the slope they occur on, but the reality appears to be that they're far more affected by the underlying rock or soil.   Softer materials seem to produce more distinct canyons than harder ones.</p> <p>Shallower slopes produce deeper and longer canyons than steep ones, and in fact very gradual slopes (typically along rivers) generate the large structures we tend to think of as \"canyons.\"   The erosion effect here is much, much stronger than the forces that shape the land, and can produce impressive canyons that descend well below the average surface elevation of an area.</p>"},{"location":"erosion-implementation/#striation-and-layering","title":"Striation and Layering","text":"<p>Many landforms---especially where they've been cut by one form of erosion or another---show significant layering of color and other properties, and even dramatic shifts in shape at certain elevations where the \"deeper\" rocks are harder or softer than those above them, and hence wear differently.   Often, these layers are parallel to the ground, but they can also tilt at significant angles as a result of uplift (or presumedly subduction, although that's unlikely to be visible near the surface.)</p> <p>A lot of this we'll need to get from texturing, but sometimes the effect is pronounced enough that it's actually visible in shape as well as texture.</p>"},{"location":"generation/","title":"Generating Techniques","text":"<p>Terrain generation is hardly uncharted territory.   In fact, this project is meant to be less about breaking new ground than it is integrating and implementing techniques to make a \"good game\" or \"good simulation,\" ideally one that's easy enough to use that it can be repurposed and modified for other games, other simulations, and other environments.</p> <p>Let's begin by going over some of the popular techniques.</p>"},{"location":"generation/#noise","title":"Noise","text":"<p>\"Noise,\" in the technical sense we're using here, means a continuous function in one or more dimensions, that exhibit some random or pseudo-random properties.</p> <p>For one-dimensional noise, think of it as a line that \"wanders\" between two values (typically either -1 and 1 or else 0 and 1) in an unpredictable fashion.  \"Continuous\" here means that if you were drawing the line, it could be done without lifting your pencil from the paper.</p> <p>Of course, computers aren't great at continuous things, and this kind of continuity is broken if you're not thinking of the function as an infinitely-divisible line over real numbers.    In real implementations, this function is \"sampled\" at discrete, usually fixed intervals.    So long as the intervals are small compared the to average distances over which the function changes value (\"period\" or the \"inverse frequency\", if we think of the function as a signal), an illusion of continuity is maintained.   If the interval approaches or becomes longer than the period, the resulting output will show moire effects or break down altogether.  This is the same effect that makes fans and propellers appear to slow, then spin backwards when seen on film.</p> <p>Probably the most famous of the computational noise generation functions is Perlin noise.  Developed for the movie Tron, it's been around for decades, and won its creator an Oscar for screen effects that used it.</p> <p>Perlin noise generates \"slow,\" smooth changes in value over the X axis.    These values are typically scaled from their [0,1] range into whatever is useful for a given application, and sampled at different frequencies in order to speed up or slow down the rate of change.</p> <p>Early in its existence, it was noticed that Perlin noise at certain frequencies tends to look like a natural \"ridge line\" when viewed edge-on.   This effect is significantly magnified by sampling the Perlin noise function at several different rates (often called \"octaves\" in the literature), and adding or subtracting the resulting curves.</p> <p>This effect persists when Perlin noise is extended to two dimensions.   When two-dimensional Perlin noise is generated over a range of (x, y) values, it produces a sort of \"lumpy\" or \"cloudlike\" pattern (very visible if you make an image of it, with each pixel given a greyscale value corresponding to the Perlin value at that position).    Again, sampling at different frequencies and adding the results together makes the clouds \"fluffier.\"  </p> <p>If you take that two-dimensional Perlin bitmap, and treat it as a heightmap, where the value indicates the \"height\" in the third dimension, you get something that often bears a remarkable similarity to natural terrain.    Messing with the scale, frequency, number of octaves, and interpretation can produce a wide variety of terrain effects, from rolling plains through dunes, hills, and mountains.    A huge amount of the \"procedurally generated\" terrains you see online are created directly from Perlin noise or a similar function.</p>"},{"location":"generation/#the-problems-with-perlin-noise","title":"The Problems with Perlin Noise","text":""},{"location":"generation/#randomness","title":"Randomness","text":"<p>If the solution were \"Perlin noise produces great terrains--we're done,\" this would be a short document, indeed.   But there are some downsides.</p> <p>The first is technical.   Perlin noise describes a single, fixed function.   Assuming no bugs, your Perlin noise generator and my Perlin noise generator will generate the same function every time.   If you use it to generate a terrain, it will generate the same terrain each run.</p> <p>Pseudo-random number generation has had this problem for ages, of course, and the solution there is to introduce a seed value, effectively a developer-provided starting point for the sequence (often itself derived from some near-random value like the microsection portion of the current clock time).</p> <p>Perlin noise is unseeded, but we can fake the seed by simply offsetting the input value by a \"seed\" distance in the X or (X,Y) dimensions, basically just moving the \"origin\" of the Perlin function somewhere else.</p> <p>There's a second issue with randomness, although it's not obvious until you look at some Perlin noise results for a while.   There's a distinct \"grid\" to it--that is, terrain features tend to appear in horizontal and vertical lines.   Whether or not this is a problem depends on the implementation; the \"grid\" is generally visible only when large amounts of the terrain are visible at once, so for a game where the player spends most of his or her time on the ground, it may never be obvious.</p>"},{"location":"generation/#simplex-noise","title":"Simplex noise","text":"<p>Both of these problems are solved in a more modern noise function, called Simplex (also developed by Ken Perlin).  First, it takes an explicit seed, so you can generate repeatable or non-repeatable output as desired.  Second, it's built over a hexagonal structure rather than a gridded one, and with more deviation from those directions.   This generates far fewer visible \"lines\" in the resulting noise.  Simplex is also somewhat more computationally efficient, so it takes less computer time to generate.</p> <p>A few specific implementations and uses for Simplex noise were covered by a U.S. patent, but there are a large number of non-infringing libraries and implementations available (e.g. \"OpenSimplex\"), and in any case the patent expired in early 2022.  Generally speaking, it's almost always better to use simplex noise rather than Perlin.</p>"},{"location":"generation/#natural-ish","title":"Natural-ish?","text":"<p>The biggest problem with both Perlin and Simplex noise is that while it generates some of the shapes and structures of natural terrain, the terrains described are only superficially \"natural.\"   A geologist looking at noise-generated terrain won't be fooled, and even laypeople will tend to find it \"boring\" compared to real-world terrains.  \"Sharp\" features like geologically young mountain peaks, deep canyons, badlands, cliffs, and other features where sudden discontinuities are common aren't produced at all by most noise-based implementations, and require a lot of tweaking to achieve even if you're trying for them.  And of course, in the typical implementations, these are generating height maps over a 2D grid, so there are never any overhangs at all.</p> <p>The last problem can be addressed in part by moving to 3D noise, which is how games like Minecraft produce their bridges, overhangs, underground caves, and other features.   But even if you ignore the blocky-ness of Minecraft, its terrains are actually less natural looking than 2D implementations:  it covers more of nature, but at the cost of producing a lot of things nature never could.  And this sort of terrain virtually requires an expensive voxel implementation because of it's three-dimensional nature.</p> <p>In the real world, mountain ranges tend to have foothills because colliding plates ripple like two pieces of gelatin shoved together.   Canyons are carved by rivers, and often still have them in the bottom.  Desert dunes are formed by the effects and direction of wind.   \"Sharp\" features in climatologically wet areas will tend to crumble and produce debris fields at the bottom.  Cliffs next to oceans or lakes will be undercut over time, resulting in sea caves or just collapse.   Hurricanes and tsunamis re-shape low-lying islands.    Unstable geology falls under gravity, structures become smoother and flatter as they (geologically) age.   Hot spots in the mantle produce \"arcs\" of volcanic mountains, caulderas, or islands as continental plates move over them.</p> <p>Noise-based algorithms deal with precisely none of that.   They generate terrains that make sense locally but not globally.  One site discussing perlin-based terrains put it succinctly as \"the terrain has no history.\"</p> <p>None of this is to say that noise-based terrains have no place.   Sometimes realism isn't important.   Sometimes local is enough.   And almost always, you can use noise-based data to add realistic detail to terrains generated by other methods.</p>"},{"location":"generation/#midpoint-displacement","title":"Midpoint Displacement","text":"<p>Another popular noise-like algorithm is \"Midpoint Displacement.\"  There are numerous variations of this, but in general, the algorithm is:</p> <ul> <li>On a very coarse grid, choose (or randomize) some known positions to be your minimum and maximum heights.   These could, for example, be mountaintops, sea floors, etc.</li> <li>Subdivide the grid in both dimensions by 2x, basically adding a new position between each existing one in both dimensions.</li> <li>Set the height of that new position to the average of the 2/4 positions around it, plus or minus a small random displacement.</li> <li>Repeat until you're at the desired resolution, possibly halving the displacement each time.</li> </ul> <p>This is a little more complex in practice than it looks like, since some of the positions need to be handled slightly differently than the others (there's a center square in each quad that has no neighbors with existing values when created), but it's relatively simple.   (\"Diamond square\" is a popular specific algorithm, there are others, as well.)</p> <p>This doesn't generate quite as nice looking a terrain as the perlin/simplex versions, but it has the advantage that you can force certain positions to have certain values:  if you want to insure that the edges of your map are below sea level, for example, or that there are mountains/continents in specific places or quantities.</p> <p>On the other hand, it shares a fair amount of disadvantages with the other noise methods:  the terrain tends to be either smoother or rougher in places than seems \"natural,\" and it in general tends toward the smooth.   If you want a deep canyon with steep walls, for example, you'll need to place the high walls and the deep floor next to each other \"manually,\" since they algorithm will almost always produce relatively shallow slopes.</p> <p>Also, despite seeming fairly fractal, there's really only two scales:  the initial \"gross features\" one and the detailed \"rough surface\" one.  Everything else tends to get smoothed out between them.  It's pretty good (maybe better than any of the other algorithms) at creating the rough structure of continents and seas, but is much poorer at things like mountain ranges and hills, unless you provide them in the initial data (which sort of defeats the point).</p>"},{"location":"generation/#voronoi","title":"Voronoi","text":"<p>Voronoi diagrams/Voronoi maps are a sort of \"region map.\"    A bunch of \"seed points\" are randomly distributed over a surface, and then every other point is assigned to the \"region\" containing the closest seed point to it.   The region edges are polygons, whose edges are along the exact midpoint between each pair of seed points.   This process effectively divides space into a random set of convex polygons.</p> <p>For example, using Alex Beutel's online Voronoi generator here, we can create an example diagram:</p> <p></p> <p>The seed points here are the black dots, and the various colored polygons are their associated regions.   Note that because they are distributed randomly, the seed points are often quite far from the center of regions they define.   Also note that every polygon is convex, and all of the edges are straight lines.   The size of the regions is defined by the density of the corresponding seed points; in the diagram above, sparser points on the left tend to lead to larger polygons than on the right.</p> <p>Despite their apparent simplicity, Voronoi diagrams can be used in a number of ways for terrain generation, as we'll see later.</p> <p>Height Map: Using the seed points as mountain \"peaks\" and defining either a fixed slope or one dependent on the polygon size can give you a base for mountain ranges and passes (the mountain passes correspond to the polygon edges, which are the low points of the height maps).   Mountains generated in this way are unnaturally conical and \"spiky,\" so this would generally be the starting point for an algorithm that then roughens the surface by adding noise, erosion, etc.</p> <p>Elevation Generation:  Let's take the above map and manipulate it a little bit.  If we take every polygon that touches the edge and make it blue, then every polygon that does not touch a blue one white, and the remainder green, you get this:</p> <p></p> <p>If you squint at that hard enough, you can see an island surrounded by water, with central snowy mountain peaks.  Using the \"distance\" (here meaning smallest number of polygons to an edge) to generate elevation zones can give you, again, a starting point for other generation techniques.  To get more elevation levels and rougher coastlines, start with more seed points. Amit Patel has a frequently-referenced online article where he takes this technique a lot farther, which you can find here:  http://www-cs-students.stanford.edu/~amitp/game-programming/polygon-map-generation/</p> <p>Others:  There are lots of other possibilities.  Andy Lo uses them to define continental plates: https://squeakyspacebar.github.io/2017/07/12/Procedural-Map-Generation-With-Voronoi-Diagrams.html, then models the \"stress\" along their edges as they push together or pull apart.  Voronoi maps can be used to place moisture maps, biomes, political divisions, or what have you.  Empyrion appears to use a similar technique to determine zone of control around planetary bases (which act as the seed points).   Basically any time you've got \"zones of influence\" that can't overlap, you're ending up with a Voronoi diagram or something like it under the covers.</p>"},{"location":"generation/#natural-processes","title":"Natural Processes","text":"<p>The natural world is shaped by continental drift, volcanism, gravity, water, and wind (and the occasional drive-by attack by comets and asteroids).   To one degree or another, all of these are relatively simple phenomenon that interact in decidedly non-simple ways to produce the astounding diversity of our planet's geography.</p> <p>This sort of \"simple rules to produce non-simple emergent behavior\" is the whole idea of procedural generation, and of course it's ideal for computational approximation.</p>"},{"location":"generation/#continental-drift","title":"Continental Drift","text":"<p>Continental Drift, or more accurately Plate Tectonics, is the science that described the large-scale movement of sections of the Earth's crust, called plates.   While it's accepted today as the best scientific description of planetary geology, it's surprisingly young--there was considerable debate about it as late as the 1960's.</p> <p>For our purposes, we can simplify the ideas to this, while losing some details:  The surface of some Earth-like planet is divided into large regions which float on top of the \"liquid\" mantle.   These plates move about, very slowly (centimeters per year).   Interesting things happen at the edges:</p> <ul> <li>Where two plates push into each other, the earth buckles up and you get mountain ranges.   This isn't a stable situation (mountains can't grow indefinitely), and the heavier plate will be pushed under the lighter one, a process called \"subduction.\"    If one or both the the plates are oceanic rather than landmasses, the physics are a little different and you'll generally get a trench or deep ocean valley instead (often with a line of volcanos or volcanic islands some distance back along the uplifted side).</li> <li>Where two plates pull apart, you get significant volcanism, and the formation of rift valleys (on land) and oceanic ridges at sea, where volcanism effectively produces \"new\" seafloor to fill in the gaps.</li> <li>Where plates slide along each other, you get buildups and releases of friction, which cause earthquakes, generally in combination with one of the other two scenarios.   The edges of plates are fault lines.</li> </ul> <p>From an implementation standpoint, we can do things as complicated as measuring stresses (see the Andy Lo article above), or as simple as just realizing that mountain ranges, rifts, and trenches tend to follow plate boundaries, and that the interiors of plates distant from the edges tend toward flatness.</p> <p>Plate tectonics as it occurs on Earth may not be a universal phenomenon; science is ongoing about its existence (or lack of same) on other worlds.</p>"},{"location":"generation/#volcanism","title":"Volcanism","text":"<p>Volcanos are a more local effect, especially if you broaden the definition to anytime magma, ash, or hot materials are brought to the surface.   They occur around continental plate edges (e.g. the Pacific \"Ring of Fire\"), often on the uplifted (higher) side of a subduction zone.  Those tend to be the clustered ones you see in mountain ranges, like the numerous volcanoes in the Cascade range of the U.S. Pacific Northwest.   But volcanoes can form anywhere that there's a sufficient \"hot spot\" below the earth's crust.  Hot spot volcanoes like this are often responsible for island chains--for example, Hawaii's islands were formed by repeated volcanic eruptions of a hot spot as the tectonic plate moved over it.   They can also be absolutely massive, such as the super-volcanos that formed the Yellowstone caldera and Lake Toba.</p>"},{"location":"generation/#erosion","title":"Erosion","text":"<p>Gravity, wind, and water all combine into erosion; the sometimes gradual, sometimes sudden process by which parts of the terrain are separated from their initial position, carried away (or just dropped), and deposited somewhere else.  Water is perhaps the most powerful of these effects; it can dissolve materials, wear them away though battering, seep into and widen cracks as temperatures change, and carry the resulting debris substantial distances.   At colder temperatures, ice is an even more powerful source of erosion.</p> <p>These changes take place over timescales ranging from seconds in the case of landslides and floods through millennia in the case of mountains flattening.  Generally speaking, we can't hope to simulate the actual processes, just the net results.    That usually means removing height from one location and adding some or all of it to another.    A crude form of this is the Gaussian Blur we use so often to smooth algorithmic maps -- it generally degrades sharp peaks and fills in sharp valleys.</p> <p>Erosion is often tied to steepness.  Avalanches, landslides, hydraulic shearing, and similar techniques rely on significant grades.  They tend to steepen the terrain at the point of loss, and make it shallower at the point of deposition.    Tidal erosion (where water hits shores) behaves very differently if it hits cliffs (generating sea caves and overhangs, and sometimes causing the higher terrain to fall into the sea) rather than shallower shores (where it will deposit beaches and generally both flatten the terrain and make the undersea portion shallower for some distance).    Rivers run faster, narrower, and cut deeper in steep terrains, then spread out, slow down, and deposit over much larger areas when they hit shallower plains.</p>"},{"location":"global-loc/","title":"Global Locality","text":"<p>At the global scale, we're concerned about shapes of things on scales ranging from several kilometers to continental.  Our primary concern here is to build the Global Terrain Template, a height map on a one-vertex-per-patch scale that will act as the backbone structure for the local maps that follow.</p> <p>Once we've done that, we can use these geological structures to create additional templates for things like moisture, temperature, wind, weather, and biomes.    If we wish, we can also use them to generate political divisions for games and simulations.</p>"},{"location":"global-loc/#global-terrain-template","title":"Global Terrain Template","text":"<p>The GTT will be sampled by the local patch generation maps to provide the rough structure that underlies that map.   For example, the GTT \"cell\" on a mountainside will have a fairly steep slope to it, the sampling insures that the terrain patch generated for that cell provides that slope.   The GTT essentially provides the contract for the local patches that are created.</p> <p>More details on this process are found in the Local Locality section.   Here, we're concerned with the generation of the global terrain template itself.</p> <p>The template really need only be a height map; a two dimensional array of floats whose dimensions are basically just the dimensions of our world divided by the patch size.   But, especially while experimenting, it's nice to be able to \"see\" that height map as whole, so we'll generate a Unity Terrain object from it that we can inspect in the scene window while the game is running.    Using a Terrain for this map also let's us build the local stuff first if we want, since we can simply use Unity's terrain editing tools to make our \"template\" manually.</p> <p>The downside of this is that we're limited by the maximum terrain size of 4096x4096 samples, but we can live with that until we're confident in our algorithms; at 1km patches, that's a sixteen million kilometer area for us to work with.   Still, we need to be aware of scale:  for game scales, that's way more than enough, but if our application needs real-world spaces, the surface of the Earth is something like 40 times that size.   Which means our algorithms need to accept pretty significant scaling and still work--either by actual scaling up to larger maps, or by allowing recursive decomposition to make higher resolution ones.</p>"},{"location":"global-loc/#step-1-generate-region-map","title":"Step 1: Generate Region Map","text":"<p>As always for Voronoi maps, we'll generate a bunch of random points.   True random tends to make \"clumpy\" points.  I'm OK with that, but if you want them a bit more evenly spread out, you can use Lloyd relaxation a couple times to move them about; you can find code for it in the Delaunay library in the references.</p> <p>We're not going to just store the points \"raw,\" though.  We're eventually going to want to mark them up in various ways, so we'll build a structure containing a point and use that.</p> <p>All we're looking for here is a sort of general elevation level.   We force areas we want to be underwater to be low (usually edges, unless your map allows the player to reach/cross the edge), and just rough out a shape for the rest of it.</p> <p>The algorithm, in pseudocode, is basically this:</p> <pre><code>Generate Random Voronoi Regions;\nAssign every pixel in the heightmap a region.\nSet the level of any region that touches an edge to zero;\nMaybe set a small number of other regions to zero to encourage bays, inland seas, etc.\nSet the level of every other region to at most one one level higher than any region it touches.\nMap the \"level\" to a set of heights\n</code></pre> <p>There are a lot of magic numbers in this implementation:  how many Voronoi regions to generate, actual dimensions, how many \"inland\" points to push down, and the like.   It also takes a fair amount of time.   On my fairly high-end system (for 2022), it takes almost a minute and numerous iterations over each pixel to produce the map.  According to the profiler, the bulk of the time is in the initial assignment of each pixel of the heightmap to it's respective region.   With a geometry-based approach and a good flood-fill algorithm, that could be dramatically improved.</p> <p>But once it's done, we'll have something like this:</p> <p></p> <p>That's a 2048x2048 patch terrain with 500 Voronoi regions and five randomly suppressed points.  (Only three are visible, one was cropped in on the bottom, and another was probably an edge region that was already zero).</p> <p>Were not trying to do anything here except get the rough shape of a continent or two.  In particular, those high and low points aren't really meant to be mountain ranges and valleys, we're going to compress this down into something much smoother and flatter in a moment.</p> <p>But before we do that, let's play around in the editor to demonstrate how we can use this to generate rough landforms.  Here's another map:</p> <p></p> <p>Here's that same map with a blue plane added above the first tier:</p> <p></p> <p>Using this, we get one large landmass with some small (well, relatively) inland \"lakes.\"   Raise the level one more:</p> <p></p> <p>Our \"lakes\" are getting bigger, and while we've still got only a single continent, it's clearly starting to break into regions.   One more level:</p> <p></p> <p>Our lakes have become bays, and the \"continent\" has a more interesting shape.   Still another level:</p> <p></p> <p>We've got two continents now (and an ocean-to-land ratio similar to Earth).   If we go another level:</p> <p></p> <p>Our \"continents\" have become small islands in a vast sea.</p> <p>Which of these (if any) is the ideal world for our game depends on the goals of the game, and in particular on whether or not sea travel (or undersea travel) is an important gameplay element.  If not, it might make more sense to increase the number of both terraces and \"pushdown\" lakes to break up the land \"earlier\" in the sea level rise path.   Here's a map with 700 regions and fifteen pushdowns, with the sea level at layer 2:</p> <p></p> <p>..or layer 3:</p> <p></p> <p>Either would make an interesting world for exploration, although this takes over two minutes to generate on my system, which is a long time to wait at the beginning of a game.</p> <p>Next, we want to smooth this out a lot.   That, at least, is easy:</p> <ul> <li> <p>First, our \"terraces\" are much higher than we need them to be; we'll just use a smaller multiplier.   </p> </li> <li> <p>Next, decide what our sea level is in terms of both heightmap level and number of terrace layers.   I pass this in as a parameter to the system, but you could just play with numbers until you find one you like, and hardcode it.</p> </li> <li> <p>Iterate across the map, setting the heights based on the sea level:  undersea ones below it, land ones above (shell here is the terrace number, and the 0.003f was just chosen by expirimentation:   <pre><code>// For the moment, just set the heights in the terrain according to the shell number.\nfor (int x = 0; x &lt; dsize + 1; x++)\n    for (int z = 0; z &lt; dsize + 1; z++)\n    {\n        // Note reversed\n        worldHts[z, x] = ((float)sealevel / (float)maxheight) +  0.003f * \n            (vregions[regions[x, z]].shell - numUnderseaTiers);\n    }\n</code></pre></p> </li> <li> <p>Finally, we'll run one or more Gaussian Blur filters over the landscape.   You can find code for this everywhere, but the algorithm is simple:  just replace each pixel's value with the average of itself and the pixels in a \"window\" around it.   It's exactly the same as the Gaussian Blur filter in apps like Photoshop, so if you're using an image library anywhere in your code, it might already be written for you.  (My \"research\" indicates that a lot of folks call this \"Smooth\" or some variant, which is accurate for the context, but I like the more generic term.)</p> </li> </ul> <pre><code>// Performs a Gaussian blur over the specified window. \npublic static float[,] GaussianBlur(float[,] array, int dimension, int window)\n{\n    float[,] returnVal = new float[dimension, dimension];\n\n    for (int x = 0; x &lt; dimension; x++)\n        for (int z = 0; z &lt; dimension; z++)\n        {\n            float newVal = 0f;\n\n            // If the window would exceed the bounds, just copy the existing value.\n            // (Cheaper, but less accurate than doing a partial sum.)\n            if (x &lt; window || x &gt;= dimension - window || z &lt; window || z &gt;= dimension - window)\n            {\n                returnVal[x, z] = array[x, z];\n            }\n            else\n            { \n            for (int i = -window; i &lt;= window; i++)\n                for (int j = -window; j &lt;= window; j++)\n                {\n                    newVal += array[x + i, z + j];\n                }\n\n            newVal /= (float)((window * 2 + 1) * (window * 2 + 1));\n            returnVal[x, z] = newVal;\n            }\n        }\n\n    return returnVal;\n}\n</code></pre> <p>You'll want to use pretty big windows in your blur (I use 10 for the first pass), or else repeat it a few times.    When we're done, we'll have a terrain that rises and descends reasonably smoothly.    We've gotten rid of all the sharp discontinuities, and now we're ready to create some new ones!</p>"},{"location":"global-loc/#step-2-tectonics","title":"Step 2: Tectonics","text":"<p>Next up, we want to put back mountain ranges and rift valleys/trenches (a rift valley or ocean trench is a tectonic canyon caused by plate movement, vastly larger than even the largest erosion canyons.)</p> <p>Earth has a lot of mountains, more than a million by some counts.  It's going to depend a lot on what you consider a \"mountain\" as opposed to a \"hill.\"    The number of mountain ranges is even more difficult to define, but it's at least dozens.</p> <p>Although mountain ranges are usually formed by tectonic plate collisions, many of Earth's mountain ranges--even some of the big ones, like the Rockies, aren't along the current edges of modern tectonic plates.   Plates merge, change shape, and divide over time, and the plates today aren't the same as the plates of a billion or so years ago.    So for an Earth-sized world, we'd probably want to model more than the 15 or so current tectonic plates.</p> <p>Some of the modern plates run along the edges of continents (particularly along the western edges of the Americas), while other landmasses are just plopped in the middle of their plates somewhere.  Most plate edges don't run across land at all, although there are exceptions in the Middle East, Africa, and India, as well as Siberia and a few islands.</p> <p>We don't really care about these details.   For our algorithm, we'll take a Voronoi map and place mountains and valleys along some of the edges.</p> <p>We can use the Voronoi map we've already got (and in fact, we've already got smoothed discontinuities along many of the edges), or we can generate a new one and overlay it.    The former option will make mountain ranges and such tend to appear near coastlines and elevation changes, the second will make them more random.   It's a stylistic decision for a game, for \"natural\" worlds, the random one more closely approximates the real world (most mountain ranges are not along modern-day plate boundaries).</p> <p>In fact, we don't really need the Voronoi map at all -- we just needs some scattered \"edges\" that we can drop mountain ranges and valleys along.</p>"},{"location":"heightmaps_and_voxels/","title":"Heightmaps vs. Voxels","text":"<p>For our discussion, assume a three -dimensional, three axis coordinate space, with mutually orthogonal axes X, Y, and Z.    The Y axis will represent height (i.e., it\u2019s the \u201cup and down\u201d axis), and X and Z represent horizontal positions.    There are numerous different representations that meet these requirements (e.g., left-handed and right-handed coordinate systems); the specific one being used doesn\u2019t matter so long as consistency is maintained across the system (or accounted for:  some of Unity\u2019s \u201cmap\u201d types invert the (X, Z) values from others).</p>"},{"location":"heightmaps_and_voxels/#heightmaps","title":"Heightmaps","text":"<p>A \u201cheightmap\u201d is a two-dimensional array of height values (sometimes represented as a greyscale image where dark areas are \u2018low\u2019 and light areas are \u2018high\u2019) that describe the height of the geology at each (x, z) coordinate in space.    This is a reasonably compact way of storing elevation/height values; is uniform in sampling; usually compresses well; and is extremely simple to implement, explain, and use.      They also lend themselves well to machine generation; there are many simple and well-known algorithms for achieving natural-looking terrain with them.    Heightmaps (aka \u201celevation maps\u201d) also exist for a wide variety of real-world scientific purposes where their overhang limitations are irrelevant, so there are a numerous sources that will let you \u201csample\u201d a portion of the real world to generate realistic looking (because it\u2019s actually real) locations for a game.</p> <p>The downside of heightmaps is that they represent only a single ground level per (x, z) location.   This means they cannot be used to represent any structure that has multiple potential \u201cground\u201d surfaces at a single (x,z) point:  most notably caves, but also bridges, tunnels, or even simple overhanging or undercut surfaces.   (It also wouldn\u2019t represent building interiors, but human-constructed structures are almost always superimposed on the geology with game objects rather than being part of them, so this limitation usually doesn\u2019t matter).</p> <p>Whether or not this is a problem depends on the game.   If the game does not require such structures, heightmaps are a good choice.  Such games are numerous, and players often won\u2019t notice the absence of overhangs among all the other details of a good-looking world.   It\u2019s also common to simply not model interior spaces as part of the terrain at all, and simply transition the player to a new scene representing the interior space (e.g., Skyrim ).</p> <p>If there are relatively few \u201coverhanging\u201d objects, they can be added to a heightmap-based world by using mesh game objects to create the overhanging parts, and adding them to the world like any other object.    For example, in this screen shot from Black Desert Online, it's likely that many of those \"rocks\" are additions to the terrain rather than part of it.</p> <p></p> <p>Conversely, recent versions of Unity allow heightmaps to define \u201choles\u201d in them, to which a custom mesh to represent, say, a cave interior can be \u201cattached.\u201d   Both of these mechanisms are reasonably complex to implement, and while they could be automated, in practice usually require a trained modelling artist to design each instance.   Less troublesome, but still a consideration is that the \u201cappended\u201d structure (the meshes that describe either the overhanging object or the cave) are different in nature from the terrain object itself, and need to be handled separately for things like collision, walkability, and navigation agent (or other AI) availability.</p> <p>This last point is especially important for games that implement player deformation of the terrain, usually in the form of digging.    It\u2019s possible to limit the player to only single-height deformations (a standard \u201cbowl-shaped\u201d crater created by an explosion would be representable in a height map, for example).    Even digging can be limited in this way, so long as digging always removes all terrain above the dug point.   The recent (as of this writing) game Valheim does this reasonably successfully because of its generally shallow digging mechanism, but it does produce some odd behaviors on steep surfaces.</p> <p>For games where digging is a primary game element (e.g. the player may be removing large hunks of the world for building, resource gathering, area accessibility, or other purposes), the limitations of heightmaps can be considerable, as the player expects to be able to cut horizontal holes in vertical surfaces.</p> <p>Similarly, in games where the player can add to the surface, the inability to create overhangs will be immediately apparently.</p>"},{"location":"heightmaps_and_voxels/#voxels","title":"Voxels","text":"<p>If a \u201cpixel\u201d is a two dimensional \u201cpixel element,\u201d a voxel is its three-dimensional counterpart: a \u201cvolume element.\u201d   Voxel terrains involve representing the environment as a three-dimensional array of \u201cblocks\u201d, in which each block is either present or absent.       This allows three dimensional structures that have as many potential \u201cground heights\u201d as they have blocks, and trivially represents things such as caves and overhangs.</p> <p>The canonical voxel world game is Minecraft .   The Minecraft world is made up of cubical blocks about half the height of the player, and effectively any block can be removed or replaced to solve the game\u2019s puzzles and build whatever the player wishes.</p> <p></p> <p>Compared to what we think of as pixels, Minecraft\u2019s voxels are very, very large.   That\u2019s typical of voxel based games, because representing any significant amount of terrain with sub-millimeter sized voxels would require prohibitively large amounts of memory.   In the block-based world of Minecraft, the large block sizes are a design benefit; much of the game\u2019s distinctive style and \u201cfeel\u201d comes from its blocky nature.</p> <p>If we wish to represent more natural-looking voxel terrain, there are obstacles that need to be overcome.     The most obvious one is the blocky nature of the terrain.</p> <p>Since the voxel \u201cblocks\u201d need to be converted to a mesh for display anyway, we can use this conversion as an opportunity to \u2018smooth\u2019 the blocks into something less chunky.    There are well-known methods to do this, in particular the marching cubes or marching tetrahedrons algorithms.    These generate meshes more complex than the pure cubes of the underlying voxel representation, but they\u2019re relatively inexpensive to implement because they\u2019re primarily lookup-table based.   At a very high level, each vertex and face of a block is replaced by a new face based on the present/not present state of the bock itself and the neighboring blocks in the direction of the vertex or face.</p> <p>When flat shaded, the resulting terrains would still not be mistaken for entirely natural.   Again, this could be an advantage:  System Era Softworks\u2019s Astroneer  uses this sort of \u201csmoothed voxel\u201d terrain as a stylistic choice; it perfectly matches the cartoony, low-polygon aesthetic of the rest of the game\u2019s elements.    </p> <p></p> <p>More sophisticated shading and texturing algorithms can mask the polygonization of the terrain and make it appear realistic. </p> <p>A bigger problem is memory, or its related sibling, resolution.    </p> <p>A default size terrain in Unity has 512 edges in each direction, for a heightmap (which records vertex heights, not planar heights) of 513x513 heights.   Assuming each height element is a 32-bit float, the heightmap can be represented in a little over 8MB of memory.   Not tiny by any stretch, but quite manageable for any modern computer.    The mesh generated from that heightmap will be larger:  each vertex of the resulting mesh having a position in 3D space, it\u2019s going to be a minimum of about 24MB, and probably more once you include texture UV\u2019s, normal maps, and all the other aesthetic niceties of the modern rendering engine.     But even if it approaches 100MB, that represents (at the commonly used scale of one Unity unit per \u201creal world\u201d meter) about a quarter of a square kilometer of space for the player to move around in; extending that to vision distances of maybe a couple kilometers square would still fit comfortably in two or three gigabytes of memory; even less with level of detail and mipmap optimizations.</p> <p>So what about the voxel world?   If we assume our voxels are one Unity unit on an edge, we need 512 x 512 x N of them, where N is the \u201cheight range\u201d of the terrain: the distance between the lowest and highest.  Since typical implementations use a square grid, if we assume a 512 block height range, we get about 135 million blocks.   If our terrain consists of a single bit of memory for each block (present or not present), that\u2019s about 16MB of data for the voxel representation.   Each block when converted to a mesh will have as many as 8 vertices using the blocky Minecraft representation, or up to about 16 using the marching cubes implementation.   At 3x32 bits for each of those, we\u2019re up to more than 1.5 gigabytes just for the basic mesh, and again some multiple of that for the other miscellanea.</p> <p>That\u2019s a na\u00efve implementation\u2014most or all of those vertices are shared with other blocks, and the majority of vertices won\u2019t be present at all in non-pathological terrains because they\u2019re inside solid areas or in the \u201cair\u201d and omitted altogether.   Ultimately, a voxel-based representation of the same terrain that\u2019s produced from a heightmap will generate identical or near-identical meshes, albeit without taking advantage of any of the voxel\u2019s overhang advantages.   But achieving these niceties takes computing time.</p> <p>At a very high level, the fundamental takeaway is this:  despite their three-dimensional appearance, heightmaps are fundamentally a two-dimensional \u201cspace\u201d (they represent a single, non-overlapping surface), and voxel terrains are fundamentally a three-dimensional one.   And the number of dimensions maps directly to the amount of computation for things like mesh generation, texturing, rendering, storage, etc.</p> <p>When we get down to grinding code; heightmap computations (for \u201cwhatever\u201d) tend to be a set of nested for loops:</p> <pre><code>For x in 0\u2026xsize\n   For z in 0\u2026zsize {\u2026}\n</code></pre> <p>Voxel implementations, on the other hand, tend to be nested three levels deep:</p> <pre><code>For x in 0\u2026xsize\n   For y in 0\u2026ysize \n      For z in 0\u2026zsize {\u2026}\n</code></pre> <p>This extra dimension buys us the advantages of a true three-dimensional representation, but at a significant computational cost (effectively a multiplier by the size of the extra dimension).</p> <p>There are a lot of ways to reduce the cost of voxel terrain operations.   Since the process tends to be of the \u201cdo some relatively simple thing on all these positions\u201d type, large gains can be achieved on modern systems by moving some of the work to the GPU via various shaders.   But even after optimizations and across a wide range of algorithms, the extra dimension multiplier remains.</p> <p>For that reason, voxel worlds tend to be generated and rendered in relatively small \u201cchunks.\u201d    While a Unity terrain object can easily handle a square 512 Unity units on an edge (and on beefy hardware, 4K x 4x or larger terrain objects), a 512 x 512 x 512 voxel cube would bring even a high-end 2021-era CPU to its knees.    More typical sizes are 16x16x16 (or if on GPU, 32x32x32) chunks, several of which are generated and placed next to each other to build the terrain.    The total number of chunks is then adjusted to computational capability of the hardware.</p> <p>A reference Unity implementation (largely unoptimized, and not using compute shaders) I use can generate roughly a 5x5 set of chunks in realtime at decent frame rates on a midrange Intel Macintosh, or about a 7x7 set of chunks on a high end gaming PC.    Many versions of Minecraft allow you to set the number of rendered chunks in the settings, as well, to play around with what numbers a decent implementation can provide.</p> <p>This breakdown into chunks provides an additional benefit.   For most circumstances, most of the time, a player avatar standing on the surface of a 512x512x512 chunk wouldn\u2019t be able to see the vast majority of the voxels, anyway \u2013 they\u2019d be \u201cunderground.\u201d   There\u2019s also no point in rendering large numbers of voxels that are all \u201cair.\u201d     The breakdown into smaller chunks means that we\u2019re rendering only the voxels that are near the surface of the terrain (or just the ones in the player\u2019s vicinity, if they\u2019re underground).   This could result in the player being able to see a full 512x512 (or greater) terrain space, not realizing that they\u2019re standing on a thin \u201cshell\u201d only one or two chunks thick.  We only need to render the deeper chunks when at least one voxel in them becomes visible.</p> <p>Is that enough?   It depends on the game world, environment, and the complexity of the rest of the terrain generation.    This is where we need to start talking about size.</p>"},{"location":"hooks-and-usability/","title":"Programmer accessibility","text":"<p>All of this work isn't of much value if it can't be used in a real game or simulation, so we're at the point where we need to think about how our eventual (developer) users are going to want to access our world tools.</p>"},{"location":"hooks-and-usability/#use-cases","title":"Use Cases","text":"<p>Specifically, here are the things we anticipate that our users will want to do, and the tools that either exist or we need to write in order to enable them.   Since \"anticipate\" is infamously subject to being wrong, I've been building a game (or at least the shell of one) to go with this.   That gives me at least an indication that we've covered enough API space for at least one user.</p>"},{"location":"hooks-and-usability/#points-of-interest","title":"Points of Interest","text":"<p>Since we're not generating anything other than the terrain itself, the game dev is probably going to want to supplement the generation with some of their own:  in particular, placing cities, towns, dungeons, airports, or whatever their particular game requires as points of interest.</p> <p>As with the terrain generation itself, this will likely need to be done at several levels of detail.   A million square kilometer region of the United States would likely contain thousands of municipalities of various sizes (most of which a player will never see), each of which with between dozens and tens of thousands of individual buildings.     Pre-baking all of that would (as per our usual refrain) be prohibitively large -- much larger than is going to be storable on anything other than a server farm.</p> <p>On the other hand, the biggest \"cities\" are likely to be anchors for things like major roads.   And while cities don't cause rivers, they tend to locate along them, so depending on how you build things, one will dictate some of the positions of the other.  (I'm going to use \"cites\" here as a shorthand for any collection of people, from a hamlet up to a real city.)</p> <p>Statista.com has some data available online about the sizes of \"incorporated places\" in the US in 2019.  I don't have the rights to reproduce the graphs and such here, but the takeaway is that there were ten cities of a million people or more in 2019, and at each size class below that (\"size class\" being about a halving of the city size), the number of them is roughly 2.5 times the number of the larger class.   This pattern continues until you get to the smallest (under 10K people) cities, at which point it jumps more 10x over the \"10-25K people\" category.    Cities of fewer than 10K people make up about 80% of all incorporated places.      The same pattern applies to more densely populated countries like India or China, although the actual numbers will of course vary.</p> <p>That's a modern, industrial, high-population world.    During the time of, say, Parthia, Kushan, the Roman Empire, and the Han Dynasty (the original \"Silk Road\" empires) the populations would be much more \"clustered,\" but also much smaller (The population of the entire planet during that period was considerably smaller than the US alone, today.).  Games set in fantasy worlds probably have an even smaller population:  the infamous eight-person \"city,\" one of which has rats in the cellar that need killing.</p> <p>From the caller's level, there are basically two points of access to the world for city generation:  </p> <ol> <li>The entire world at the terrain template level of detail (e.g. one \"height\" value per terrain tile, with the associated generation parameters like wind, moisture, land/sea, steepness, etc.)</li> <li>The terrain patch handler, at patch load time.</li> </ol>"},{"location":"hooks-and-usability/#world-generation","title":"World Generation","text":"<p>Of course, the first thing they'll want to do is actually generate a world.   By making the World Generation Params a scriptable object, they can configure it in the editor to their liking, and then make a call to GenerateWorldAsync() to invoke it, mostly on a background thread.</p> <p>It's not clear that we need to give them further hooks here: all they really care about is \"when is it done,\" which can be handled by a simple await.   Once that's completed, all of the construction arrays (heights, moisture, etc) and the terrain template itself will be available for them to place whatever they need.</p> <p>In my test game, I take this opportunity to generate a player starting city, a dozen or so \"large\" city locations, a few ports, and a few \"fortresses.\"   </p> <p>All of these are done by walking the terrain template and looking for specific criteria:</p> <ul> <li>Starting village: I look for a \"flat\" area that's close (a few tiles) to a \"steep\" one.  This puts the player near two distinct \"geomes\" right at the beginning.  Given the size of the world, significant travel might otherwise be necessary to actually see any variable base topography.</li> <li>For cities, I look for \"flat\" areas that aren't particularly wet or dry.  (I'd also add \"riverside\", except that as of this writing, rivers aren't implemented, yet.).  Basically boring plains areas where we can spread out, since I intend to put large cities at these locations.</li> <li>For ports, I look for land that's within two tiles of sea.</li> <li>For fortresses, I look for steep terrain (given the fantasy genre, \"mountain fortresses\" are a standard trope.)</li> </ul> <p>This sort of \"criteria-based location finder\" is going to be generally useful to devs, so I'll likely move the \"Location Manager\" object that's currently into my game into the world generation library.</p> <p>The habitations created here will be serialized and stored, to sort of act as \"permanent beacons\" in the world.</p>"},{"location":"hooks-and-usability/#patch-loading","title":"Patch Loading","text":"<p>Even with the permanent locations chosen, that constitutes a few motes of humanity(?) in a vast wilderness.   Most games are going to want more civilization than that, and for the smaller cities (as well as more quest-focussed stuff like ruins, dungeons, wayside inns, etc.), we'll need to generate them at patch load time.</p> <p>These will likely be transient.  Depending on the game, they may only get serialized once they become \"important\" in some way (the player visits them, a quest or map references them, etc.).  </p> <p>Alternatively, we can use a seed based on the terrain tile to generate the locations in (and around, see below) that tile.  This allows us to always generate the same \"random\" locations for the tile, and serialization of the actual locations becomes unnecessary (changes made to them will still need to be saved.)</p>"},{"location":"local-loc/","title":"Local Locality","text":"<p>At the other extreme of our maps, \"local\" patches generate the terrain that the player actually interacts with, traverses, potentially even manipulates.</p>"},{"location":"local-loc/#base-level-generation","title":"Base Level Generation","text":"<p>The first step in generating a patch is to figure out where that patch is in relation to our larger world.    The Global Terrain Template is a height map with very coarse sampling:  the height vertices are spaced at the same distance as a single local patch covers.   That is, if our local patches are 1024 meters wide, there will be one height vertex for ever 1024 meters.</p> <p>Since patches actually represent the spaces between vertices (much as the polygons of a Terrain object define the actual surfaces of that terrain), each patch is influenced by four vertices of the Global Terrain Template: one at each corner of the patch.   Two of these corners are shared with every horizontally or vertically adjacent patch, and one corner with every diagonal patch.    This sharing of data between adjacent patches is what allows us to generate them independently.</p> <p>The first step is to create the base level.   That's effectively a heightmap for the terrain that just implements two large triangular regions, one connecting the lower left, upper left, and upper right corners of the terrain, and one connecting the upper right, lower right, and lower left.   You could also divide the terrain along the other diagonal, but doing it this way makes the test for which of the two triangles a given point represent as easy as testing if its (local) X value is greater or less than it's Z value.</p> <p>From a distance, this will look like tessellation.   Because it's possible (in fact, likely) that the four corners of the patch cannot share a single plane, two triangles are the minimum \"flat\" representation.</p> <p>The process of generating base heights for each location in the patch is relatively simple given the right mathematical tools:</p> <pre><code>for each x\n   for each z\n       if (z &gt;= x)\n          set height based on a Barycentric weighting from each of the ll, ul, ru corners\n       else\n          set height based on a Barycentric weighting from each of the ur, lr, ll corners\n</code></pre> <p>Barycentric weighting is easily found on the Internet (search \"Barycentric coordinates\").   It's the same process used in \"vertex shading\" (aka Gouraud shading) and other pixel-mapping-from-a-vertex processes, where the value of something at a point p in a triangle takes on a weighted average of the value at each of the triangle's vertices.   The value depends on the distance to each corner, with closer corners contributing more value.</p> <p>But what you'll find on the Internet is academic papers and academic blog discussions, describing the process in equations and vectors.  It's surprisingly difficult to find actual code.    The best I've found are references to the book Real-time Collision Detection, by Christer Ericson.    That code is in C++ and not defined in terms of Unity types, but translating it to C# isn't hard:</p> <pre><code>// Compute barycentric coordinates (u, v, w) for\n// point p with respect to triangle (a, b, c)\npublic static Vector3 Barycentric(Vector3 p, Vector3 a, Vector3 b, Vector3 c)\n{\n    Vector3 v0 = b - a, v1 = c - a, v2 = p - a;\n    float d00 = Vector3.Dot(v0, v0);\n    float d01 = Vector3.Dot(v0, v1);\n    float d11 = Vector3.Dot(v1, v1);\n    float d20 = Vector3.Dot(v2, v0);\n    float d21 = Vector3.Dot(v2, v1);\n    float denom = d00 * d11 - d01 * d01;\n    float v = (d11 * d20 - d01 * d21) / denom;\n    float w = (d00 * d21 - d01 * d20) / denom;\n    float u = 1.0f - v - w;\n    return new Vector3(u, v, w);\n}\n</code></pre> <p>The output vector contains the weights for point a, b, and c in that order.  These are normalized -- the three weights always add up to 1.0--so you can just use them as multipliers directly:</p> <pre><code>float upLeft = templateHeights[templateindexZ+ 1, templateindexX];\nfloat upRight = templateHeights[templateindexZ + 1, templateindexX + 1];\nfloat downLeft = templateHeights[templateindexZ, templateindexX];\nfloat downRight = templateHeights[templateindexZ, templateindexX + 1];\n\nVector3 ulVec = new Vector3(0, 0, 1);\nVector3 urVec = new Vector3(1, 0, 1);\nVector3 dlVec = new Vector3(0, 0, 0);\nVector3 drVec = new Vector3(1, 0, 0);\n\nfor (int z = 0; z &lt; worldGen.terrainTileSize + 1; z++)\n    for (int x = 0; x &lt; worldGen.terrainTileSize + 1; x++)\n    {\n        if (z &gt;= x)\n        {\n            // Upper Left half\n            Vector3 b = Barycentric(new Vector3((float)x / (float)(worldGen.terrainTileSize + 1),\n                                                0,\n                                                (float)z / (float)(worldGen.terrainTileSize + 1)),\n                dlVec, ulVec, urVec);\n            result[z, x] = b.x * downLeft + b.y * upLeft + b.z * upRight;\n        }\n        else\n        {\n            // Lower Right  half\n            Vector3 b = Barycentric(new Vector3((float)x / (float)(worldGen.terrainTileSize + 1),\n                                                0,\n                                                (float)z / (float)(worldGen.terrainTileSize + 1)),\n                dlVec, urVec, drVec);\n            result[z, x] = b.x * downLeft + b.y * upRight + b.z * downRight;\n        }\n    }\n</code></pre> <p>Note that this calls Barycentric() for every point in the heightmap, and we do this for every heightmap we generate.  There's an old computer programmer saying:</p> <p>Premature optimization is the root of all evil.</p> <p>I'd like to add my own to it:</p> <p>...but there's no need to be stupid.</p> <p>In this case, we're doing a lot of work repeatedly.   Some of it is easy, like the cast of worldGen.terrainTileSize + 1 to float that could be pulled out of the loop.   But most of it is inside Barycentric().   That is the \"in a vacuum\" case given above.   In context, we can see that for every point in a given triangle, the only thing that varies in that function is p, which means that any line that doesn't use p or variables which are calculated from it--about half the function, including the first three dot products and the denominator calculation--could be done once and applied to every point in a given triangle.   That optimization alone would reduce the cost of this process by half--I leave it up to you whether that's worth the significant cost in readability (and by extension, maintainability and reusability).  But I will say, if you have to prematurely optimize, you could do a lot worse than reducing the cost of the inner code of a double nested <code>for</code> loop.</p> <p>When this process is done, you'll have terrain height values that smoothly spread the rise or drop across each of the two triangles.   The corners will be in the exact position of the template heights, and the edges will interpolate between them.  Importantly, when adjacent terrains do this same algorithm, they'll generate the same values (give or take floating point rounding) for the edges and corners, so they'll still line up, without our having to reference the other patch's heightmap.  So we've maintained independence of generation.</p> <p>That rounding gets worse the \"higher\" we go in our game world, and the steeper the slope, so some stitching is going to be inevitable.   I've found that 8K as a global maximum height produces edges that are off on the steepest, highest hillsides by about a quarter meter vertically.   In flatter terrains, there's usually no visible gap at all, and the occasional one is a few centimeters in height.     Even the quarter meter ones should be easily stitched without requiring a skirt zone.</p> <p></p> <p>Another downside is that our terrain is effectively flat (or at least smoothly sloped) except for a possible discontinuity along the center line if the two \"triangles\" that make up the terrain are significantly different in slope.    We're \"correct\" in the global function (rising toward mountains, falling toward plains, etc.), but quite uninteresting in the local space.</p> <p>And those centerline discontinuities can be pretty obvious.  Here our robot is overlooking a valley corner where 3-4 of these barycentric lines come together:</p> <p></p> <p>Even after applying noise (see below), those are some fairly straight lines (and the shadows emphasize it in the image).   Generally, this would indicate a failure of our global generation system - these discontinuities appear only in places with fairly sudden changes in global elevation slope, which is a pretty unnatural case to begin with.   But if we need to support that, the solution is to add some variance into our base elevation \"triangulation.\"   Other than the corners and edges (which are necessary in order to align with connecting patches), there's no particular reason our \"centerline\" needs to be a line, and by replacing it with something like a weighted random walk (or repeated midpoint displacement), we can make that shape much more interesting.</p>"},{"location":"local-loc/#patch-size-and-steepness","title":"Patch Size and Steepness","text":"<p>There's a subtle relationship in this algorithm between the size you use for patches (1024x1024 in our examples) and the maximum steepness of our terrains.   Theoretically, the steepest surface you can have is a terrain that reaches the minimum and maximum world heights in a single patch.   The absolute height that can be reached is unchanged depending on patch size, but the rate at which you can \"get there\" isn't.   A 1km patch can achieve its elevation gain (assuming the base heights algorithm and ignoring local features) over a full kilometer.   A 250 meter patch can be four times as steep, just because it's edges are a quarter as far apart.</p> <p>Going from sea floor to maximum height in a kilometer isn't likely to happen, of course, but this same logic applies to any elevation gain:  they can be steeper with smaller patches.</p> <p>With a 5K maximum elevation and 1K terrain patches, even pretty \"mountainous\" slopes can be walkable using Unity's default 45-degree rule, although that that rule is fairly unrealistic.   Humans can't really walk up 45 degree slopes in anything like a normal gait.  If you want steeper ones, you can, of course, raise the maximum elevation; but an easier solution is just to lower the patch size -- this gives us more flexibility for platform support (particularly mobile), as well.</p>"},{"location":"local-loc/#adding-interest","title":"Adding Interest","text":"<p>We've got a lot of tools remaining to apply, including biomes with detail and vegetation, as well as features applied at the regional level.   But before we get to all of those, it makes sense to use the easy one:  Perlin (or rather Simplex) noise.   While noise generates pretty mediocre terrain by itself, it makes an excellent \"roughening\" agent for covering up overly-regular structures, and it doesn't get much more regular than our base height map.</p> <p>We don't need much noise; just enough to break up the ground a little and not make the user feel like they're walking on a gradually terraced plain all the time.  And in fact, we don't want too much noise, because we need to be able to apply this everywhere in order to keep edges aligning.</p> <p>For now, those two algorithms (base height plus noise) will be enough to give us decent terrain shapes that define our local patches in terms of their global context, and look fairly natural doing it.   We'll add more later, but right now, we've got the beginnings of a world. </p>"},{"location":"locality/","title":"Locality","text":"<p>For our implementation, we're considering worlds that are very large--much larger than could reasonably be held in memory, and even much larger than could reasonably be stored on disk.</p> <p>While we'll take literally infinite sized worlds off the table for the moment, generating even a 1000x1000 km world (about  a tenth the size of the United States or China) would take more than four terabytes for the height values alone at 1 meter resolution, not even considering any textures or auxiliary data.</p> <p>Further, the player isn't likely to need detailed access to nearly that much space.   Assuming our game allows flight, it's not at all impossible that the player might see all that space from great distances away, but they're not going to be wandering around chopping down trees, fighting monsters, opening chests, or talking to NPCs in more than a tiny, tiny fraction of it.</p> <p>So the huge tool in our toolbox here is going to be on-demand generation.   We'll want to generate only those areas we need to in order to fulfil some player need: certainly the terrain immediately around them, and maybe rough terrains for very large features like oceans and mountain ranges, but at any given moment, effectively all of the world will be unneeded.</p> <p>This gives is a bunch of benefits:</p> <ul> <li>We only generate what we need.  If the player never goes someplace, we don't need to generate it (at least not in detail).</li> <li>We only need to keep the immediately-relevant part of the terrain in memory.</li> <li>When a player leaves an area, we only need to store any changes they made there; if they return, we can always just regenerate it and re-apply those changes.</li> <li>If a player's changes to an area are minor, impermanent, or separated significantly in time, we don't need to store them at all, we can just regenerate the area \"clean\" when (if) the player ever returns.</li> <li>We can \"change the rules\" dynamically.   If the powers of chaos are darkening the skies, withering the trees, and making the rivers run with blood, we can fade these effects in over time as we generate or re-generate terrains.</li> <li>The player doesn't need to wait for us to generate and store one million square kilometers of terrain before they can begin playing.</li> </ul>"},{"location":"locality/#requirements","title":"Requirements","text":"<p>OK, but to get all that, we need to follow some rules and solve some problems.   To help us talk about them, let's give a name to our \"generation units\": a patch or terrain patch.</p> <p>For our implementation, let's assume that our terrain patches are about a square kilometer: 1024 x 1024 meters on a side (height maps in Unity have to cover power-of-two area, so we'll just go with it.)   The exact size doesn't matter, and in fact we may need to experiment with values to find one that works across our desired platforms.     1024x1024 gives us 1025x1025 vertices, each holding a 32-bit float height value, for just over 4 MB per patch in height data.   Even on most mobile platforms, that should be manageable; if not, we can reduce to 512x512 or even 256x256 meter patches for corresponding reductions in size.    But we'll assume 1024 for now.</p>"},{"location":"locality/#independent-generation","title":"Independent Generation","text":"<p>So our first requirement--the one that we'll be spending more effort on than any other--is that we need to be able to generate these patches independently of each other.    That is, the contents of our patch must be able to be generated without requiring that we've already generated the patches adjacent to it.   Obviously this is helpful for multiprocessing (independent patches can be generated in any order and on demand), but it's more critical than that.    If our patches are not independent, then generating a patch requires generating the patches around it, which requires generating the patches around those...and we're back to needing to generate the entire world.</p> <p>Independent generation does not mean that patches need to be generated in a complete vacuum.   The generation process can \"know\" things about the surrounding patches and its own place in the world, it just can't require that those adjacent patches actually exist.</p> <p>For example, let's consider a very easy case:  we'll generate our entire world from Perlin/Simplex noise. </p> <pre><code> public WorldPatch(WorldGenerator worldGen, long worldllx, long worldllz, float localllx, float localllz)\n        {\n            _localPosition = new Vector3(localllx, 0, localllz);\n            _position = new double[3] { worldllx, 0, worldllz };\n            _longPosition = new long[3] { worldllx, 0, worldllz };\n\n            TerrainData td = new TerrainData();\n            Vector3 tdsize = td.size;\n            tdsize.y = worldGen.maxheight;\n            td.size = tdsize;\n            td.heightmapResolution = worldGen.terrainTileSize+1;\n            td.baseMapResolution = worldGen.terrainTileSize + 1;\n\n            float[,] result = Noise.Calc2DPatchInvert(worldllx, worldllz, \n                                                      worldGen.terrainTileSize + 1, 0.06f, 0.0005f);\n            td.SetHeights(0, 0, result);\n\n            terrainGO = Terrain.CreateTerrainGameObject(td);\n            terrain = terrainGO.GetComponent&lt;Terrain&gt;();\n\n            terrainGO.transform.position = new Vector3(localllx, 0, localllz);\n            terrainGO.transform.SetParent(worldGen.gameObject.transform);\n       }\n</code></pre> <p>This code takes a \"WorldGenerator\" and global and local (X,Z) base coordinates.   We'll talk more about those in a minute, but for right now, just assume that WorldGenerator is a structure with some \"global\" constants and such in it.</p> <p>Most of this is just boilerplate; we'll expand and improve on it as we go.   But these two lines are critical:</p> <pre><code>        float[,] result = Noise.Calc2DPatchInvert(worldllx, worldllz, \n                                                  worldGen.terrainTileSize + 1, 0.06f, 0.0005f);\n        td.SetHeights(0, 0, result);\n</code></pre> <p>Calc2DPatchInvert is a simplex noise generation function.   The exact details don't matter, but note that it takes the position of the patch (\"WorldPatch\") as arguments.   The 2D Simplex function uses that position as the (x,y) position to start from in it's noise space, and returns a two-dimensional array of [terrainTileSize+1,terrainTileSize+1] height values.    This code is sufficient to generate an entire, seamless world:  each patch is located exactly terrainTileSize distance apart, so each patch's Simplex noise \"picks up\" exactly where the previous one left off.</p> <p>Here's a screenshot of a character standing at the corner of three patches, one of which hasn't been generated yet.  Notice that the three existing patches blend smoothly at their edges, there are no gaps or \"spikes.\"</p> <p> </p> <p>The problem, though, is probably already obvious:  this isn't a very interesting world; just meter-high rolling hills to the end of infinity (or at least the end of the integer coordinate space).</p> <p>Part of this is that we're just using a very simple noise function.    Usually Perlin-class noise functions add several different \"octaves\" (scales) of the noise together to get a rougher terrain.   The technique is called fractal Brownian motion (FBM), which I personally think is a horrible name, because (while accurate), there's no actual \"motion\" here, and it's very different from what we usually think of as Brownian motion: the random walk of individual particles in something like a liquid.    I'm just going to call it \"fractal noise,\" and accept that the Nobel committee will probably overlook me.</p> <p>We could use fractal noise to generate a much more \"rough\" terrain by letting it generate features ranging in size from the sort of local hills we've got here through the size of the entire world.     But pure Perlin-class noise generation doesn't generate very interesting worlds at the best of times, and so we probably want to reserve it as a final \"roughening\" step after we use some other technique.</p> <p>The other issue with Perlin-class noise terrains is that they need to be uniform between patches.   If the frequency changes between two patches, the \"position\" of hills and depressions will move around, and a \"hill\" that begins in one patch will likely be somewhere else (or missing altogether) in the next, and this will increase the further you are from the origin.   Similarly, amplitude changes will cause features to have different sizes (but not positions) between patches, also causing discontinuities and gaps.  Stitching can probably cover minor differences, but attempting to generate a wide range of different terrains with varying Perlin-noise is going to make for a lot of blending problems.</p>"},{"location":"locality/#local-and-global","title":"Local and Global","text":"<p>And this is where it starts to get ugly.   Consider a mountain.   Mountains are generally much, much larger than a square kilometer, and are often in mountain ranges which are larger still, and characteristically have foothills rising up to that range, which cover more area still.    </p> <p>That mountain has an influence over patches potentially hundreds of patches away.   My \"foothill\" patch needs to do it's part in the rise from plains to mountain range, even though that mountain range is very, very distant.  And in fact, my patch will \"care\" what's in the other direction, too.   If there's a second mountain range on the other side of us, we'll likely generate very different terrain than if the land falls off to the sea (look at a relief map and notice that subduction mountain ranges tend to \"fall off\" much faster on one side than the other.)</p> <p>So it seems like we need to know about, say, every mountain in our world to generate every patch.   And probably every valley, canyon, lake, river, road, etc. over a certain size.  That's the very opposite of independent generation.</p> <p>On the other end of the scale, say there's a ten-meter pond in our patch.  If it's entirely within the bounds of our patch, no problem.   But if it spills over into the next patch, we don't want it to simply end in a hard line at the patch boundary.   We could solve this--very simply--by having small features like that always generated inside patch boundaries; but there are problems with that solution, too:  players may well start noticing a sort of \"grid\" layout to some feature types, it's clumsy to use with features of different sizes, it doesn't work for things near or slightly larger than a whole patch, and it gets even more limiting if our patch sizes need to get smaller to support mobile or older consoles. </p> <p>Then there's the stuff in the middle: that ten kilometer mountain lake, the sheer cliff that runs for a few kilometers, a swamp contained in the caldera of an extinct volcano, or even a small cinder cone volcano itself rising out of an agricultural plain.</p> <p>And while we're solving all these problems, we still want to have our patches be able to be generated independently of the ones adjacent to them (and to match them at the edges, at least closely enough that we can cheaply stitch them.)</p> <p>The world tends to be fractal:  that is, structures repeat in spirit (not exact shape) at many different scales: coastlines share their ruggedness no matter how far you zoom in and out, erosion builds the same patterns in great hills and canyons as it does in your dried backyard dirt piles, desert dunes are recursive in shape.</p> <p>But for our purposes, we can try to simplify this into four categories:</p> <ul> <li>Global structures are those that are vastly larger than our patches:  the shapes of continents, mountain ranges, oceans, supervolcano calderas, (large) impact craters, and the like.  Anything whose structure spreads over more than maybe 2 or 3 kilometers in any direction, we'll consider global.</li> <li>Regional structures are those that affect a small number of nearby patches:  a single hill, small volcanic crater, small to medium lake or island, smaller impact craters, buttes, valleys, rock outcroppings, glacier, or something like Devil's Tower.   The specific criteria here is that their influence or physical structure must be contained within a small number of patches: say 5-10 at the absolute most.   We'll also assume that they get rarer as they get larger.</li> <li>Local structures are those that are smaller than a single patch:  the \"lay of the land\" itself, plants, trees, cave entrances, small hills, etc.  These will often be entirely internal to a patch, and must never impact more than the directly adjoining patch(es).</li> <li>Network structures are rivers, roads/paths/natural trails, and long erosion paths (think the Amazon or Nile Basins or the Grand Canyon).  Typically long, thin things that have a beginning and end, and can join with others of their kind in a sort of branching structure, and often get wider or narrower in different areas.</li> </ul> <p>Each of these will require solutions separate from the others, so let's get started.</p>"},{"location":"mtns-and-valleys/","title":"Global Features","text":"<p>It's time to add some features to our global terrain Template.   Here, we're only concerned with features that are vast relative to a tile.   That certainly includes mountains, mountain ranges, and rift valleys, but also might include standalone volcanoes, craters, ice caps, and even major rivers, if our world has these things.</p>"},{"location":"mtns-and-valleys/#mountains-and-ranges","title":"Mountains and Ranges","text":"<p>We'll start with mountains.  As massive, geologically ancient structures, mountains get much of their character and shape from erosion, which we'll discuss later.   Right now, we're just trying to sketch in the basic shapes.</p> <p>There are a few standalone mountains (particularly volcanoes) on Earth--and fantasy worlds tend to love having \"lonely mountains\" of one sort or another--but generally mountains appear as part of mountain ranges.</p> <p>For our purposes, a mountain range is a rough line or arc of mountains, from a handful to hundreds.  </p> <p>The Andes, Earth's longest mountain chain on land, has about a hundred peaks of 6000 meters or more.  The Andes are also interesting in another way; they run for almost 7000 km along a tectonic plate boundary, making them also likely one of the youngest mountain chains in the world.   </p> <p>We're going to ignore \"plate\" edges and skip the Voronoi step.   Our mountain range algorithm will just pick two endpoints within a specified range of distances apart, and put a bunch of mountains between them.</p>"},{"location":"mtns-and-valleys/#mountain-map","title":"Mountain Map","text":"<p>We're going to make a mountain map, and keep it around.   This is a two dimensional float array of values that are all initially zero.   We'll draw our mountains and valleys onto that map in the range -1 to 1, and then add that map to the existing terrain heights map to actually \"apply\" our mountains. </p> <p>Why?   It's a little easier to work with a constant height system, but the key reason is that we're going to want to be able to find our mountain ranges later, and having an array that's all zeros except for the mountains and valleys  will make that process easier.   We'll eventually be making a lot of these maps, each one of which acts as a \"layer\" of our world, giving us a place to look for moisture levels, wind and weather, biomes, sea or land, even political or \"difficulty\" zones.    If our world has 2048x2048 tiles, these \"maps\" are only 16 MB each if we're storing floats, and smaller still if they're of bytes or Booleans.   That shouldn't be too much of a problem for most platforms, and for mobiles we can lower the map resolution if we need to.</p>"},{"location":"mtns-and-valleys/#algorithm","title":"Algorithm","text":"<p>That algorithm is pretty simple:   just \"walk\" the line segment, and every so often place a mountain at a random point in a circle around the present position.</p> <p>So what does a mountain look like?    There are lots of different kinds; they tend to differ primarily in how they're formed (uplift, volcanism, or both) and how they've eroded (an indirect measure of age, as well).   The shallowest are almost worn away entirely like the Appalachians, the steepest (Alps, Andes) tend to be tall, sharp, and have average slopes near 45 degrees.   Much of what we think of as the \"shape\" of mountains is cumulative effect of both the main peak and ones around it.</p> <p>For us to get good mountains (we're looking for something more realistic than Perlin noise, here), we'll need to simulate a bunch of steps.   But the first one is to get something in place at all, and for that, we're going to make cones.</p> <p></p> <p>That looks more like a spill in a waffle-cone factory than mountains, but it's a first step.   Note that a lot of these are very steep -- that's an artifact of the terrain object I'm using to model them:  in order to actually see shapes, the \"height\" multiplier is about twice what the actual mountains would look like.   Look at a relief globe, and notice how very flat mountains actually are compared to the Earth; even the tallest are barely noticeable as being raised.   We also haven't eroded these at all, yet.   Once those two factors are put back, we should get more natural slopes.</p> <p>If we add a little bit of noise to these, we can make them look a little rougher.   We're using uniform noise here:  basically just take each point and move it up or down a small random amount:</p> <p></p> <p>That looks a little better, but it's still pretty artificial looking.   So we may want to go back to our favorite Gaussian Blur algorithm and smooth these out a little (moving the noise generation to after the blur, since blurring is basically noise suppression.)</p> <p></p> <p>That looks better, and at this point we've got all sorts of parameters we can fiddle with:  the length and number of chains, the density of mountains, the range of heights available, the distance \"off\" the line segment, amount of smoothing, amount of noise, etc., etc.</p> <p>For defaults, I'm going to tone down the number of mountain chains but increase the number of mountains in each of them a bit.  That gives us a pretty good (at least from a distance) set of mountain chains:</p> <p></p> <p>Note that that world is not flat in the non-mountain areas, but the mountain ranges are so much larger than the other features that we can't really see them at this distance (and I've got my \"sea plane\" turned off).   That's approximately correct for the real world, but for a game, we'd probably magnify the other features a bit.  Putting back the \"oceans\", we get:</p> <p></p>"},{"location":"mtns-and-valleys/#foothills","title":"Foothills","text":"<p>Note the little blue spots around the base of the mountains:  Those are areas where the random noise added happened to be both negative and (in an absolute sense) greater that the height of the mountain + ground elevation at that point.</p> <p>Those blue dots are pointing out at least three problems for us:</p> <ul> <li>The sea is everywhere!   Because we've just got a plane at sea level standing in for our oceans, we get \"water\" anywhere the elevation of the land drops below sea level.   That's actually not geologically correct, Earth has many places where the land dips below sea level and doesn't fill with water (Death Valley is one).   This is just the beginning of our problems with water, and we'll deal with them another day (and many games don't deal with them at all; a global sea level isn't really that huge an issue.)</li> <li>Mountain ranges are caused by uplift, which should be lifting the ground around them, too.   The above images where mountain ranges basically fall off to the \"average\" terrain height are geologically weird.  This will also help with things like that mountain chain in the ocean in the upper left; bringing it back \"above ground.\"</li> <li>Even ignoring uplifted ground level, it's rare (not impossible) for plains to meet mountains without a gradual falloff into smaller mountains and foothills.</li> </ul> <p>So let's enhance our model a bit to solve both problems.</p> <p>First is foothills.   For our purposes, foothills are just smaller mountains that \"fade out\" with distance from the range.  Note that these distances are actually fairly short -- you usually get foothills for a few dozen kilometers before the range, not spreading out for hundreds.</p> <p>So that's easy enough.  We'll just cycle back over our mountain range a second time, using a wider \"spread\" but much shorter mountains.</p>"},{"location":"mtns-and-valleys/#rift-valleys","title":"Rift Valleys","text":"<p>There are all sorts of valleys.   Some just form are just the spaces between higher features.  Canyons and similar shapes are eroded from the passage of wind or water.   Ice or volcanism can crack terrains and seed valleys.</p> <p>We don't care about those, yet.  Right now, we're only concerned with rift valleys; those caused by the spreading of tectonic plates away from each other.   These are massive -- far larger in scale than any standard valley, these are measured in the same scales as mountain ranges.   They tend to be wide, deep in total but shallow in slope, and huge.</p> <p>Even on this scale, water is a powerful force.   The separation of plates that causes rift valleys on land tends to form ridges undersea.   Undersea trenches tend to form by the same subduction effects that raise mountain ranges on land.   In short, the effects of plates moving apart or together are largely reversed underwater.</p> <p>For our purposes, tranches are just rift valleys undersea.</p>"},{"location":"performance-1/","title":"Performance","text":"<p>We interrupt our regularly scheduled program for some notes on performance, because if you've been trying to duplicate my code and algorithms (or actually USING my code and algorithms; contact me if you want the current source), you're probably getting slammed by it about now.</p>"},{"location":"performance-1/#unitys-expectations","title":"Unity's expectations","text":"<p>Although built on C# and .NET, Unity's docs don't generally make much mention of the standard multithreading/asynchronous models.</p> <p>First, be aware that despite the absolute silence from the docs, async and await work just fine in Unity.   If you're already familiar with them, they may be all you need.    To this, Unity adds at least three different models for unblocking the all-important main thread:   Coroutines, Jobs/Entity Component System, and Shaders.</p> <p>Shaders are different in kind from the others: they move work from the CPU to the GPU, and are particularly useful for things where you're performing essentially the same calculation over many, many entities:  computing colors by applying a function to every pixel every frame, computing waves by calculating distortion maps, moving entities by applying a function to ever vertex, etc.   They're not a generic mechanism for doing \"background\" work; rather they solve certain common problems extremely efficiently.    So we're not going to talk about them right now.</p> <p>To begin with, Unity's main unit of computation is the frame.  That's a little unusual if you come from a non-game development world, but it makes sense for games.   Everything in MonoBehaviour is built around execution in a specific order, generally once per displayed frame.    For most code that's in one of the Update() or similarly-named methods that get executed every frame (or at least on a cadence similar to frames, like FixedUpdate()).</p> <p>Unity is also very, very main-thread focused.   Almost none of it's API can be called anywhere except the main thread (at least not successfully). </p> <p>All of this leads to a model where Unity expects work to be done in very small chunks, per frame, on a single thread.    So nothing that goes in the MonoBehaviour implementations should take more than about 1/60 of a second at worst, and modern games are aiming for more than twice that frame rate.</p> <p>With my \"straightforward\" implementations, generating a new terrain patch can take up to about a second, depending on size, and we're still just generating shape.    Generating the initial world's Global Terrain Template can take minutes.</p> <p>Both work, even when done in MonoBehaviour contexts.   But I'm stretching the definition of \"work\" here -- the game basically freezes at startup until the GTT is complete, and freezes for a second or two every time a new terrain patch (or more than one) needs to be created.   It's fine for this sort of initial research and experimentation, but it would be unacceptable as shipping code.    And it's going to be unacceptable for us, pretty soon -- this is only going to get worse as we add functionality.</p> <p>This is a good time to start working on performance, because if we're doing something wrong in a way that can't be moved to a more performant mechanism easily, we're just making more work for ourselves the longer we wait.</p>"},{"location":"performance-1/#first-things-first","title":"First things first","text":"<p>Does our code really need to take so long?   I'm a big believer in straightforward, na\u00efve algorithms for experimenting.   Simple code means fewer bugs and more time to spend on the fun stuff.</p> <p>As an example, my Voronoi diagram code (to assign each pixel a region) basically looks like this:</p> <pre><code>for each x position\n   for each z position\n       for each voronoi point\n            {\n            calculate the distance from (x,z) to that point\n            if it's the smallest we've seen so far, remember it as \"smallest\"\n            }\n       set the region for (x,z) to smallest\n</code></pre> <p>That works great, always gives the right answer, and is really simple to write; the actual code's only a little longer than the pseudocode here.    But Vector2.Distance() is actually pretty expensive, and we're creating vectors constantly from both the test point at the Voronoi points in order to call it -- that allocation is moderately expensive, too.</p> <p>Here's a different way to do it:</p> <pre><code>radius = 0\nrepeat until no pixels are assigned during a pass (or equivalently, all points are \"finished\")\n{\n    for each voronoi point that's not \"finished\"\n       {\n       Find every pixel in a \"ring\" around the point at radius\n       if that pixel hasn't been assigned yet, assign it to this voronoi point\n       if all of them were already assigned, this voronoi point is \"finished\"\n       }\n    increase radius by 1\n}\n</code></pre> <p>This never calls Distance() (or makes any vectors) at all.  Each pixel is touched only once unless it's on an edge or corner.  The \"ring\" calculation is complex and needs to not miss any pixels, so the actual implementation replaces it with an expanding square instead of a circle, which is much easier to calculate. </p> <p>A circular expansion would produce the same basic map as the old algorithm.   The simpler, expanding-square algorithm does not.  It's not an actual Voronoi map.   Convexity isn't maintained, and in fact, the regions are much more unevenly shaped.  </p> <p></p> <p>But we're using this to make a rough map in the first place, and for this use, convexity isn't really much of a benefit and might even be a penalty.  (This image compresses some 700+ different regions into 256 color levels, so not all of the region edges are visible.)</p> <p>And while it may not be obvious from the pseudocode, the second algorithm is much, much faster than the first.   For 400 Voronoi points on my system, it lowers the cost of the GTT from 2.5 minutes to 18 seconds.    And the benefits get higher as you increase the number of Voronoi points:  it takes the new algorithm about 25 seconds to do 900 points, but over ten minutes for the first one.</p> <p>Huge gains like this aren't actually that uncommon in code like games where you're often iterating repeatedly over huge numbers of pixels.   So it's worth a look to find out if there's an optimization somewhere that can bring your computational costs down.    The profiler is a good place to look for this; it's a bit complex to use in Unity, but it's time well spent.   You're going to need it, eventually.</p>"},{"location":"performance-1/#two-kinds-of-asynchronous","title":"Two kinds of Asynchronous","text":"<p>Even after our optimizations, though, we've still got several hundred or thousand times too much work to do in a frame, and it doesn't seem likely that any amount of algorithmic tweaking is going to bring us down into the right range.     So we need to look at ways to keep our game performant while still getting that work done.</p> <p>The two major tasks we've concentrated on thus far are fundamentally different from each other:</p> <p>Global Terrain Template generation is done once at startup, and it needs to be completely finished before the rest of the game can do anything.   Until there's a world that exists, there's nothing for the player to explore.   So some amount of time needs to pass here.   We can toss up a \"Loading...\" screen and just let GTT generation hang the game on the first frame until it's done, but it would be better to let the player do something else (customize their avatar, watch a cut scene, configure a network match, whatever) while it's happening, and just not let them go \"in world\" until we're ready.   On subsequent runs loaded from a saved game, the GTT will presumably be loaded as part of the game data and not need to be regenerated at all.</p> <p>A few Terrain Patches need to be present at startup, too -- at least the one the player is standing on.  But mostly, these will be generated as the player moves around the world, in sets of three to five (or so, depending on generation radius) at a time.   After the initial ones, Terrain Patches are generated while the player is still a full patch or more away from them.  The player can only move so fast, so if these take a few hundred--or even a few thousand--frames to be generated, we're still good.    These will be an ongoing cost as the player plays the game; they will likely still be generating new patches a hundred hours into the game.</p> <p>So our goal in both places is to spread the computation out over many frames, without preventing the other things that are going on in our game from happening.    And there are essentially two ways to do that:</p> <ul> <li>We could do a little bit of work each frame, and then give up (\"yield\") the rest of that frame's computational time to the rest of the game.   In this model, everything would stay on the main thread, both our computation and the rest of the game's could use Unity APIs whenever they wanted, and the time taken would be determined by the total amount of computation and the single-threaded performance of the hardware.    This is the model implemented by the simple async/await without Tasks, and by Unity's Coroutines.</li> <li>We could offload the work entirely to another thread or process, which on most modern systems means moving them to another real or virtual/hyperthreaded CPU core.   This will be faster in absolute terms for both the expensive computation and the rest of the game -- they're effectively not competing for resources at all, and don't even need to interact until the expensive computation is done and the results are ready.    Unity APIs would be mostly unavailable to the expensive computation, which often isn't as much of a penalty as it seems, since these are generally mathematical and data-structure oriented.   This is the model implemented by async/await combined with Task.Run(), and by the Unity Jobs System.     There's also a variant on this where you run essentially the same code simultaneously on many threads/cores with different inputs (say, generating 50 terrains at once) using ParallelForTask, but such routines are usually better suited to shaders on modern systems, and scale poorly between devices with varying numbers of cores.</li> </ul>"},{"location":"performance-1/#yield-based-methods","title":"Yield-based Methods","text":"<p>The most common of the yield-based methods is the Coroutine.   To write one of these, you declare the routine to be of type IEnumerable (Coroutines can't return values), do whatever you're going to do, and break up the work with an occasional \"yield return\" statement of some sort.    Each time the yield is encountered, the system will suspend the work, go do something else for a while, and resume on the next frame.</p> <p>Aside from this \"spreading\" over time, the co-routine does just what it would do without the yields.  Everything's executed in the same order, the results should be the same (unless they're dependent on time), and anything waiting for those results will still need to wait for them.    It's always executing on the main thread, so there aren't any limitations in what Unity APIs you can call.   Most importantly, the routine will still take the same amount of (clock) time to execute, or even a little more.   You're not getting any true parallelism here--no background threads or multiple CPU cores are involved.   The code just isn't blocking all the other stuff going on (in Unity, especially other game objects and the like).</p> <p>A variant of the yield is yield wait, which effectively says \"when you block me, don't bother giving me more time until this much time has passed.\"   It's useful for expensive calculations that need to happen occasionally, but not on every frame.</p> <p>And that's the ideal case for co-routines:  things that are just running \"beside\" the regular stuff going on, and don't need to interact much with the rest of the system.    If a co-routine calculates a value that other routines need, for example, you'll need to code the delivery mechanism of that value yourself (raise an event, call a function, or whatever when the coroutine is \"done\").    Things co-routines are good at:  moving monsters around; turning on and off lighting and other effects as the players move around, tracking projectiles, etc.   What they're bad at is:  go do this expensive calculation, and give us the answer.  The expensive calculation will take at least as long as just calling a normal function, and we have to figure out a mechanism for getting that answer from a routine that has no return type.</p> <p>The C# language and .NET runtime provide the async and await keywords which do something similar in their basic forms:  async functions can yield to suspend themselves for a while, and they can await the results of other async functions (await is \"yield until I get the answer I want.\").    It works very similarly (Coroutines are implemented using async/await under the covers), but is somewhat more general and less Unity-specific.   You can use either, or both.</p> <p>I'm not giving examples here, because these are complex topics and there are much better examples and explanations on the Internet.</p>"},{"location":"performance-1/#thread-based-methods","title":"Thread-based Methods","text":"<p>If your intent is to actually parallelize work--that is, use multiple threads and cores to get the same amount of work done in a shorter clock time--you need a different mechanism.  Again, you've got at least two to choose from.</p> <p>The Unity Jobs system allows you to package work as an IJob, whose Execute() method contains the work to do.   When executed, the job will do its computation on a background thread, possibly on another core (resources permitting).</p> <p>The standard .NET runtime offers Task objects, which are fundamentally similar.   Tasks are integrated with async/await so that you can use the same syntax, but await on a function that returns a Task is actually waiting for that Task's thread to complete.  (You can also aggregate Tasks with a list of array, and then await \"Any\" or \"All\" of them completing.)</p> <p>Again, the exact syntax is left for more specific sites on the Internet to provide.  Also note that the Jobs system has some   integration with the \"Boost\" compiler, and I'm not sure that's necessarily true of Tasks, if you want that extra functionality.  That said, I'm not a big fan of libraries that replace something already built into the language with functionally identical replacements.</p> <p>But there are some takeaways here:   The first is:  Jobs or Tasks are how you get more work done in the same time.  They're actually happening in parallel, usually on different cores of the hardware itself.   If you've got a lot of these running, they'll all complete in the time that would be taken by the single longest one if you were running them sequentially.</p> <p>The second takeaway is the dev cost.   Multithreaded programming is harder than single-threaded programming.  The order of things becomes less predictable if the system is scheduling things for you.    Two threads accessing the same variables can have all sorts of weird effects.    It's easy to end up with circular dependencies that deadlock your application.  Shared resources need to be carefully controlled to prevent multiple threads from modifying them in an unexpected order.   The syntax (particularly in C# and .NET) is obtuse and finicky.  And there's a always a little added cost for all the synchronization (although you usually get far more back from the parallelism.)</p> <p>There are a number of standards and best practices for multithreading, but those only help for the code you write.   Unless they specifically say so, you shouldn't expect any libraries you call--even standard system ones--to be thread-safe.</p> <p>In particular, that goes for Unity's APIs.   There are a small number of API points you can call from Jobs (they're listed on Unity's web site), but in generally assume you can't do anything that calls Unity-provided endpoints from a background thread.   Even things you might expect to work, mathematical functions like Random.Range(), aren't thread safe.</p> <p>So Jobs and Tasks are meant for computationally expensive stuff that can be written mostly using the base capabilities of the language, or with explicitly thread-safe APIs.  For our purposes, that's actually not such a huge limitation, since we're generally just modifying large arrays of basic types using basic math.   But if you do need math libraries, you'll likely want to use the .NET ones instead of Unity's, and for anything class based (like System.Random), you'll want to instantiate one class per thread, not share them between jobs (especially if you're using seeds to try and insure the same sequence of numbers.   If multiple threads are pulling numbers from a single Random object, the order of numbers in each thread will not be deterministic even with seeding.)</p> <p>A note of caution:  If you're planning to use these true multiprocessing APIs, write your algorithms synchronously first:  Unity in particular is very bad about reporting errors that occur in background threads, and you can't use Debug.Log() there.  The most common sign that something has gone wrong is that await just never returns.   Once you've got the routine working to your satisfaction, then move it to the background.</p>"},{"location":"performance-1/#so-what-does-that-mean-for-terrain-generation","title":"So what does that mean for Terrain Generation?","text":"<p>After a few days of performance tweaking, I've achieved some significant gains.  In particular, the multi-second freeze that used to occur near patch edge traversal has been reduced to a sub-second hiccup.   But it took a fair amount of work.</p> <p>Achieving playable performance seems to be effectively impossible without using actual background threads, so that was the main focus of my efforts.</p> <p>The single, most important takeaway: work with height maps as float[,] arrays and similar low-level structures, not Unity objects, as much as possible, and when not possible, try to do the \"math\" stuff and the \"Unity Object\" stuff separately.</p> <p>In my case, that meant doing effectively all of the actual work of terrain generation directly on primitive arrays:  I calculated all of the heights, stitching heights, texture values, etc. ahead of time on simple arrays, did them in async Tasks, and only actually transferred them to TerrainData objects after that background work completed.   It still takes 8-12 seconds to make a terrain patch and stitch it to it's neighbors, but all of that gets done on a background thread while the player is still on a remote patch, and the actual copy (which has to be done on the main thread) takes a fraction of a second when we're done.</p> <p>This has a second benefit:  ECS and similar shader and shader-like technologies also don't work on most Unity objects, so we're ahead of the game moving to those technologies, too. </p> <p>I also moved the Voronoi \"shell\" height generation into a background task, and the entirely-independent mountain range generation into another one.    These can be executed in parallel, have no dependencies on each other, and are similarly complex, so they roughly cut the world generation time in half (to on the order of 15 seconds instead of half a minute).   I used Tasks instead of Jobs for no particular reason.  Since these make heavy use of random number generation, I built my own small random class that just implements the routines I need with the same names as Unity uses, but can be instantiated per-thread rather than based on a static.</p>"},{"location":"redo-mtns-and-valleys/","title":"OK, let's try this again...","text":"<p>So here's the thing:   The mountain algorithms described on the previous page suck.  </p> <p>They're not generating mountains that are anything like natural.   By layering multiple different erosion methods over top of them, it's possible to get a sort of natural level of \"roughness,\" but there's still this sort of conical nature to them that you don't see in real ones.</p> <p>Worse, it's the sort of \"wrong\" that's obvious to a viewer.   If you watch good artists make mountains by hand in a terrain editor, their results invariably have properties that our generated ones do not.   In particular, none of the popular algorithms seem to generate ridges very well, the sort of high connections that make up mountain chains.  If you look at a mountain range edge-on, you don't see each mountain slope descending to a point, then suddenly reversing and rising up the next mountain.  Instead, it's sort of dips-and-rises, often without smooth differentiation of where one mountain ends and another begins.   Some of this is illusion:  mountains behind the first \"row\" will tend to \"fill in\" those gaps, but the actual effect is quite real.</p> <p>A second issue is that most of the standard generation techniques generate particularly picturesque mountains, not your sort of mundane, everyday ones.</p> <p>So let's start again, and ignore the \"known\" methods and try to develop our own.</p> <p>The US Geological survey has tons of elevation maps for the US, and at least rough ones for most of the world.   Let's see what some of our own planet's mountains look like:</p>"},{"location":"redo-mtns-and-valleys/#real-mountains","title":"Real Mountains","text":"<p>Here's the Cascade mountains near my home in Oregon:</p> <p></p> <p>First off, you don't see many \"round cone\" mountains there (there are a few on the desert side to the east and south, which in Oregon are probably volcanos).  In fact, if I were describing this, I'd probably say something like \"a series of uneven ridges separated by canyons\" (most of which are rivers, creeks, or streams if you turn on the water overlays.)  A lot of what we see as a \"mountain\" is either just a high point on one of these ridges, or the ridge itself viewed edge-on.   It's really, really hard to say what the boundaries of a particular mountain are, here.</p> <p>Are these typical?  Let's take a look at the northern Rockies, up in Idaho, Montana, and Wyoming:</p> <p></p> <p>A key difference here is that these are sharper and taller, although the canyons are actually even denser.   The effect isn't quite as pronounced as these images show, because they're not to the same scale.  The rockies are younger, less eroded mountains.   You may also notice a sort of vertical bias here -- there are more north/south features than east-west ones, both ridges and especially valleys.   That's even more obvious if you look at a much more zoomed out view of the Southeastern United States:</p> <p></p> <p>Here we're seeing uplift directly, these are basically the same ridges that you get if you shove a towel into another one on a smooth table.  And it's sort of fractal effect:  On this scale, whole ranges are the \"peaks\" that get separated by wide valleys (albeit in a more aligned pattern).  Most of that space is the Great American Desert, so there's a lot of erosion going on here, too.</p> <p>The USGS doesn't have as much detail outside the United States, but here's a shot of Peru (colored differently because different data sets are available there).  These are the Andes, even younger mountains still:</p> <p></p> <p>From this, we learn that the coastline of Peru is made up of very, very straight lines (some sort of map artifact).  But inland, we see the multiple \"ripples\" of tectonic squishing, and the same sort of \"long ridges broken up by canyons\" model we see elsewhere.  There's a sort of interesting effect here, too:  note that the western (whitish) areas tend to have most of the large canyon features north-south, but the greenish inland areas switch to almost exclusively east-west.</p>"},{"location":"redo-mtns-and-valleys/#so-where-to-from-here","title":"So where to from here?","text":"<p>If you look at the higher zoom maps (more detail), you notice a fairly recurring pattern:  the elevation rises to a ridge (not usually a point) more or less evenly between each bordering canyon.</p> <p>So how about skipping the whole \"place the mountains and erode them down\" idea in favor of \"place the canyons and then push up the space between them?\"</p>"},{"location":"redo-mtns-and-valleys/#first-try-at-canyon-algorithm","title":"First Try at Canyon Algorithm","text":"<p>So here's my proposed solution:</p> <pre><code>Create a \"canyon map\" of boolean values, initialized to \"false\".  \"true\" in this map represents the bottom of a canyon.\n\nDetermine the overall bounds of our mountain range\n\nUsing Midpoint displacement, draw some \"edges\" to those bounds as canyons.\n\nrepeat\n    Find a point that's on a canyon\n    draw a canyon from that point in a random direction (but not outside)\n    if you get to another canyon point, stop N% of the time, always stop on an edge\nuntil we think we have enough canyons\n\nfor each pixel on the height map, set it to a height based on how far it is from the nearest canyon.\n</code></pre> <p>For the \"draw a canyon\" step, we'll probably want to use a sort of weighted random walk, where each step is more likely to be \"forward\" than any other direction, but still provides a lot of randomness.     The N% stopping time is based on observation of the real-world maps, where canyons occasionally hit another canyon and still cross it.  This happens fairly rarely, so N might be 95% or so.</p>"},{"location":"redo-mtns-and-valleys/#random-canyons","title":"Random Canyons","text":"<p>This turns out to be surprisingly easy to implement. After playing with it for a bit, I went with generating random-walk canyons for the entire map, and then just sampling it in places where I wanted the mountain ranges to me.</p> <p>The implementation is just to pick a direction and randomly move toward it most of the time, or slightly to the left or right instead the rest of the time, stopping when we hit another canyon.  (N = 100 for this first pass).   Generating for a whole 2049x2049 vertex heightmap (our Global Terrain Template), I settled on 10K \"canyons\" after some experimentation.  That gives me a canyon layout like this:</p> <p></p> <p>My algorithm tends to prefer axis alignment or 45 degree angles, which is visible when you see the entire mesh like this.   It works for my purposes, but a better random walk would produce less aligned lines.   </p> <p>From there, we iterate across every pixel repeatedly, assigning all non-assigned surrounding elements a height value of one greater than our current value, resulting in a height map that looks something like this:</p> <p></p> <p>Tell me that doesn't look a lot like the image of the Rockies, above!    But the proof is in the rendering.   If we select swaths of this as our mountain ranges and blend them into the current terrain:</p> <p></p> <p>We can feed them into the GTT generation, replacing the current mountain rendering and erosion steps:</p> <p></p>"},{"location":"redo-mtns-and-valleys/#success","title":"Success?","text":"<p>I think that's a pretty good set of mountains.  It looks much more like a real mountain range than the conical shapes we were seeing before. and there are real ridges and a larger variety of shapes.   More interestingly, it's less expensive than our previous mountain algorithm, and the above images are out of the box.  We get varying heights, ridges, and erosion for \"free.\"   And zipping around the map a bit to look around, there aren't any \"weird\" shapes.</p> <p>It's not perfect:  the edges of the real-world mountain ranges get \"looser\" as the canyons widen, and right now all of our canyons are all the same depth relative to the overlaid terrain.   We could add some noise for roughness, some options for higher, steeper mountains or smoother, lower ones.   It could use some added erosion to break up those fairly smooth edges.   We'll iterate on it again, but it's good enough at the moment to let us move on to related problems. </p>"},{"location":"redo-mtns-and-valleys/#mountain-ranges","title":"Mountain Ranges","text":"<p>Our mountain range algorithm can use some work.   The test case above was pretty much just some polygons, blurred at the edges to make them fade out back to the plains (or whatever the baseline terrain is)   They don't look much like ranges, just mountainous \"patches\" in our world.</p>"},{"location":"references/","title":"References","text":"<p>I'm not the first, or the fiftieth, or the five hundredth person to walk this path.  Here are references to others who have gone before or after me.  (Yeah, yeah, I'm casual about my references.   No AP style here; this isn't being submitted for publication.   If you care about the details, go look at them!) </p>"},{"location":"references/#concepts","title":"Concepts","text":"<p>Polygonal Map Generation for Games uses Voronoi diagrams to \"lay out\" biomes and elevations before filling them in with other techniques.   This is a fairly old and frequently-referenced article written by Amit Patel.</p> <p>Andy Lo took that same technique a lot further, attempting to literally implement the stresses of continental plate interaction.  Procedural Terrain Generation With Voronoi Diagrams.</p> <p>Some insights into the troubles with spherical worlds from Andy Gainey:  Procedural Planet Generation</p> <p>A whole blog detailing one developer's descent into this rabbit hole:  Fantasy Maps for fun and glory.</p> <p>Along the same lines, someone playing with not just the physical maps, but their political divisions, as well: Generating fantasy maps.</p> <p>A little tutorial on shader-based mountain generation, by Paul Neale on Youtube.  This is using different software (3DS Max), bends the definition of \"procedural,\" and it's ultimately different from the way I generated them, but it's a powerful technique.</p>"},{"location":"references/#implementations","title":"Implementations","text":"<p>An implementation of the Marching Cubes algorithm using Unity.   There are several of these available in your favorite search engine, but this ones had one of the most functional implementations as of the time I was looking.   I was originally planning on extending it for this project, but ultimately had sufficiently different requirements that it wasn't a good fit.  This is a github project reference, so it's code, not so much text.  Marching-Cubes-Terrain</p> <p>Voronoi diagram implementations differ greatly based on what you intend to do with the results.    If you can iterate over pixels, it's pretty much trivial to generate a bitmap representation.   If you can cheaply repeat distance checks over a lot of vertices simultaneously, implementation is fairly simple (this is why Voronoi noise is such a popular shader effect for anything that has a sort of \"viscous liquid boiling\" effect - lava, mud pits, cauldrons, etc.).    But if you need the actual structures in geometric form: i.e. a list of vector edges, it becomes quite a bit harder.   As developers, we like it when other people do the hard stuff for us, as in this implementation by someone with the handle \"PouletFrit\": GitHub implementation, Description with some sample \"how to use code\".</p> <p>A bunch of techniques from Jacob Olsen for simulating erosion:  https://web.mit.edu/cesium/Public/terrain.pdf</p> <p>Penny de Byl has a several courses on procedurally generating various things, the terrain one is here (Udemy, possibly available elsewhere, as well):  https://www.udemy.com/course/procedural-terrain-generation-with-unity/</p>"},{"location":"regional-loc/","title":"Regional Locality","text":"<p>Our final \"locality\" resolution is \"regional.\"  This is a surprisingly \"busy\" level of detail in landforms.</p> <p>At the one extreme, any geometry features larger than maybe five or ten kilometers can be encoded directly into the global template.   Its height resolution is at the tile level, representing somewhere between 256 meters and a couple kilometers of space per height value (depending on the \"Terrain Tile Size\" in the world params).</p> <p>On the other hand features of only a few meters can be represented entirely at the terrain patch level, so that's where we do texturing, trees, rocks, buildings, world items, etc.</p> <p>But there are a lot of features in the natural world that are larger than a tile (or at least larger than our smallest potential tiles) and yet too small to accurately model at the world level.   Things like small hills, ridges, buttes, plateaus, canyons, lakes, etc need a larger canvas. </p> <p>There's another category, too:  things that would fit in a tile, but which are troublesome if they overlap the edges:  a great example are the erosion \"scoops\" we talked about earlier.   If there's a scoop at the edge of one tile but not the adjacent one, we get a very sharp and linear \"stitching\" that results in the seam lines being very obvious.</p> <p>For some features, we can just force them to lie fully inside the terrain.   That works for very small things like rocks and trees (where even if they overlap a bit, it isn't visible), but as we mentioned in a previous section, doing it for large features will result in a potentially obvious \"grid\" of places where features aren't.</p> <p>This undesired gridding is itself fractal:  we handle it at the global level by forcing the edges of the world map to water.  That's an OK \"hack\" at that level, until we start trying to make even larger worlds by positioning adjacent \"worlds\" next to each other, at which point\u2014again\u2014the \"grid\" if seas breaking up landmasses might become too obvious, although the player will likely never see enough of the world to notice the pattern if you don't show them a \"map\" or similar bird's eye view.</p> <p>We're going to have the same issue at the regional level:  if we generate \"regions\" (of whatever size) in a grid, and don't let large features overlap the grid, the player may or may not be well-travelled enough to notice.   But we can solve this problem with a little ingenuity, and for these mid-sized regions, we should.</p>"},{"location":"regional-loc/#whats-a-region","title":"What's a Region?","text":"<p>For our purposes, a \"region\" is a grid of space larger than a tile and smaller than the world.   It needs to be large enough to handle our largest reasonably detailed features, but small enough to be manageable.  We'll start with a region of 20 tiles on each edge.</p> <p>Now here's the trick for de-gridding:  we'll position the origin of one of these regions at every tenth tile.   That means that the regions will overlap:  any given tile will be in two regions:  one with it's origin at <code>tilenum % 20</code>, and one with it's origin at ten tiles to one side of that in each axis.   We can draw features in any region, while staying away from the edges of that region.  The grid will be hidden by the overlapping regions, whose edges will be in the center of the first one. </p> <p>When we generate each patch, therefore, the terrain height levels there becomes the sum of:</p> <ul> <li>The levels of the terrain patch itself</li> <li>The levels of the 20,40,60,80... grid region part that overlaps it, and</li> <li>The levels of the 10,30,50,70... grid region part that overlaps it.</li> </ul> <p>In theory, this should solve the stitching problem:  features that overlap tile boundaries will now be drawn into one of the regions, and the adjacent tile will pick up its \"portion\" of the feature from there, too.   So we'll only get very severe edge discrepancies when we want them (e.g. when an actual cliff runs along the edge of a tile).</p> <p>These regions don't necessarily need to be kept at the same resolution as tiles, since to draw the player's current patch and the few surrounding it could require keeping as many as 4 of these 20x20 regions in memory at once.</p> <p>For example, if our local tiles are 1024x1024 meters, we could have as many as:</p> <p>1024 x 1024 x (20 x 20 sized region) * 4 regions * 4 bytes/height = ~6.7GB.</p> <p>If we're willing to live with 1/4 that resolution at the regional level, we can reduce that to:</p> <p>256 x 256 x (20 x 20 sized region) * 4 regions * 4 bytes/height = ~420 MB</p> <p>That's pretty reasonable for holding in memory except on mobile, (and on mobile the tiles will probably be smaller, anyway).</p>"},{"location":"runtime-patch-management/","title":"Runtime Patch Management","text":"<p>Our world generation has two primary parts to it:  the creation of a large world, and the management of that world as the player plays the game.  Both are quite expensive; involving the creation or use of many Unity assets, iterating repeatedly over various attribute maps (height, textures, wind, etc.).  But in one way, they're fundamentally different.</p> <p>World Creation is done only once, and maybe not even that (the developer may generate and serialize a \"fixed\" world as part of the development process.)   It might take a long time--minutes, even.   But that creation time is predictable, will likely happen at the beginning of the game when there's other things going on (character creation/naming/selection, lobby stuff, whatever.)</p> <p>Patch Management on the other hand, is an ongoing process that has to happen at runtime.  It's triggered based in the player's movements around the world, so at times that are relatively unpredictable.   The patch will take some time to generate, and ideally the player should not interact with it before it's ready, but we don't want to \"hang\" the player waiting for it (at least not after the first initial patch.)</p> <p>So for patch management, we have three initial requirements:</p> <ul> <li>Speed:  We should take as little time as possible to make the patch available.   Possibly, that might include making it available before it's completely \"done,\" so long as we know the \"fill in\" won't be visible or distracting to the player (because of position, field of view, or whatever.)</li> <li>Non-disruptive:  Patches being generated should affect gameplay as little as possible--in particular, we don't want to be dropping frames where the player is in order to create where they might eventually be.</li> <li>Predictive:  Since we'll never be able to make patch creation instantaneous, we should, whenever possible, create them before they're needed, so that by the time the player \"reaches\" the patch, it's ready and in place.</li> </ul>"},{"location":"runtime-patch-management/#prediction","title":"Prediction","text":"<p>We'll start with the last category.   There's generally two kinds of movement that players perform in games:   the ordinary \"walk around the world at a walk/jog/run pace\" kind, and various forms of \"fast travel\" that we'll lump under \"teleportation.\"</p> <p>Ordinary travel isn't too bad:   assuming our player can't cross a terrain patch in less time than it takes to make several more, the algorithm is fairly obvious:  Generate the patch the player's in, then the ones immediately adjacent to that, and then the ones immediately adjacent to those...for as far out as resources or game design require.   As long as the patches are \"ready\" by the time the player gets there, they'll get they appearance of a seamless world.  As they leave each patch and enter another, create the \"new\" patches that surround them (and maybe delete existing ones that have now become farther away.)</p> <p>Teleportation is more difficult.    Sometimes it'll be predictable (the user is approaching a portal of some sort that will move them to a known location), but far more often the user will be choosing the destination in some way (e.g. the classic open world fast travel map), and it won't be practical to pre-build patches at every possible destination.</p> <p>A wait (in the form of a loading screen or something else) may be inevitable when teleporting.  Certain roguelike games have a built-in delay before \"word of recall\" type spells kick in, to prevent using them to get out of difficult situations, but generally once the teleport has been \"triggered,\" we don't want the player doing anything until it completes.</p>"},{"location":"seeds/","title":"Determinism and Seeding","text":"<p>We have a large number of algorithms dependent on random numbers in our generation code---in reality, probably more of it's random than not.   That's great for generating varied terrain, but it becomes a problem when we need to reproduce that terrain exactly.</p> <p>If a player leaves a terrain patch, then turns around and re-enters it, it would be weird if the mountains moved.   They might not notice trees being in different places, or the distribution of rocks changing, but anything bigger is going to be a problem.</p> <p>If our game is multiplayer, then even the trees and the rocks need to be in the same places, or two players in the same place won't be seeing the same things.</p> <p>The solution, if you'll recall back to your introductory programming courses, is seeded pseudo-random number generators.   The seed initial value, which you provide, will cause the PRNG to produce the same sequence of \"random\" numbers in the same order.</p> <p>If you're like me, you probably spent most of the time in those early courses trying to defeat the seed and get different results each time (a common solution was to use the current wall clock time as the seed), but now we really want that determinism.</p> <p>While doing the initial development for algorithms, though, it's often valuable to have new results every time (this gives is a sense of what the \"outlier\" possibilities are for our algorithms, e.g. occasionally generating a world that's entirely underwater).   And as algorithms change their consumption of random numbers, even using the same seed won't generate the same results, so we'll typically add seeding a little later in the development process.</p>"},{"location":"seeds/#determinism","title":"Determinism","text":"<p>Ultimately, we want all of our algorithms to be deterministic.  That is, we can rely on them producing the same results when given the same inputs, every time, on every computer.    That may seem a touch weird for worlds which are entirely defined by randomness, but we need it for reproducibility.</p> <p>That determinism need to exist across the entire world, at every scale.   And we have the additional constraint that we generate our world, at runtime, as needed.   We won't always be generating terrain patches in the same order, for example.   Even if the player moves along the same path, the background patch creation may complete in different orders based on available CPU cores and other minutia, and most players aren't going to just traverse the same path over and over.</p> <p>So it's not enough to have one seed.   Every task needs to have its own seed available (although it might share it with other tasks), and it needs to be able to determine and use that seed in a deterministic fashion, as well.</p> <p>But let's back up a bit.  We can achieve determinism in several ways:</p>"},{"location":"seeds/#single-generation","title":"Single Generation","text":"<p>If something is generated only once, then stored and reloaded, the determinism problem becomes much easier.    We'll do this, for example, for our Global Terrain Template.  Once it's generated, we'll write it to disk, and reload it when we reload our game.   If the seed is unrecoverable after generation, that's not all that important, because we have the results.</p> <p>Minecraft uses something similar for chunks that have been modified: when the player gets far enough away, those chunks are stored, and the stored version is reloaded in preference to a generated one when the player returns.   Unmodified chunks could be done the same way, but it's often more efficient to just re-create them.</p> <p>More generally, there are all sorts of things for which \"store and load\" is a better solution:  monsters, treasure, resource nodes, etc.   These things will change as the player interacts with them, and shouldn't generally \"reset\" to their initial configuration every time the player re-enters their vicinity.    Even if this sort of stuff \"respawns\" after a time, most games will want to do that under their own control, not as a side effect of the player's movement.     They're also---compared to terrains, anyway---very cheap to store.  Keeping the location, type, and current hit points of a monster might take only a few dozen bytes.   Items and resource node states are even lighter weight.   It's not uncommon for save game files for even large worlds to be in the few tens of megabytes in size.</p> <p>For all of these sorts of scenarios, the solution is generate once (possibly on the server, for multiplayer games), store, and reload as necessary.</p>"},{"location":"seeds/#positional-generation","title":"Positional Generation","text":"<p>Next up is our old favorite: generate based on position.    Perlin/Simplex noise works this way -- the functions are continuous across the entire number space, and we just sample from it based on our position.   This works well for things that are in fixed locations relative to some coordinate system:   our terrain patches, for example, can use their global coordinates as inputs.  Within a terrain, the position of details objects, rocks, trees, and the like can be relative to the terrain's coordinates, the world's coordinates, or both.</p> <p>This feature of the Perlin-class noise algorithms is why they're used so ubiquitously in terrain generation.  Not only do you get the same results when you feed in the same positions, but those results are continuous across adjoining positions.  So anything generated from P-noise should line up across terrain patches without requiring extensive stitching.</p>"},{"location":"seeds/#seeds","title":"Seeds","text":"<p>Nearly all computer pseudorandom generators these days are seeded, so for simple stuff like RandomRange() and the like, we're covered.   Perlin noise is not a seeded algorithm--the function always produces the same values for a given position--but we can use a large offset to work as a surrogate seed.   Simplex noise does allow a seed value (and has other advantages), so we'll prefer that, anyway.</p> <p>But being able to seed isn't the whole solution.   Even with a seed, if we want to generate the same sequence, we need to generate exactly the same number of values after that seed, in the same order.    Which means if we, say, generate the global terrain, then patches A, B, C, D, and E without re-seeding, we always have to generate those same patches in that same order every time.</p> <p>Alternatively (and more practically), we need to reseed our generators every time we start a new task.  We could use the global template's seed to generate a seed for every patch, but it's probably easier just to use some inherent property like the patch's position as the seed value.</p> <p>Seeded algorithms don't necessarily have the continuity-at-edges feature of Perlin noise, so if we use them to create macroscopic features, we need to be aware of how they ineract at the edges.   For a lot of stuff (placing rocks, trees, puddles, etc.) continuity at edges doesn't matter, and seeds are great for that.</p>"},{"location":"size_of_game_worlds/","title":"Size of Game Worlds","text":"<p>By and large, game worlds are tiny.   Very, very tiny.    This includes the \u201chuge open worlds\u201d that have become popular.    We have become accustomed to the metaphors of game environments to the point where we often don\u2019t consciously notice the weird sizes of things.</p> <p>Let\u2019s assume human player characters are of standard human size, that is, somewhere between 1.5 and 2 meters in height.  Normal human walking speeds are in the vicinity of 3-5 km/hour, maybe twice that at the \u201cfast jog\u201d that seems to be most game characters\u2019 primary locomotion.  From these, along with timing how long it takes to make various journeys in the game world, we can get rough estimates of the sizes that these worlds and objects would be if \u201csuperimposed\u201d on the real world.</p> <p>The numbers are surprisingly small.   You could fit all of World of Warcraft\u2019s Azeroth, along with all the lands of its various expansions, several times into the county I live in\u2014it is variously estimated online at being between 200-800 square miles.   A knowledgeable player could cross the original continent of Everquest\u2019s Norrath on foot in about 40 minutes.   (Spoiler:  it takes a little longer than that to cross real continents on foot.  Try it!)  </p> <p>Those huge, craggy, massive mountains that blot out the sky of Skyrim?</p> <p> </p> <p>The tallest is about 700 meters\u2014certainly big enough that it might be called a mountain rather than a hill (especially with that craggy shape), but nowhere near the size you\u2019d expect for its prominence in the skyline.    The entire world of Skyrim covers less than 15 square miles, smaller than my town.  For comparison: the base of Mt. Hood\u2014a single, moderately large volcano in Oregon\u2014covers more than 92 square miles.   Whole planets in No Man\u2019s Sky are somewhere in the vicinity of twenty miles in diameter (although there are quintillions of them, so the total land area is\u2026big.)</p> <p>Ignoring infinite and/or near-infinite procedurally generated terrains and real-world sampled ones, I\u2019d estimate that all of the artist-designed game worlds in all computer games ever written put together would fit comfortably inside of Texas, with room left over for a lot of rattlesnakes.</p> <p>Things get even weirder when we consider population centers and commerce:   a bustling metropolis in most of these games would have fewer than 100 residents; it\u2019s not uncommon for there to be a fully-stocked store that could literally outfit an army of adventurers in a town of eight people (and for some reason, a frighteningly large number of these shopkeepers need rats removed from their cellars).</p> <p>There are reasons for all these numbers, of course.    The primary one, of course, is that even granting very fast travel, the vast majority of an \u201cEarth-sized world\u201d would never be seen by players.  Games also have limits on how large they can be, even in an era of multi-terrabyte storage.   Artists are expensive, and designing large spaces that are unlikely to be seen isn\u2019t a financially productive use of their time.    Players would likely be bored if they had to spend real-world amounts of time traversing between towns or navigating about a city.   </p> <p>For games, a better metric of world size is probably something like \u201cinterest density;\u201d some sort of measure of how many distinct places there are to visit or things to do in the game world.   You can see this in city racing games:  there\u2019s a lot of physical space, but it\u2019s not very detailed (almost no buildings would have interiors, for example) because the players will be literally racing by it at high speeds most of the time.    On the other side, Skyrim has hundreds of points of interest and biomes scattered around it\u2019s very small (by real world standards) land area.   The environment is vastly denser than the real world, giving players much less \u201ctravel time\u201d for their adventures while still creating a psychologically large sense of scale.</p> <p>Of course, games aren\u2019t the only uses for terrains, and for simulation purposes, real-world-analog sizes and distances are important.   Microsoft Flight Simulator models, in essence, the entire (real) Earth by way of streamed geologic data.   Google Earth similarly presents Earth itself in its full-scale glory.   We have renderable terrain data for Mars and the Moon, as well.    In some applications, there\u2019s value in being able to generate \u201creal world-like\u201d terrains at will (often enhancing or emphasizing some geologic attributes for things like pilot training in mountainous areas or spacecraft landing in unexplored terrains.)</p> <p>All this boils down to:  it would be useful to be able to create both \u201cfull scale\u201d and \u201ccompressed\u201d terrains in order to maximize the applicability of our terrain generation engine.</p>"},{"location":"size_of_game_worlds/#geology","title":"Geology","text":"<p>We\u2019ll start with geology.   As mentioned earlier, many games combine the notion of the physical shape of the land with the biological and other \u2018things\u2019 that grow or sit upon it, referring to the entire combination as a biome.   For example, the Minecraft desert biome is always made up of shallow, gentle hills (reminiscent of dunes), the mountain biome is steep and (even by Minecraft standards) jagged, swamps are almost entirely flat, and so on.    While these describe real-world scenarios well enough that they don\u2019t seem unnatural (at least in a world made entirely of cubes), they discount a number of possibilities.    Not all mountains, jungles, hills, or forests are the same, and in many cases a single geographical entity might have many biological environments represented in it (mountains and hills, in particular, cross multiple elevations, and even with something like deserts there\u2019s amazing variablility).</p> <p>Still, it\u2019s handy to have a term for a particular geologic \u201cshape,\u201d so we\u2019ll call this a geome.   A geome is a description of physical shape (e.g. mountainous, hilly, flat, etc.) without regard to the life or other decoration that appears on it.</p> <p>There are a number of mathematical mechanisms that we can use to generate natural-looking geological structures, most of them fractal in nature--that is, if we \u201czoom in\u201d on a particular sub-region of a geome, the types of shapes we see will be similar to the shape of the whole geome in structure, jaggedness, height variability, and so on.      For example, \u201csmooth\u201d old mountains tend to be fairly smooth at every scale due to long-term erosion; \u201cyoung\u201d mountains tend to be sharper and rougher at both the large and the small scale.    The rippling shapes of sand on desert dunes is often reminiscent of the shape of the dunes themselves.</p> <p>Real world fractals can maintain this self-similarity over very large ranges of size, but our synthetic ones stop at whatever the \u201cresolution\u201d of the world is on the small end (e.g. for Minecraft, the fractal nature ends at each cube.   For a marching cubes/tetrahedrons implementation, it would break down each \u201ccube\u201d one more time into polygons, which would then be flat and not broken down further.)</p> <p>On the larger end, the structure stops at the point of the largest element of the geography:  a mountain, a hill, a plain, an ocean, or whatever, although you could extend the model to whole continents or planets (and once you get planets, you\u2019ve got self similarity on huge scales again: moons orbiting planets orbiting stars orbiting galaxies orbiting clusters\u2026).</p> <p>Ignoring water, mountains and other large structures are the hardest part of generating terrains simply because they\u2019re large.   This is particularly true for voxel terrains, because a single mountain (even a game-scale one, and definitely real-scale ones) would consist of far more 16-32 meter voxel chunks than are ever likely to be loaded at once.     But even the simpler terrain objects have trouble representing spaces that are larger than a kilometer or so in a given direction.</p> <p>Solutions fall into several categories.    The simplest is just not to allow such large terrains:  even a lot of \u201copen world\u201d games have maps that cover very small amounts of actual space, usually by increasing the detail density to a level that you would never see in the real world, but that meets our expectations in games.    For artist-generated worlds, the artist can combine multiple terrain or voxel objects; build the terrain as a monolithic whole, then separate them again for streaming at play time.</p>"},{"location":"size_of_game_worlds/#floating-point-precision-limits","title":"Floating Point Precision Limits","text":"<p>There is, as always, another problem.  Coordinates in Unity (and for that matter most game engines and video cards) are stored as standard 32-bit floating point numbers.   The \u201cfloating\u201d of \u201cfloating point numbers\u201d means that the decimal point can move around in the number--but the total number of bits is fixed.  Which, in turn, means that there are only so many bits to go around:  the larger the integer portion, the fewer bits are available to represent the decimal portion of the numbers.  So, it\u2019s an inherent property of floating point numbers that the larger the absolute number, the lower the decimal precision.</p> <p>For most numerical uses, that\u2019s fine\u2014even desirable.   We generally care about precision as a number of significant digits, and we tend to \u201cround\u201d large numbers more than small ones, anyway.   If you are doing math on large numbers, the rightmost decimal digits are likely noise or uncertain, anyway.</p> <p>But when we\u2019re using these numbers to generate positions for things: trees, rocks, animals, mountains\u2026that lack of decimal precision is problematic.     As our virtual character wanders farther and farther from the origin of their world, the \u201cgrid\u201d size on which things can be placed becomes coarser and coarser.    This is because while we\u2019re considering the character\u2019s position on some global grid, the character\u2019s interest is always local--concerning the objects that are positioned around them.</p> <p>Unity doesn\u2019t enforce any particular interpretation of its units, but common convention sets one Unity Unit to one real-world meter (e.g. an approximately human character is between 1 and 2 Unity Units high in most game engines).    </p> <p>A 32-bit floating point value has roughly 7 (decimal) digits of precision.    So, if we\u2019re representing a position in meters, and we want at least centimeter (1/100 meter) resolution, we need to reserve two of those digits for decimal places.    That leaves us about 5 decimal places of potential distance, or about 10,000 meters in which we can position things with centimeter accuracy.   Since the coordinate systems are usually centered on the origin, that means our character can move (or see) about five in-game kilometers from the origin before the precision with which we can place objects drops to a tenth of a meter instead of a hundredth.   Beyond 50 kilometers, we can\u2019t even place objects a meter apart reliably, and it gets worse from there.</p> <p>How the game engine deals with this falloff of precision after 5000 meters or so depends on the engine, but most of them will have objects \u201cjumping\u201d or \u201cshaking\u201d when placed with higher accuracy than the coordinate position allows, and certainly if you\u2019re allowing travel beyond 20 kilometers or so the errors will be too large to ignore even if you\u2019re willing to tolerate the jitter.</p> <p>For objects with small details, the problems get worse:  render pipelines will often include steps where the model\u2019s \u201cobject\u201d coordinates are translated into \u201cworld\u201d coordinates.  In such scenarios, the individual vertices of the model will be subject to the precision limits of that part of the world, and serious (and very visible) distortion will result.</p> <p>The financial industry has been dealing with this for decades, and their solution is usually to not use floating point numbers at all to represent currency, but rather a (large) integer number of the smallest currency unit (in the US, pennies, maybe) that needs to be represented.</p> <p>That doesn\u2019t help us come actual render time, because we don\u2019t have control over Unity\u2019s coordinate implementation.   But we can borrow this mechanism for actually positioning things in our \u2018world\u2019 for long-term storage or generation:   We can calculate positions using (say) 64-bit integer numbers of centimeters, which gives us a \u201cresolution\u201d of several quadrillion meters before we run out of space\u2014enough to represent the entire surface of any planet in our solar system to centimeter accuracy.   Double-precision floats can give you trillions of meters at centimeter accuracy.   Need more?   Most platforms give you access to 128-bit integers, as well, which can safely be described as \u201ceffectively infinite\u201d for this purpose.  If your platform provides quadruple-precision floating point numbers, you could use these, too.  The \u201cinteger number of some fraction\u201d is effectively the same idea as fixed-point numbers, also provided by some libraries in large forms.</p> <p>Once we need to push polygons to screen, though, we\u2019re going to have to live with the limitations of 32-bit floating point numbers.  And that means we need to keep moving the origin so that it stays near the player.   This implies a degree of chunking even to non-voxel worlds (in the voxel case, the chunks are handily already provided!).    When a certain distance is reached, the player and every \u201cnearby\u201d object will be translated or reloaded relative to a new origin point closer to the player.</p> <p>Finally, consider a character standing on a mountaintop on a clear day.   On Earth, that character would be able to see perhaps a hundred kilometers in any direction.    This seems to throw a wrench into our \u201ceverything has to be within 5000 meters of the player\u201d rule.   There are all sorts of reasons why this scenario is hard, not the least of which is that there\u2019s an awful lot of \u201cthere\u201d there.   But once we observe that the character isn\u2019t going to be seeing centimeter-sized details from 5km away, nor meter-sized ones at 100km, it becomes a little easier.    We\u2019re going to have to solve level-of-detail (LOD) issues, anyway, so we just need to be aware that floating-point precision is going to put an upper bound on how precise each detail level can be--probably a weaker bound than the actual amount of space, though.    If the world is divided into 1km square terrain pieces, our mountaintop viewer can see potentially a hundred thousand of them at once, which means each one can\u2019t have many polygons, anyway.</p>"},{"location":"size_of_game_worlds/#level-of-detail","title":"Level of Detail","text":"<p>...and that brings up the next point.   It's fairly common to have terrain rendered on top of an (x,z) mesh where the vertices are one meter apart.  Unity uses \"unity units\", which have no explicit size out of context, but by common convention many modelers base it on a 1UU = 1M scale, so a typical human is somewhere between one and two units in height, depending on age, gender, physique, etc.</p> <p>The continental United States is approximately 4500 x 2500 kilometers, so it would have something over eleven trillion \"vertices\" if measured on a coordinate grid.   Of course, at that scale the curvature of the Earth would make a mess of pure cartesian coordinates, but in many applications\u2014even some real world ones\u2014that can be ignored.   Real world map data at that scale does exist, for example see the USDA Geospatial Data Gateway for LiDAR maps of most of the United States at one meter resolution.   But these data sets are very large:  even if you could somehow represent each height element as a single byte, that's over eleven terabytes for the U.S. alone.</p> <p>Technical discussions always run the risk of rapidly becoming outdated by the march of technological progress, but at least in mid-2021, it's reasonable to say that no video card is going to push 11 terabytes of coordinate data (nor the 22 trillion resulting tesselated polygons) at all, much less in real time.   And never mind the GPU:  very, very few modern computers are likely sporting 11 TB of RAM, and while a few folks may have that level of secondary storage, they're going to be in a distinct minority, as well.</p> <p>So there are compromises to be made, depending on the applicaton.</p>"},{"location":"size_of_game_worlds/#reduce-the-data-set","title":"Reduce the data set","text":"<p>As a starting point, consider whether we need to store that much data at all.</p> <p>Flight simulators, and other scenarios where the player isn't going to get a good look at terrain details can probably get by with much less than 1m resolution.   If you can reduce your grid to, say, ten meter spacing, your 11 TB dataset is now a mere few hundred GB, and if you can use hundred meter spacing, you're within the sizes acceptable for a downloadable game these days.</p> <p>Alternatively, even in a flight simulator, the player isn't going to be able to see most of the US at any given moment--or for most flights, at all.   Streaming just the needed data from a large cloud source might work, especially in combination with level of detail calculations.  For example, your game might need 1m resolution near airports, but only 10m resolution for most of a flight where the plane is at 30,000 feet altitude.</p> <p>Similarly, GPS mapping systems meant for automobile navigation need basically no data at all for areas not on a roadway.  Even better, roads have relatively few configurations in the real world, so a hundred miles of interstate through the central plains might take only a few hundred bytes of data to accurately represent.  A curving mountain road or logging road might require a much higher information density, but they make up a relatively small percentage of the total roads.</p> <p>For game worlds that don't represent real worlds, we have additional options.   There are numerous mechanisms for generating \"random\" terrains from seed values, or for using seed values to 'search' into some mathematical space, such as Perlin noise.   In both cases, the seed values can be generated hierarchically from a higher level seed.  For example, use one seed integer for the world, use that to generate the seed values for quarters of the world, subdivide each of those into quarters using those seeds, and so on, in a space partitioning algorithm.   Since this generates a binary (really quadrary) tree, the \"deepest\" seeds can be generated from the initial seed in a logarithmic number of steps; a few dozen iterations will get us to US-sized spaces with ease.</p> <p>Even easier is to skit the seed generation (or use only one \"world\" seed) and just use the coordinates of the desired location as the \"seed\" or lookup key.   Minecraft does this, using the world coordinates to look up a position in a Perlin (or similar) continuous 3D noise function to generate its terrain.  Non-voxel games, or games in which 3D overhangs do not appear, can use a simpler 2D noise function.</p> <p>If the player cannot modify the world, using these generation methods means that no terrain data need be stored at all; it can always be regenerated (and on modern systems, possibly even done on the GPU itself via shader) based on the player's (or some other entity's) position.</p> <p>If player interations are limited to surface elements (chopping down trees, picking up rocks, killing monsters, etc.), only the \"things\" in the world need to be stored long term.</p> <p>Even if the player can modify the world's terrain itself (digging, building, blasting, whatever), it is likely\u2014approaching certainty\u2014 that the vast majority of \"chunks\" in the world will remain forever untouched by the player, and only those chunks that are modified need to be stored.   It's not uncommon for even that storage to be time- or space-limited; returning to a modified terrain after a long absence (or after modifying lots of other terrain in the interim) will in some games result in the player's modifications being lost:  the game trades off the likelyhood that the player will return to a long-ago-important location against limiting the size of the \"modified chunks\" cache.</p> <p>(I'm suddenly envisioning a hypothetical game in which players smash asteroids into planets from space, thus violating the \"players only modify things really near themselves\" rule, but that'll take a different design entirely.)</p>"},{"location":"size_of_game_worlds/#reduce-the-level-of-detail","title":"Reduce the Level of Detail","text":"<p>Somewhat orthogonal to the question of how much data is being used to store the world, is the question of how much data is being used to store the parts of it that the player can actually see at any given moment.</p> <p>The answer might be \"all of it;\" Astroneer, No Man's Sky and Empyrion all feature gameplay elements where a player approaches a planet from space, and at least during these ascent/descent sequences can see a significant fraction of the entire surface of the planet.   All of them make visibly obvious simplifications to the world in order to pull it off.  Most notably, terrain \"snaps\" into more and more detailed configurations as the player approaches the surface, and \"ground\" clutter things like trees, plants, objects, and animals either do not appear at all until landing, or only when the player is extremely close to the ground.  (No Man's Sky uses the same \"hide the details\" mechanism even for relatively low flight, unless the player's spacecraft moves extremely slowly with respect to the terrain.)</p> <p>More typically, the player is on the ground, and their vision will be limited by any nearby objects, the terrain itself, the horizon (real or virtual), and other factors.   Video games often supplement this with fog or other visibility-limiting elements, but we're interested here in the \"clear day\" scenarios.</p> <p>The Internet\u2014which would never lie to me\u2014tells me that the horizon as viewed from a height h above the ground on a flat plain or ocean, will be a distance d away based on: $$ d = 10000 * \\sqrt{h / 6.752} $$ Where d and h are in centimeters.  (Drop the 10K multiplier to get d in kilometers, since it will typically be large).   For an average height human standing up, that's going to be a little less than 5km (or 3 miles, but games almost always use metric internally).   Which isn't too bad.   A circle with that radius has about 78 million square meters, most of which will be out of sight behind or to the side of the player.    It's still too much to draw:  If we can \"see\" 25 million of them, and they have two polygons each, that's about 4.5 billion polygons we'll need to draw per second to get 90 fps.  A fairly high end modern video card can do that under optimal circumstances, but it doesn't leave us much polygon budget for the rest of the game, nor any real support for more typical hardware.   It's also (probably coincidentally) just about the range at which floating point errors will start being visible.</p> <p>And that's a best case.   Add even a few dozen centimeters (say, a character jumping), and you'll get a few hundred meters of additional view.    Let the character be standing on a mountainside at 3000 meters, and they'll be able to see more than 200 kilometers.   That's reversible, too; a player on a flat plain will be able to see at least part of a 3000 meter mountain even if it's 200 km away from them.   Even that's hardly a worst case.   Earth's Mt. Everest is almost three times that high, even the old and eroded Cascade Mountains in the US Pacific Northwest have several volcanos well over the 3000 meter mark.  Lower-gravity worlds are likely to have features many times larger still.</p> <p>As we discussed above, though, the primary problem here are those 1m resolution polygons.    At the opposite end of the equation, I could represent the entire United States (badly) with a single polygon.  It wouldn't capture most of the details, but with a reasonable texture on it, and viewed from, say, the moon or high orbit, it might work fine. </p> <p>The \"Secret Sauce,\" of course, is to mix the levels.   Things nearby the player should be modelled at higher resolutions, things further away can be modelled at progressively lower resolutions the farther away they are.   It would probably be possible to build a continuous version of this (where every polygon has its vertex resolution determined by how far it is from the player), but more generally LOD systems tend to be zoned.  For example, terrain within a kilometer of the player is at 1meter resolution, then 2 meter resolution for another kilometer, 16 meter resolution for another few kilometers, and 128 meter resolution beyond that (these numbers are made up for example and will be highly dependent on the particular engine and game.)</p> <p>Zones will usually \"round\" to chunks of some sort.   For example, if we're using Unity Terrain objects with 512x512 meter edges, it would be reasonable to have the Terrain in which the character is located be at 1meter resolution, as well as the nine or twenty-four Terrains encircling that one.  Beyond that, the Terrain objects may be built at lower resolutions (and possibly higher sizes) out to whatever maximum visibility we allow.    As the player moves and leaves their current Terrain, the ones that are now \"too far\" from the player are discarded in favor of lower-resolution versions, and new ones that have come \"in range\" are generated or re-generated at higher resolution.</p> <p>Note:  Unity terrains have an internal level of detail that describes their shapes using a minimum set of polygons:  rough areas have more polygons than smooth ones (which discard vertices which would just be in a plane described by the surrounding ones).   This is done automatically and internally as the shapes of the terrain change, and scales with the scale of the terrain.    So we get this extra optimization \"for free,\" at least until our terrains become too complex.</p> <p>We may be able to apply additional optimizations:   If the game does not allow a player to move underwater, it's reasonable to simplify \"undersea\" terrain significantly, since the player will never see it.   A very sophisticated engine might be able to determine that large amounts of terrain are \"over the hill,\" occluded by a mountain, or otherwise blocked by the existing terrain from the player's view point, and not generate that space at all until needed.</p>"},{"location":"tags/","title":"Articles By Tag","text":""},{"location":"tags/#articles-by-tag","title":"Articles by Tag","text":"<p>{{ tag_content }}</p>"},{"location":"unity-terrain-types/","title":"Terrains and TerrainData","text":"<p>Unity has a \"Terrain\" component that can be added to any gameObject.   This combines a mesh, a level-of-detail (LOD) system, as well as layered textures and systems for representing trees, plants, rocks, and other details.  For build-time terrain objects, there are extensive tools available in the Unity editor for shaping, texturing, and adding objects to terrains.</p> <p>Some of these components are deferred into a TerrainData class; one TerrainData is present in each Terrain object.   Generally, it's not important for this discussion which attributes are where, but most of the data heavy objects (alpha, detail and height maps, for example) are in the TerrainData object.</p> <p>We won't use all of its capabilities.   For example, Terrains have a navMesh component which can be used to determine where players or NPCs can travel.   Because this is \"baked\" at build time, it can't easily be used with entirely procedurally generated terrains.</p>"},{"location":"unity-terrain-types/#heightmaps","title":"Heightmaps","text":"<p>Unity Terrains are heightmap-based; that is, they consist of exactly one height (y) value for each (x, z) coordinate.   As such, they are effectively 2.5D objects -- they cannot themselves represent overhangs, bridges, caves, etc.   If such things are needed, they must be composed of multiple terrain objects, added on as separate meshes, or implemented using some other technique such as voxels.</p> <p>A quirk of Unity heightmaps is that they are stored as (z, x) values rather than (x, z) ones; it's not clear why the indices are backward, and different types of maps on the TerrainData object handle this differently, so you need to keep an eye on it.</p> <p>Terrain objects can be stitched together to create larger terrains, and individual terrains within the group loaded and unloaded as necessary for resource management.   For editor-built terrains, this stitching is automatic; we'll need to do it manually for ours.</p> <p>The heightmap data is provided as an array of values from 0 to 1, and this brings us to our first challenge.</p>"},{"location":"unity-terrain-types/#the-problem-of-scale","title":"The problem of scale","text":"<p>Terrain objects (or more specifically, their TerrainData substructure) have a field called size.   When the Terrain gameObject is first created (from a supplied TerrainData), this is set to a vector containing the x, y, and z dimensions of the object.   \"x\" and \"z\" will be determined from the number of samples in the heightmap and their resolution, but \"y\" is always set to 1.  Since that value corresponds to the maximum height in the terrain in world units, \"1\" wouldn't work for all but the very flattest of regions.   So we can snag the y value and set it to something else:</p> <pre><code>  TerrainData td = new TerrainData();\n  Vector3 tdsize = td.size;\n  tdsize.y = 1200f;\n  td.size = tdsize;\n</code></pre> <p>The question is, what do we set it to?</p> <p>The heightmap [0,1] values are interpreted in the scale of size.y.    If we set it to 1200 (as shown in the code example), that means a value of 0.0 in the heightmap is 0 world units high, and a value of 1.0 is 1200 world units high.  </p> <p>For the typical interpretation, that's 1.2 kilometers of vertical range over whatever our horizontal dimensions are.  Terrain objects are typically pretty small -- maybe a square kilometer at the largest, 512 meters square being more common.   (Sizes need to be a power of two).   Very steep mountains might have areas where they gain 1.2 kilometers of height in less than a kilometer of horizontal space, but we could probably live with that limitation for our engine, and if we couldn't, a size.y value of 2500 would probably cover even the exceptional cases.</p> <p>...but wait.</p> <p>Remember that our terrain object is a patch of a larger world, and we have to stitch those terrains together seamlessly.</p>"},{"location":"unity-terrain-types/#how-high-is-mt-everest","title":"How high is Mt. Everest?","text":"<p>The first question we need to answer is: \"What does 0 mean as a height?\"    It's the lowest possible value for that terrain patch, but what does that mean?</p> <p>Consider a relatively flat plain, where there's no feature higher than 1 meter over our square kilometer patch.    Assuming size.y is set to 1000, that means that the difference between the lowest heightmap value and the highest one will be 0.001.</p> <p>Does that mean that all the values will be between 0 and 0.001?</p> <p>If we have two such plains, but one (call it \"Plain A\") has values between 0 and 0.001 and the other (\"Plain B\") has values between 0.5 and .501, what's the difference between them?   We'll come back to this question in a bit.</p> <p>Mt Everest's height is often given as about 8,800 meters, but that's from sea level.  From its \"base,\" it's between 3,500 and  5,000 meters (it's a big mountain, the base covers a lot of area.)    And if we're measuring in a system that starts at zero and needs to cover every undersea area, as well, our \"absolute zero\" point would be the bottom of the Marianas Trench, which would add maybe another 11,000 meters, for a total of about 20,000.   An argument might be made for going deeper than that, too, in case we needed to account for undersea drilling or somesuch.</p> <p>Any of these values are reasonable.   </p> <ul> <li>If we're representing a 1km square chunk of Everest using a 0 - 20,000 range, a large amount of the range will be \"wasted,\" that could be used for greater precision (see below).  </li> <li>If we don't use a 0 - 20,000 range, we need to keep track of what \"zero\" height for whatever range we do use is.   It also means that a heightmap value of, say, 0.5 will mean something different for each individual terrain.   That difference needs to be accounted for at stitching time.</li> </ul> <p>The heightmap values are floating-point, and there's no way to make use of negative values, so our range limit of roughly 0.0 - 5,000.0  for the scaled height applies.   If we have a larger range than that, we'll start to lose resolution; at 0.0 - 20,000, our precision will drop to about a tenth of a meter.</p> <p>That may be fine:  terrains are fairly coarse objects, and maybe centimeter resolution isn't required, especially because the horizontal resolution of vertices is usually an entire meter.   Whether or not this produces visible polygonization artifacts and jitter will likely require testing on your target hardware.</p> <p>Does your world have a Mt. Everest and a Marianas Trench?   If it's modelled on the real world...maybe.    Airplanes are notoriously bad at flying underwater.   Does your flight simulator need to measure distances below sea level?    Naval simulators are unlikely to get near tall mountains.   Subnatica takes place almost entirely underwater; the highest land areas in the game are less than fifty meters high.</p> <p>If you're making a computer game, unless \"huge vertical range\" is a specific feature, there are good arguments for a much smaller one.   Those Skyrim mountains are very short:  not because the artists were afraid of heights, but because climbing five thousand feet up the side of a mountain takes a lot of time. That's more than the player likely wants to commit.   Realistic or not, a five kilometer \"range\" from the deepest ocean to the tallest mountain is a pretty big canvas for an adventure.</p>"},{"location":"unity-terrain-types/#height-models","title":"Height models","text":"<p>So there are at least three possible mechanisms we can use here, each with some pros and cons.   Let's look at each one in turn.</p> <p>Uniform Range, Uniform Base (UR, UB):  In this model, we use the same range for every terrain patch; it represents the entire distance from the lowest point to the highest point in our game world.   Stitching is automatic as long as the patches generate compatible edges.    In this model, the difference between Plain A and Plain B above is altitude, Plain B is higher above sea level (or closer to sea level if undersea) than Plain A is.  Because the scale, range, and interpretation of every terrain patch is uniform, this is relatively easy to implement.   The major downside of it is that the entire world's highs and lows need to fit into the chosen range, which means either limiting it to 5 kilometers of vertical scope or accepting some loss of resolution (and associated artifacts) to get a larger one.</p> <p></p> <p>Uniform Range, Dynamic Base (UR, DB):  In this model, we use the same range for every terrain patch (so a \"hill\" 0.15 high will be the same size in every patch), but interpret the bottom of the range differently for each patch based on it's local highs and lows.   For example, a patch from the side of a beach rise might range from 0 to 500 meters above sea level, the next patch \"uphill\" from 400 to 900 meters.   This involves keeping a \"base height\" for each terrain that indicates what \"zero\" means for that patch.    This method allows us almost unlimited global range.  In implementation, the entire terrain object will be moved up or down relative to its neighbors to achieve the correct relative position.   This is only slightly more difficult to implement, and allows a sort of \"floating origin\" to keep the terrain chunks near the player in the 10K window (negative values are possible here because you can position the actual terrain lower than zero).   Whether stitching is problematical or not will depend on implementation details; assuming the edges match in \"real world\" heights and the terrains are properly positioned vertically, things should match up.  But because there are more transforms involved here, there's a chance of small rounding differences showing up as holes along the patch edges.    </p> <p></p> <p>Dynamic Range, Dynamic Base (DR, DB):  In the most complicated model, we use the entire range available for each patch to cover it's local highs and lows.   That is, if the lowest and highest points in a patch are 300 and 500 meters, the range for the patch will be 200.    If the low and high are -500 and 3000 within that single patch, the range will be 3500.   This needs to be coupled, like the last case, with a dynamic height to indicate what \"0.0\" means in that patch.   Generation also becomes harder because the same \"distance\" in heightmap [0,1] space means something different in each patch.   Rounding is almost certain to be different between patches, making stitching difficult.</p> <p></p> <p>(The missing case here, Dynamic Range and Uniform Base, combines the worst of all worlds and doesn't really merit discussion, \"implicit\" stitching will almost certainly fail because of artifactual differences between patches.)</p> <p>The DR, DB model only makes sense for scenarios where very fine gradations in height are critical, or where there are patches of very extreme local height differences (five kilometer high sheer cliffs or some such) to justify the need for dynamic range.</p> <p>The other two have the advantage that their implementations are similar enough (basically some fixed vertical offsets) that migrating from one to the other at any time would be both simple and could be done locally at the point of terrain generation.    So it makes sense to implement that easier (UR, UB) model initially, and move to the UR, DB one if the need arises.</p>"},{"location":"unity-terrain-types/#stitching","title":"Stitching","text":"<p>\"Stitching\" is the process of making sure there are not visible seams or holes where the terrain patches meet each other.  Ideally, we'd like \"implicit\" stitching.  That is, for the edge vertices (which are in common to both patches), we'd like those edges to have exactly the same values.   Since we're using floating point numbers, that pretty much means they need to be generated in exactly the same way, by a deterministic process that's idempotent, a Scrabble-caliber word that just means that the same inputs always produce the same outputs.</p> <p>Most Perlin/Simplex noise generators meet this criteria, so if our algorithm uses the same position values in each patch, it should produce identical heights along those edges (realizing that the height range of the patch is an input here, which is where uniform ranges are a benefit).    In the best cases, implicit stitching allows our implementation to do nothing at all and get proper behavior.   The edges just \"line up\" in space as a property of being in the same position.</p> <p>If we can't assume that our generation for different patches will produce exactly the same results, we then have to consider an actual stitching algorithm.   This is likely when the two patches have even slightly different geomes:  they may both be using Perlin-ish noise, but probably not the same Perlin-ish noise for a mountain as for a foothill.    And once we start implementing erosive forces, they may also have different effects on the \"same\" position in neighboring patches.</p> <p>There are several possible algorithms.   Unity has a built-in one, that basically takes an \"old\" and a \"new\" terrain and forces the values of the \"new\" terrain to match the \"old\" one, even when new heights are provided.   This is really meant for use in the editor, and there's some uncertainty around its use in procedurally generated terrains.    An experiment for another day, perhaps.</p> <p>If we expect that two adjacent patches have very similar edges (e.g. they differ only by rounding differences or a just a few centimeters of displacement), we can use a <code>for</code> loop to run along the edge of one or both terrains and set their height values to match (either one taking the other's values, or both taking the average of them.)   Note that corners (which are shared by four patches instead of two) may need to be handled separately.</p> <p>If the differences can be larger, a more complex solution is called for, to prevent large \"corrections\" happening within the small one-meter distance to either side of the seam.   Generally, this involves creating a \"skirt\" in the last several meters of each terrain that can be manipulated by something like midpoint displacement to \"spread\" the change in altitude out over a larger space.   (In the simplest implementation, just divide the distance delta by the number of meters of skirt and apply that fraction to each vertex.)    This algorithm isn't incredibly difficult, but it's got some weird edge cases (pun necessary), especially at the corners, and it requires a fair amount of computational time.    </p> <p>So ideally we'll want to build implementations where the edges don't differ enough to need skirt stitching. If we use this requirement to inform our choices of generation algorithm, we can hopefully avoid expensive skirt computations.</p>"},{"location":"unity-terrain-types/#multiprocessing","title":"Multiprocessing","text":"<p>There are two big caveats we need to be aware of in our stitching algorithms.   The first is multiprocessing.   Virtually all \"chunk\" or \"patch\" based implementations are going to need to generate several of those chunks at a time (probably nine initially, 3-5 at a time as the player moves about).    Most devs will want to do this with multi-processing techniqes; at least co-routines, and probably something like the Jobs system.</p> <p>Multi-processing is only simple if the \"units\" of work are entirely independent of each other.  (Non-implicit) stitching depends on at least two different patches.   So the implementation either needs to make sure that the patch generations are far enough apart in time that the \"existing\" patch is always complete when the \"new\" one needs it for stitching, or else separate stitching out to a single-threaded post-processing step.    The latter is likely easier, especially if an inexpensive stitching algorithm is used.</p> <p>The same sort of considerations apply if you're trying to use a GPU shader to do the terrain generation (you can find GPU implementations of things like 3D Simplex noise out on the Internet.)</p>"},{"location":"unity-terrain-types/#distortion","title":"Distortion","text":"<p>The second consideration for (again, non-implicit) stitching is that it changes one of more of the terrains as it adjusts the heights along the edge.   If that edge is sufficiently close to the player camera, the result may be a visible distortion of the ground over either the edge or the entire skirt area, depending on the stitching algorithm.</p> <p>Of course, if the player can see the edge, they can presumably also see the \"new\" terrain popping into place, which is far more distracting.    In both cases, the solution is to make sure that new patches generate at a distance from the player where they're either invisible or sufficiently tiny as to not matter, and do stitching as early as possible, long before the player approaches the edge.</p> <p>Most chunking algorithms keep a \"buffer\" of loaded space around the player anyway, so this isn't likely to be a big problem.   But you should do stitching before placing detail objects on the terrain, since the ground may shift underneath them.</p>"},{"location":"unity_base_classes/","title":"Unity MonoBehaviour vs. ScriptableObject vs. Standard Classes","text":"<p>In my first few weeks and months of learning Unity, I made a lot of mistakes and had a lot of misunderstandings about what base classes worked best to create my \"things\" on. </p> <p>This doesn't have anything to do with Procedural Terrain Generation (and in fact, if you're reading a multi-thousand-word rambling treatise on PTG, you're probably past these learning steps).    But my reading indicates there's still a ton of misunderstandings about these out there, and the winds of GoogleBing can bring all sorts of readers to these shores.</p> <p>So let's review.</p>"},{"location":"unity_base_classes/#what-are-we-talking-about-anyway","title":"What are we talking about, anyway?","text":"<p>Unity's scripting environment is a C# framework (actually several) that provides APIs useful for game development and related tasks.    Like most frameworks, it provides \"root classes\" that your own classes can derive from.</p> <p>When implementing a class to provide functionality in a Unity app, we generally have three choices:</p> <ul> <li>Inherit from UnityEngine.MonoBehaviour</li> <li>Inherit from UnityEngine.ScriptableObject</li> <li>Don't inherit from either (just use a \"standard\" C# class).</li> </ul> <p>As far as the language is concerned, these are all the same:  MonoBehaviour and ScriptableObject are just C# classes that can be inherited from, like any other.    But they are used quite differently.</p>"},{"location":"unity_base_classes/#monobehavior-gameobject","title":"MonoBehavior / GameObject","text":"<p>Classes derived from MonoBehaviour create components which are added to a Unity GameObject.  (At a conceptual level, a GameObject is basically just an empty container for it's components.  By default, they contain nothing at all except a transform, and even that's a component.) They can define what the GameObject is (Mesh, Terrain), or just add capabilities and properties to it (transforms, navagents, colliders).  There are two reasons to use a MonoBehavior:</p> <ul> <li>For any entity that has \"physical\" presence in a scene.  This is by far the most common usage of MonoBehavior.  Anything you add into the Hierarchy window is a GameObject, usually with MonoBehaviour components on it.   If your player can look at it, or through it, or more generally anything for which its position in the virtual world (or screen) is important, use MonoBehaviour.  For example, a monster, camera, terrain, treasure chest, tree, mesh, UI Button, etc.   </li> <li>For any non-physical entity that needs the \"heartbeat\" of frames or the lifecycle of scenes.   If you want your object to be created when a scene loads and deleted when it goes away, or if you need to do something at regular intervals guided by Update() or its ilk, use a MonoBehaviour.  Examples here tend to be called \"managers\" and implement things like network connections, quest and dialog systems, spawners and deleters of game entities, and other things that do work every frame (or every few frames) and come and go with parts of the world.</li> </ul> <p>GameObjects (and therefore their associated MonoBehaviours) are created with scene entry and deleted on its exit.  Conceptually, they exist as part of the world that's represented by each scene.</p> <p>It's possible (through the DontDestroyOnAwake() mechanism) for a game object and its MonoBehaviours to survive between scene changes---and there are even some good reasons why you might do so---but generally needing to do this is a sign that your design could use some work.   For example, if your managers can save and load their data, as they're destroyed and recreated, they can just be included in the scenes they're needed for and left out of others.    Or even more commonly, they would be better represented as one of the other two class types.</p> <p>MonoBehavior interacts with the Unity Editor; adding a MonoBehaviour script to a GameObject lets you edit that behavior's public and [SerializableField] properties from the Unity Inspector window.   This alone is not sufficient to require MonoBehaviour, since ScriptableObject also allows inspector editing.</p>"},{"location":"unity_base_classes/#scriptableobject","title":"ScriptableObject","text":"<p>ScriptableObjects are assets, like files, materials, images, sounds, prefabs, and the like.   They have no inherent presence in the game world, and are not tied to the lifecycle of scenes.   They are, by definition, persistent unless explicitly destroyed.   </p> <p>Reasons to choose ScriptableObject:</p> <ul> <li>One or more instances of your class needs to exist (or at least be available) throughout the lifetime of the application.  Managers that don't need integration with the Awake/Start/Update/FixedUpdate sort of cycle are good candidates for ScriptableObjects.</li> <li>You want to allow devs or artists to create and edit them using the Inspector.  Adding the [CreateAssetMenu] attribute to the class will allow them to be created from the Asset menu or by right-clicking in the Project window, like any other asset.</li> <li>To define a specific set of values for entities, as a sort of \"template\" for instantiating later (think of it as a much more generic version of a template, or a database---see below.)</li> </ul> <p>Like MonoBehaviour, ScriptableObject allows you to edit public and serializable properties from the Unity Inspector window.</p>"},{"location":"unity_base_classes/#example","title":"Example","text":""},{"location":"unity_base_classes/#first-implementation","title":"First Implementation","text":"<p>That last case may require an example.  Let's say we're making an RPG game with monsters: Orcs and Kobolds.   We'll need to be able to place or spawn these monsters into the scene, so they're going to be MonoBehaviors attached to a gameObject.</p> <pre><code>public class Orc : MonoBehaviour {\n  public int maxHitpoints = 10;\n  public int hitPoints = 10;\n  public int damageDone = 5;\n  public GameObject prefab;\n  ... the rest of the stuff it does..\n}\n\npublic class Kobold : MonoBehaviour {\n  public int maxHitpoints = 3;\n  public int hitPoints = 3;\n  public int damageDone = 1;\n  public GameObject prefab;\n  ... the rest of the stuff it does..\n}\n</code></pre> <p>This works, and for a small enough game, it might actually work fine.   But it has some problems:</p> <ul> <li>Almost all of the code is duplicated, except for some constant values</li> <li>If we create a new \"Goblin\" monster, we need to copy paste and create a new class (and since it's in code, a dev needs to do it).   That might become unweildy in an RPG with 50 or 60 different kinds of monsters.</li> </ul>"},{"location":"unity_base_classes/#more-flexible-implementation","title":"More Flexible Implementation","text":"<p>One of the uses of ScriptableObject is to allow a \"template\" to be created as an asset, with no new code necessary after the initial creation.    So, consider this:</p> <pre><code>[CreateAssetMenu(fileName = \"MonsterSpeciesData\", menuName = \"MyCoolGame/MonsterSpecies\")]\npublic class MonsterSpecies : ScriptableObject {\n  public string name;\n  public int maxHitpoints;\n  public int damageDone;\n  public GameObject prefab;\n}\n</code></pre> <p>Now any time a developer, artist, or other team member wants to add a new monster type, all they need to do is right-click in the Assets folder, choose \"MonsterSpecies\" from the \"MyCoolGame\" submenu, name the asset, and define the values for that monster.  Notice that \"hitpoints\" isn't defined here, since it's an attribute of a particular monster, rather than all of the monsters of a given species (if you damage one orc, the others remain undamaged.)</p> <p>Then, we can just have a generic \"Monster\" class that references it:</p> <pre><code>public class Monster : MonoBehavior {\n  public MonsterSpecies monsterSpecies;\n  public int hitPoints;\n    ... generic monster-y behaviours ...\n}\n</code></pre> <p>This ability to treat \"Monster Species\" as an asset, the same way you'd define a Material asset for painting a mesh, makes adding new species of monsters require no code at all, just connecting stuff up in the editor, which frees up your devs to do more development work, and enables your artists and game designers to do their work without needing the devs.</p> <p>This same sort of thing used to be done using a database (and if you've got very large amounts of data, the database is still the better solution), but for small-to-medium size datasets, defining assets is easier for everybody.</p> <p>Important:  Note that like all assets, ScriptableObjects created while the game is running in the editor will be removed when you end the game.   This makes it easy to \"try out\" a new monster race even while the game is running, but also means you'll lose your work if you forget and build an asset meant to be permanent during a run. </p> <p>Having just one MonoBehaviour class for all monsters may or may not be sufficient, depending on whether the behaviors of those monsters differ in ways that need to be expressed in code (for example, bipeds vs. quadrupeds vs. snakes may be sufficiently different in motion that actual subclasses make sense), but you'll need many fewer of them.    (And you might reduce further by defining \"Locomotion\" ScriptableObjects for various locomotion methods, and make them available as assets, too.)</p>"},{"location":"unity_base_classes/#generic-class","title":"Generic Class","text":"<p>If you've got an entity that doesn't require any particular integration of Unity mechanisms at all -- say something purely mathematical, or \"business logic\", or AI algorithms, you can just use an ordinary C# class.  Nothing in Unity requires you to derive your classes from MonoBehaviour or ScriptableObject if you don't need the capabilities they offer.   Such classes are available only from code; they have no presence in the Unity Scene Hierarchy, Project Hierarchy (except for the script files that they're defined in), or Editor.    They must be instantiated and destroyed by code, and cannot expose properties to the Inspector.</p> <p>Which sounds like a lot of limitations, but really, that's just code, of the kind you've probably been writing for years.  It works just fine in Unity, it just doesn't do anything special there.  It also describes basically all third-party C# libraries and APIs, unless they are specifically designed for Unity.</p>"},{"location":"unity_base_classes/#conceptually-assets-are-databases","title":"Conceptually, Assets are Databases","text":"<p>The title of this section isn't literally true:  assets are serialized and deserialized by Unity in various ways, but they aren't actually backed by a traditional database, so far as I know.    But if your programming background is from the database world, it's a powerful metaphor for understanding the Asset library, and ScriptableObject's role in it.</p> <p>Different databases use slightly different terminology, but generally speaking a database table defines a group of related fields (\"columns\" in some terminology) and their types; strongly analogous to a struct or class that contains only properties and variables.   Tables are types: they define the structure and layout of data, they aren't the data itself.</p> <p>A database row or entry is the instantiation of that: the specific set of values for one entity.   Rows are variables (often constants); they define the values for one instance of the type defined by their table.</p> <p>At this very basic level, you can think of the project's Asset folder as a database:  the types of assets available are the tables.  This includes the ones Unity gives you:  Material, Prefab, Textures, Sprytes, Images, and so on and on.   It also includes any that you define yourself via ScriptableObject.   A ScriptableObject class definition is effectively a database table definition.</p> <p>When you actually right-click and create an asset of any type, you're conceptually adding an entry/row to that table.</p> <p>Assets are traditionally used for compile-time databases:  usually constant elements that define the available options built into a game:   The materials things are made of, the logos displayed in your UIi, the kinds of monsters, the kinds of treasures, the kinds of items that players can pick up, towns and dungeons in your world, etc.    They usually don't change, or at least don't change much, as the game executes.</p> <p>Assets are typically not used for the sort of updated-at-runtime databases you might use to store information that changes at runtime:  character names and avatars, the \"picked up\" state of items, the character's actual inventory and equipment.    That sort of dynamically changing information tends to go into a save file or traditional database.    </p> <p>There's no technical limitation here:  you can create and modify assets at runtime, you could use them as your save game storage.  But doing so makes it very hard to \"reset\" the game to starting conditions, and it's not a best practice.</p> <p>This isn't a perfect analogy:  ScriptableObjects can add behaviors---not just properties and variables, and sometimes you'll have ScriptableObjects that are entirely code and define no traditional fields at all.   This often isn't true of more traditional databases.  Many don't allow \"executable\" entries in their tables (in many traditional usages, this could be a security nightmare), nor the inheritance ScriptableObject gets from its C# nature.    And most databases provide querying and combining operators that aren't needed in the ScriptableObject case because C# or .NET provide equivalents.</p>"},{"location":"water/","title":"Water","text":"<p>Ignoring things like precipitation, steam, fog, and ice, liquid water exists in three forms in most open-world games:  oceans, ponds/lakes, and rivers.  The exact form these take will vary, depending on the game and the player's abilities within it.   Often, water is simply a decoration, a sort of scenic, \"wall\" that acts to contain and direct the player.   In other games, players can swim or dive in the water, making it a full-fledged environment in its own right.   Subnautica and other games in the \"diving adventure\" genre may take place nearly entirely underwater.</p> <p>Water is usually transparent to one degree or another, which means that visually it's an addition to another terrain type, not just one of its own.  (It also usually means it's a little more expensive, performance-wise, than other terrains.)</p>"},{"location":"water/#common-features","title":"Common Features","text":"<p>Up until now, our focus has been entirely on world generation and the dynamic loading of terrains.   It has generally not been about the characteristics of the games that might use it: that is, we're generating the world, not defining how the player interacts with it.  That's mostly still going to be true, but water is \"special\" enough that we'll at least talk about game implementation possibilities.</p> <p>If water is a barrier to the player--if they cannot enter it at all, or cannot enter it past walking depth--then the implementation is pretty simple:  the player will never \"see\" from underwater, the edges of the water can be limited like any other terrain boundaries, and all you really need is a good shader for the surface(s); many free and paid ones exist for all of Unity's shader pipelines.   For most such scenarios, the water \"surface\" can be a simple plane.</p> <p>But if the player can swim, we're opening up a bunch of new considerations.</p>"},{"location":"water/#vision","title":"Vision","text":"<p>Generally, we can't see as well or as far underwater as we can above it.  Natural water is often not entirely transparent -- debris, algae, cyanobacteria and the like will physically limit the distance we can see.    This is very, very similar to fog above water, and we can implement it the same way:  distance or volumetric fog, or both.   Land fog is typically white or grey; underwater fog-like effects will usually take on the colors of whatever's causing it:  muddy browns, various greens and blues, red from blood, or whatever.</p> <p>Even in the absence of particles, human vision is blurred underwater.  This is because our eyes evolved for the the refraction at the surface of the eye to be with the eye itself and air.  (For the opposite reason, many amphibious creatures can't see well in air, because their eyes expect an eyeball/water boundary.).  Water bends light more dramatically than air, and provides more pressure on the eyeball itself.   The refraction is expecially significant if you're looking from the air into water in which something is partially submerged.  Even in relatively shallow water, distances and even true positions are difficult to judge.</p> <p>Ray tracing systems can work with refraction directly. Most other games tend to just ignore its effects (or \"roll them in\" to the fog, or assume the player is wearing a mask or goggles), because they're difficult to implement and generally deleterious to game play.  It's very unusual for a game to implement surface refraction, for example.</p> <p>Air and water are both generally moving, but the effect is considerably more obvious in water.  This causes changes in density that both visibly affect refraction and eyeball pressure.  The resulting \"wavy\" distortion is easily simulated with post-processing if you want it.</p> <p>Finally, water isn't completely transparent.  As you descend, less light from the surface reaches you, and the darker the surroundings will be.  In the real world, light is very dim by 200 meters down, and even in the best conditions completely dark by 1000 meters down.   Whether or not this matters to your game depends on whether there's any ability of your characters reaching such depths.  Note that this only applies to sunlight:  emissive surfaces (e.g. underwater lava), bioluminescence, and player-carried illumination can all bring light to the depths.</p> <p>Generally, these effects are placed based on the camera position, not those of the player's eyes (except in first-person where the two are coincident).   This can cause a very common effect where by placing the camera right at the surface of the water, you can often \"see\" underwater using the above-water lighting and visibility settings.   Solving this is extremely expensive; even AAA games tend to just live with it.</p> <p>Note: Want to solve it?  Calculate the line where the surface plane intersects the \"near\" plane of the camera frustrum.   Create a second camera at the same position.  Apply the underwater settings to the second camera, and render to a texture that is invisible to the underwater camera, but rendered on a plane perpendicular to the above-water camera with its top at the waterline.  Expect a drop in frame rates, since you've effectively just doubled the amount of stuff you're rendering.</p>"},{"location":"water/#armature-position","title":"Armature Position","text":"<p>Generally there are three basic character postures for our player's avatar model:</p> <p>Walking: In shallow water, they will walk, run, or stand just like they do on land.  (If they're doing it exactly the same as they do on land--i.e. on top of the water itself--remember to turn off or remove the mesh colider for the water surface object!)</p> <p>Swimming: The normal standing posture will continue until the player reaches some depth, typically either the waist or the shoulders (shoulders is more realistic), at which point they will pivot to a \"swimming\" posture.   This is prone along the surface of the water.  Hands and legs tend to stay near the body or each other except during strokes.   Typical swim animations are either the Australian crawl, some sort of dog paddle, or a breaststroke.  The latter two have the advantage that the eyes remain forward looking, and are much more likely in a world where swimming is a very occasional occurrence, rather than something that people would train in.   Swimming animations tend to mostly ignore the up-down motion of real swimming in favor of \"sticking\" to the water surface.   It's just as two-dimensional as walking/running, just in a different positure, and with greater penetration of the \"ground.\"</p> <p>Diving/Underwater: In some games (for example, Valheim), that's all the swimming that's allowed.   But most games in which water is a traversable environment also allow underwater movement.   This is typically initiated by a sort of head-down dive.  Once the body is below water, orientation becomes somewhat arbitrary; the swimmer is effectively in a zero-gravity environment.  The arms and legs tend to be spread further away from the body and used for larger, more sweeping motions.  For humans, direction changes are fairly rapid (like a figure skater pivoting), but orientation changes (where the spine and head point, relative to \"up\") are quite slow (probably because they require movements that make no sense on land).</p> <p>These three positions are mutually exclusive; probably different animation layers or controllers altogether.</p>"},{"location":"water/#surface-distortion","title":"Surface Distortion","text":"<p>Except in exceptionally windless conditions, water is almost never perfectly flat.  Moving water never is.  Larger bodies of water are also subject to waves and tides from non-wind sources.   While games often present a sort of \"foam\" effect at the edges of water, this is relatively uncommon in the real world, especially in freshwater.    Water that moves rapidly over uneven surfaces will incorporate air and become whiter and more opaque.   Waves that reach a certain height and shape will \"break,\" again incorportating air and scattering droplets.  Disturbed water will make characteristic expanding \"rings\" around the point of disturbance.</p> <p>All of this is hard to model, and modelling it completely accurately in anything like realtime is well beyond desktop computers.   But it can be faked reasonably well, particularly using GPU shaders.  These are typically of two types:  reflection shaders distort the visual aspect of water; waves and patterns on the water being achieved through (in effect) moving normal maps, where changing the direction of reflections at each point can give a very convicing illusion of an uneven surface.  This illusion falls apart at shorelines, though, where the unmoving \"edges\" of the water destroy the effect.   Reflection shaded water doesn't lap onto beaches, or change it's depth.</p> <p>Surface displacement shaders actually move the vertices of the water, generating \"actual\" waves.  As such, they will rise and fall above the \"beach\" textures sufficiently to model actual incoming waves.    These displacements are also much more convincing when viewed from almost parallel to the water line, because they're actually moving.</p> <p>Available shaders can do either, and often both, of these effects.  Assuming sufficient GPU resources, they're also pretty cheap and provide reasonably good-looking water.   Want to roll your own?  There are dozens of tutorials out there on the Internet.</p> <p>_Note: I talk about water surface \"planes\" a lot in this section, but because our players may be able to see the surface from underneath, the implementation actually uses a very thin solid (a squished cube), instead, so that it has both a top and bottom side.   You can also turn on the flag to make plane meshes visible from both sides, but I've had mixed luck with shaders handling that. </p>"},{"location":"water/#seas","title":"Seas","text":"<p>The first case is the (literally) biggest:  the oceans and seas that border the landmasses of our world.  While not strictly physically accurate, we can just represent this as a single \"sea level\" plane that covers the entire map.</p> <p>This does have the downside that we can't have \"dry\" areas of land that are below sea level (see the page on mountain generation for some cases where that causes issues), but living with that limitation is a small price to pay for the simplicity of a single sea-level plane.</p> <p>...or is it?</p> <p>In all but the simplest graphical environments (maybe a very low-poly game), our water is likely to have one or more shaders on it.  That makes it relatively expensive to render compared to our other polygons, and it's not necessary for the vast majority of non-underwater terrains---basically just the seashore and the ocean.    Even if it's not drawn, though, the engine needs to spend effort culling it.</p> <p>Very large polygons (and ours might be hundreds of thousands of meters along an edge) are also a pain in other ways.   Shaders and textures may or may not tile correctly on them (or require so many tilings they exceed the capabilities of the system), they block views in the scene editor while we're debugging.   They're just not on the same scale as most anything else in the world, which means they're a special case everywhere.</p> <p>And then there's that pesky \"sea is everywhere\" problem.  It's relatively easy for us to solve in our world generation (we simply don't generate parts that are under that layer unless we want them), but what if the player is allowed to manipulate the terrain?  Or if we have caves or other underground structures?   Do they fill with water wherever they're below sea level?</p> <p>We may or may not care.   If our game doesn't need any of that stuff, and is OK with the performance, the single-sea-plane thing might work fine.   If not, we can just generate a local, patch-sized sea plane for those spots we want to have ocean in.  As long as we're careful about tiling so that the edges meet correctly, we should get a decent sea from that.</p> <p>We could also keep the sea plane, but just make the parts of that are under land 100% transparent.   That's harder to do correctly (again, since the scales are so different), but it's a simpler conceptual model.</p>"},{"location":"water/#lakes-ponds-swamps-puddles","title":"Lakes, ponds, swamps, puddles","text":"<p>Water (usually freshwater) accumulates above sea level in various forms.  Some, like puddles, are very transient and perhaps best handled as part of precipitation shaders.    But for larger bodies---particularly ones deep enough for a player to swim or submerge---we'll need to handle them sort of like little oceans, with some exceptions.    </p> <p>There are some really deep lakes in the world: Crater Lake is over five hundred meters, and Lake Baikal is more than three times that depth.   But generally speaking, lakes aren't as deep as oceans, and we can probably accept not having depth-based light fade for simpler implementations.</p> <p>Generally, we'll want to define a simple \"volume\" to contain the water.  The top of this volume will define the water's surface, the bottom will be below the maximum water depth, and the other edges will define the horizontal limits in (x, z) space.  In most cases, we can probably live with a scaled cube.  Recent versions of Minecraft refer to such structures as aquifers, a name that's good enough that I'll steal it.   Unlike real aquifers, these are generally no larger than necessary to contain their body of water.</p> <p>For something like a swamp, we might want our aquifer to cover the entire area of the terrain (and maybe adjoining ones, as well).  For more contained bodies of water, we can make them smaller, and even place them with the body.</p>"},{"location":"updates/global_shapes/","title":"Revisiting Global Shape","text":"<code>#heightmap</code> <code>#realism</code> <p>So we've been living with a fair number of limitations in our terrains.   It's time to loop back and see if we can eliminate some of these problems.   But first, let's enumerate them and see if we can understand their root causes.</p>","tags":["heightmap","realism"]},{"location":"updates/global_shapes/#beaches","title":"Beaches","text":"<p>We'll start with one of the more straightforward issues:   the interface between land and sea isn't very good.   We get a lot of this sort of thing:</p> <p>(Image)</p> <p>Effectively multi-kilometer stretches of beach with shallow tide pools, followed by a very, very slow deepening into the ocean proper.    The opposite effect occurs inland; the land rises very slowly from the sea, so slowly that any terrain feature that descends below the base terrain height is likely to \"fill\" with water, sometimes even dozens or hundreds of kilometers inland.</p> <p>There exist places like this in the real world, but they're not the norm.   Generally, beaches extend less than a kilometer inland from the water edge, and the \"tide pool\" zone is at most a few dozen meters.   In other places, there are almost no beaches at all; just steep cliffs that fall off directly to the sea.  </p> <p>Once offshore, what happens depends on the coast's relationship to continental plates; either a gradual deepening or an abrupt falloff to great depths within a few kilometers of short.  In either case, the effect is not nearly so gradial as we've generated thus far.</p> <p>The hardest part here is the \"cliff\" scenario:  viewed from above, a cliff is a rough, irregular line of effectively no width.   Our heightmap-based terrain is capable of representing that (although the fixed position of the coordinates means that the cliff will need to be at least one meter further out at the bottom than it is at the top).   But in the other direction, those cliffs can run for hundreds of kilometers, sometimes interrupted by small stretches of more traditional beaches.   Our current model isn't very good at generating sharp discontinuities that extend to the edges of individual patches or beyond, and our current use of heavy smoothing will tend to \"round\" such cliffs in any case.  (And of course, undercuts and sea caves aren't possible at all using simple height maps.)</p>","tags":["heightmap","realism"]},{"location":"updates/global_shapes/#stitching","title":"Stitching","text":"<p>Our stitching model still has bugs:  When irregular features intersect patch edges, you get ugly and unnaturally-straight \"cliffs.\"</p> <p>(Image)</p> <p>There are still visible (and depending on your model, traversable) gaps at some of these discontinuities, particularly at corners, probably caused by the stitching algorithm being applied more than once to one side of a patch.  The corners of patches also tend to produce weird discontinuities.   All of these issues are largely hidden in nearly flat terrains, but it's difficult to traverse any distance in hilly or mountainous ones without encountering them; they're certainly not stable enough to build games around.</p> <p>Ideally, we'd like to eliminate stitching altogether:  doing this would mean that any two adjacent patches would need to generate identical edges: each with no more information about the other patch than it's four corner heights and coordinates.   That's not as impossible as it sounds (for example, Perlin-class noise requires nothing more than coordinates across the entire floating-point space), but it's going to require some more complex algorithms than we've been using, thus far.</p>","tags":["heightmap","realism"]},{"location":"updates/global_shapes/#pointiness","title":"Pointiness","text":"<p>While we're addressing stitching, it's probably time to get rid of the polygonal base shapes of our mountains.   We've been using erosion to cover it up somewhat, but it's still really obvious:</p> <p>(Image)</p> <p>The erosion is also causing us stitching problems (see above), so relying on it to do such heavy lifting is likely to cause us problems later.</p> <p>With a very few exceptions, our mountain problem is planarity.  We've covered earlier that mountains aren't cones, but they're not pyramids, either.   Across dozens of tiles, the general shape of our ranges is fine, but once we get down to that local space, they look far too geometrical.  It's especially obvious when (like the image above), we view them from a middle distance away.</p> <p>Mathematically, what we're doing when we generate our local terrains is surface subdivision.  We're currently using the most primitive (and cheapest) form: a linear subdivision that we then perturb with a little perlin noise and then erode the heck out of.    But there are other implementations:  non-rational B-splines (NURBS), various other Bezier subdivisions, Catmull-Clark division, etc.    Most (really, all) of these produce much \"rounder\" division shapes.     Incautious use of these could turn our mountains back into cones again, but they offer a fairly powerful toolset for generating a less \"flat\" base terrain that we can then apply our other tools to.</p>","tags":["heightmap","realism"]},{"location":"updates/global_shapes/#range-generation","title":"Range Generation","text":"<p>At an even more global scale, we've still got a couple issues with our mountian range generation, that are likely to leak into other aspects of our terrain gen.   One is a simple bug:  if you generate a few hundred terrains and look at them, you'll notice that the mountain ranges tend to be clustered toward the origin; they're not evenly distributed across the surface.</p> <p>The other is less subtle:  the ranges tend to be very directional along compass lines: north-south or east-west.   From far enough away, these are basically straight lines.   That's a feature of the very simple random walk algorithm we used; it tends to \"correct\" deviations from a straight line pretty quickly; too quickly, in this case.   It also has really only eight cardinal directions, it's not very good at movine south-southeast, for example.  Since we're using that same walk for our canyons (and we're going to use it again soon, for additional erosive features, rivers, and roads), all of those features are also going to be more directional than we actually want them.</p>","tags":["heightmap","realism"]},{"location":"updates/global_shapes/#scale","title":"Scale","text":"<p>Finally (at least for this batch), we have a subtle scale issue.   We do things like mountain generation, smoothing, feature application, etc. across the global height map: the scale here is effectively \"patches.\"   So a mountain range might be 200 patches long and ten wide, our canyons similarly smoothed and places based on coordinates that are basically patches.</p> <p>The problem is, patch size is a variable in our generation:  they could be 256 meters, 512 meters, a kilometer, or something else on an edge.   And our mountain ranges and such are built based on them.  The net effect is that a world with 256-meter patches will have mountain ranges a quarter as wide and four times as steep as a world with kilometer patches.</p> <p>This also makes it harder to see issues in implementation:  if we're using one size patch in our testing, we might not notice things becoming unnaturally steep or shallow for other patch sizes.   And we'd like the sizes of things like mountains to be either a real-world scale (for simulations), or some explicitly chosen variation on the real world (for games), not dependent on what resolution we use for their subdivisions.</p>","tags":["heightmap","realism"]},{"location":"updates/global_shapes/#ok-lets-get-fixing","title":"OK, Let's get fixing","text":"<p>That's probably enough issues for us to tackle as a next step. so let's address them.s</p>","tags":["heightmap","realism"]},{"location":"updates/grass/","title":"First thoughts on grass","text":"<code>#grass</code>","tags":["grass"]},{"location":"updates/grass/#grass-and-ground-cover","title":"Grass and Ground Cover","text":"<p>Note:  It's been a while since I've put anything here.   I wasn't kidnapped by aliens (that I know of); rather I have been spending much of my time on an actual game that uses this stuff.   That's currently at a place where I can start turning my attention back to the world generation itself.</p> <p>We discussed ground cover a little bit in the Biomes section, but it's time to come back and look at it more deeply.</p> <p>In the real world, we have grass.   It hasn't always been that way:  prehistoric dinosaurs didn't eat grass, because it hadn't evolved yet.  But these days, it's pretty ubiquitous, and even the modern dinosaurs sometimes eat it, or at least its seeds.  And where there isn't grass, there's often some other ground cover: ferns, leaves, heather, clover, or potentially hundreds of other species.</p> <p>For our purposes, grass is part of the \"biome,\" which means that it's probably a <code>GenerationDependentObject</code>, and will be selected for or against based on its relationship with the individual terrain patch.</p> <p>Primarily, of course, that's moisture.   Grass grows faster (though not necessarily taller) as moisture availability increases; that's why many people water their lawns.   But wind can remove ground cover; either directly or by removing the topsoil it depends on.   With the exceptions of mosses and lichens, little ground cover grows on rocks or other exposed stone, and in places with relatively loose soil, it won't grow well on steep slopes, either.</p> <p>These aren't all-or-nothing.   Over much of the moisture range, changes aren't represented by \"thinner\" ground cover so much as different species of it.    Even some fairly arid areas have some cover.    For moisture changes that are climatic or seasonal rather than locational, the color and thickness of the ground cover will change throughout the year even if the species remains the same.    Aside from the turning of leaves of deciduous trees in the fall, the color and health of ground cover is often the best way to identify the season or recent weather of an area by sight.</p> <p>The terrain maps we generated much earlier have information for wind, water, and general erosion; the height maps can give us both local and more regional slope information.  So, given a sufficient number of meshes and textures, we should be able to generate <code>GenerationDependentObject</code> specifications for them to allow for us to select them at runtime.</p>","tags":["grass"]},{"location":"updates/grass/#implementation","title":"Implementation","text":"<p>As discussed back in the Biomes introduction, Unity gives us at least three ways to put stuff \"on\" a terrain:</p> <ul> <li>We can just place prefabs there like any other element of a scene.</li> <li>We can place the elements as \"trees\" in the tree layers.</li> <li>We can place the elements as \"details\" in the detail layers.</li> </ul> <p>Each of these has some pros and cons.    Placing prefabs individually is the most flexible, but also the most expensive, performance-wise.  Trees can use wind zones and levels of detail, but are relatively limited in number.   Detail layers allow many more placements, but cannot usr LOD at all, and cut off completely after a certain distance from the player.</p> <p>Within the detail layer, there are a number of sub-options, basically allowing us to choose billboarded textures, actual meshes, and various steps in between.   These are effectively tradeoffs of performance against quality. </p> <p>At the current moment, in late 2022, we're probably going to want to use detail-based ground cover, or else ground cover placed as \"trees.\"</p>","tags":["grass"]},{"location":"updates/grass/#grass-shaders","title":"Grass Shaders","text":"<p>But there's another option:   As GPUs become more powerful and ubiquitous, we can let them render the grass entirely on their own through the use of shaders.</p> <p>\"Shaders\" is a sort of generic, catch-all term for units of functionality that run on the GPU.  There are all sorts of different kinds:  vertex shaders move the actual vertices of models around on the GPU, allowing us to, say, upload a model once to the video card, then animate it from there using just shaders.   Texture/pixel shaders generate the pixel colors at each pixel based on the underlying meshes and input textures: all rendering in Unity is through shaders, for example.  There's one attached to every material.    There are a number of other types, as well.</p> <p>Ideally, we could write a grass shader that let us specify things like the physical characteristics of the ground cover (height, thickness, \"bendability\", color, etc.) and just \"paint\" that material onto the terrain like any other texture, and the GPU would just go off and make some grass for us.</p> <p>These can generate impressive amounts of grass that can do cool things like blow in the wind, automatically adjust LOD for view distances, bend away from a creature traveling through it, or even leave trails behind, and at frame rates that even simple textured quads can't hope to compete with.</p> <p>You can find numerous examples of that online and in the Unity Asset Store.</p> <p>But...there are catches.</p> <p>The first catch is that the easiest way to do this is with a geometry shader. Geometry shaders, as their name implies, create geometry (in this case, whatever triangles make up each blade of grass) during the render pipeline's execution, and draw it based on information passed to the shader.</p> <p>There are a number of such examples present online, and some of them are quite impressive in their functionality.</p> <p>But geometry shaders are a dead technology.  They're relatively memory inefficient even in the GL world where they're most used, and mobile platforms often have trouble rendering them.    Shader experts have been recommending moving away from them for some time, and important ears are listening:  the Apple platforms (based on the Metal rendering engine) don't support geometry shaders at all, and they're discouraged on other modern platforms that use OpenGL or Vulkan.</p> <p>All is not lost:   The replacement for geometry shaders is just to do the same things in a general-purpose compute shader, available on all your favorite platforms.   Compute shaders, as the name implies, allow us to do general purpose computing on the GPU (particularly for highly parallel algorithms, where you're doing the \"same thing\" across thousands or millions of pixels/objects/textures/whatever).  They're one of the underlying technologies behind DOTS and similar efforts.</p> <p>So the trick here is to just create a compute shader that does what the geometry shader did:  generate some additional meshes each frame, and draw them on the GPU when requested.</p> <p>There aren't very many examples of these around, and with good reason.  Computer shaders are (for this task, at least), much more complicated to write than most other shader types, requiring a level of expertise that's not as common as the other kinds (and that yours truly doesn't currently possess).   And of course, they scale in functionality with the graphics hardware they're running on; particularly on Windows and Linux, that's a fair amount of variation that needs to be supported.</p> <p>But as these become better understood and more ubiquitously available, migrating to grass shaders seems like a no-brainer option.</p>","tags":["grass"]},{"location":"updates/grass/#hybrid","title":"Hybrid","text":"<p>However, grass shaders are likely to remain most ideally suited to actual grass: long blades of relatively simple shapes that can be described by a few splines or polygons.   More elaborate forms of ground cover\u2014such as ferns, flowers, and branching-stem plants\u2014will be more difficult to render.  It's not impossible:  many of the \"vegetation systems\" today render their plants based on things like Lindenmayer systems (https://en.wikipedia.org/wiki/L-system), which are recursive visual \"grammars\" that can produce remarkably natural-looking plants.   But these are still more complex, and arbitrarily-recursive algorithms tend not to have good shader solutions, anyway.</p> <p>So ideally we'd be able to use a hybrid version:  generate our base grasses with a good, general-as-possible grass shader, and supplement the other ground covers using the other options available.</p>","tags":["grass"]}]}